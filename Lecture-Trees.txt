Trees

Trees, Binary Search Trees, Heaps & Applications

Dr. Chris Bourke

Department of Computer Science & Engineering

University of NebraskaLincoln

Lincoln, NE 68588, USA

cbourke@cse.unl.edu

http://cse.unl.edu/~cbourke

2015/01/31 21:05:31

Abstract

These are lecture notes used in CSCE 156 (Computer Science II), CSCE 235 (Dis-
crete Structures) and CSCE 310 (Data Structures & Algorithms) at the University of
NebraskaLincoln.

This work is
Attribution-ShareAlike 4.0 International License

licensed under a Creative Commons

1

Contents

I Trees

1 Introduction

2 Definitions & Terminology

3 Tree Traversal

3.1 Preorder Traversal

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.2

Inorder Traversal

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.3 Postorder Traversal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.4 Breadth-First Search Traversal . . . . . . . . . . . . . . . . . . . . . . . . . .

3.5

Implementations & Data Structures . . . . . . . . . . . . . . . . . . . . . . .

3.5.1 Preorder Implementations . . . . . . . . . . . . . . . . . . . . . . . .

3.5.2

Inorder Implementation . . . . . . . . . . . . . . . . . . . . . . . . .

3.5.3 Postorder Implementation . . . . . . . . . . . . . . . . . . . . . . . .

3.5.4 BFS Implementation . . . . . . . . . . . . . . . . . . . . . . . . . . .

3.5.5 Tree Walk Implementations

. . . . . . . . . . . . . . . . . . . . . . .

3.6 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4 Binary Search Trees

4.1 Basic Operations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5 Balanced Binary Search Trees

5.1

2-3 Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.2 AVL Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5.3 Red-Black Trees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

6 Optimal Binary Search Trees

7 Heaps

7.1 Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2

Implementations

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

7.2.1 Finding the first available open spot in a Tree-based Heap . . . . . .

2

4

4

5

7

7

7

7

8

8

8

9

10

12

12

12

14

15

17

17

17

19

19

19

20

21

21

7.3 Heap Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

25

8 Java Collections Framework

9 Applications

9.1 Huffman Coding

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9.1.1 Example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Stack-based Traversal Simulations

A.1 Preorder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.2 Inorder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A.3 Postorder

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

List of Algorithms

1

2

3

4

5

6

7

8

9

Recursive Preorder Tree Traversal

. . . . . . . . . . . . . . . . . . . . . . . .

Stack-based Preorder Tree Traversal

. . . . . . . . . . . . . . . . . . . . . . .

Stack-based Inorder Tree Traversal

. . . . . . . . . . . . . . . . . . . . . . . .

Stack-based Postorder Tree Traversal . . . . . . . . . . . . . . . . . . . . . . .

Queue-based BFS Tree Traversal

. . . . . . . . . . . . . . . . . . . . . . . . .

Tree Walk based Tree Traversal . . . . . . . . . . . . . . . . . . . . . . . . . .

Search algorithm for a binary search tree . . . . . . . . . . . . . . . . . . . . .

Find Next Open Spot - Numerical Technique . . . . . . . . . . . . . . . . . .

Find Next Open Spot - Walk Technique . . . . . . . . . . . . . . . . . . . . .

10 Heap Sort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11 Huffman Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Code Samples

List of Figures

1

2

3

A Binary Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

A Binary Search Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Binary Search Tree Operations

. . . . . . . . . . . . . . . . . . . . . . . . .

3

26

26

26

28

29

29

30

31

8

9

10

11

12

13

16

23

24

25

28

6

15

18

4

5

A min-heap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

Huffman Tree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

29

Part I
Trees

Lecture Outlines

CSCE 156

 Basic definitions
 Tree Traversals

CSCE 235

 Review of basic definitions
 Huffman Coding

CSCE 310

 Heaps, Heap Sort
 Balanced BSTs: 2-3 Trees, AVL, Red-Black

1

Introduction

Motivation: we want a data structure to store elements that offers efficient, arbitrary retrieval
(search), insertion, and deletion.

 Array-based Lists

 O(n) insertion and deletion

 Fast index-based retrieval

 Efficient binary search if sorted

 Linked Lists

 Efficient, O(1) insert/delete for head/tail

4

 Inefficient, O(n) arbitrary search/insert/delete

 Efficient binary search not possible without random access

 Stacks and queues are efficient, but are restricted access data structures
 Possible alternative: Trees
 Trees have the potential to provide O(log n) efficiency for all operations

2 Definitions & Terminology

 A tree is an acyclic graph
 For our purposes: a tree is a collection of nodes (that can hold keys, data, etc.) that

are connected by edges

 Trees are also oriented : each node has a parent and children
 A node with no parents is the root of the tree, all child nodes are oriented downward
 Nodes not immediately connected can have an ancestor, descendant or cousin relation-

ship

 A node with no children is a leaf
 A tree such that all nodes have at most two children is called a binary tree
 A binary tree is also oriented horizontally: each node may have a left and/or a right

child

 Example: see Figure 1
 A path in a tree is a sequence nodes connected by edges
 The length of a path in a tree is the number of edges in the path (which equals the

number of nodes in the path minus one)

 A path is simple if it does not traverse nodes more than once (this is the default type

of path)

 The depth of a node u is the length of the (unique) path from the root to u
 The depth of the root is 0
 The depth of a tree is the maximal depth of any node in the tree (sometimes the term

height is used)

5

 All nodes of the same depth are considered to be at the same level
 A binary tree is complete (also called full or perfect) if all nodes are present at all levels

0 up to its depth d

 A sub-tree rooted at a node u is the tree consisting of all descendants with u oriented

as the root

a

b

d

e

c

f

g

h

i

j

k

l

m

n

o

p

q

r

Figure 1: A Binary Tree

Properties:

 In a tree, all nodes are connected by exactly one unique path
 The maximum number of nodes at any level k is 2k
 Thus, the maximum number of nodes n for any binary tree of depth d is:

d(cid:88)

n = 20 + 21 + 22 +  + 2d1 + 2d =

2k = 2d+1  1

 Given a full binary tree with n nodes in it has depth:
d = log (n + 1)  1

k=0

 That is, d = O(log n)

Motivation: if we can create a tree-based data structure with operations proportional to its
depth, then we could potentially have a data structure that allows retrieval/search, insertion,
and deletion in O(log n)-time.

6

3 Tree Traversal

 Given a tree, we need a way to enumerate elements in a tree
 Many algorithms exist to iterate over the elements in a tree
 Well look at several variations on a depth-first-search

3.1 Preorder Traversal

 A preorder traversal strategy visits nodes in the following order: root; left-sub-tree;

right-sub-tree

 An example traversal on the tree in Figure 1:

a, b, d, g, l, m, r, h, n, e, i, o, c, f, j, p, q, k

 Applications:

 Building a tree, traversing a tree, copying a tree, etc.

 Efficient stack-based implementation

 Used in prefix notation (polish notation); used in languages such as Lisp/Scheme

3.2 Inorder Traversal

 An inorder traversal strategy visits nodes in the following order:

left-sub-tree; root;

right-sub-tree

 An example traversal on the tree in Figure 1:

l, g, r, m, d, h, n, b, e, o, i, a, c, p, j, q, f, k

 Applications:

 Enumerating elements in order in a binary search tree

 Expression trees

3.3 Postorder Traversal

 A postorder traversal strategy visits nodes in the following order: left-sub-tree; right-

sub-tree; root

7

 An example traversal on the tree in Figure 1:

l, r, m, g, n, h, d, o, i, e, b, p, q, j, k, f, c, a

 Applications:

 Topological sorting

 Destroying a tree when manual memory management is necessary (roots are the

last thing that get cleaned up)

 Reverse polish notation (operand-operand-operator, unambiguous, used in old HP

calculators)

 PostScript (Page Description Language)

3.4 Breadth-First Search Traversal

 Breadth-First Search (BFS) traversal is a general graph traversal strategy that explores

local or close nodes first before traversing deeper into the graph

 When applied to an oriented binary tree, BFS explores the tree level-by-level (top-to-

bottom, left-to-right)

3.5 Implementations & Data Structures

 Reference based implementation: TreeNode<T>

 Owns (through composition) references to: leftChild, rightChild, parent

 Can use either sentinel nodes or null to indicate missing children and parent

 BinaryTree<T> owns a root
 SVN examples: unl.cse.bst

3.5.1 Preorder Implementations

Input : A binary tree node u
Output: A preorder traversal of the nodes in the subtree rooted at u

1 print u
2 preOrderTraversal(u  lef tChild)
3 preOrderTraversal(u  rightChild)

Algorithm 1: Recursive Preorder Tree Traversal

Stack-based implementation:

8

 Initially, we push the trees root into the stack
 Within a loop, we pop the top of the stack and process it
 We need to push the nodes children for future processing
 Since a stack is LIFO, we push the right child first.

Input : A binary tree, T
Output: A preorder traversal of the nodes in T

1 S  empty stack
2 push T s root onto S
3 while S is not empty do
4

node  S.pop
push nodes right-child onto S
push nodes left-child onto S
process node

5

6

7
8 end

Algorithm 2: Stack-based Preorder Tree Traversal

3.5.2

Inorder Implementation

Stack-based implementation:

 The same basic idea: push nodes onto the stack as you visit them
 However, we want to delay processing the node until weve explored the left-sub-tree
 We need a way to tell if we are visiting the node for the first time or returning from

the left-tree exploration

 To achieve this, we allow the node to be null
 If null, then we are returning from a left-tree exploration, pop the top of the stack and

process (then push the right child)

 If not null, then we push it to the stack for later processing, explore the left child

9

Input : A binary tree, T
Output: An inorder traversal of the nodes in T

1 S  empty stack
2 u  root
3 while S is not empty Or u (cid:54)= null do

4

5

6

7

8

9

10

if u (cid:54)= null then
push u onto S
u  u.leftChild

else

u  S.pop
process u
u  u.rightChild

end

11
12 end

Algorithm 3: Stack-based Inorder Tree Traversal

3.5.3 Postorder Implementation

Stack-based implementation:

 Same basic ideas, except that we need to distinguish if were visiting the node for the

first time, second time or last (so that we can process it)

 To achieve this, we keep track of where we came from: a parent, left, or right node
 We keep track of a previous and a current node

10

Input : A binary tree, T
Output: A postorder traversal of the nodes in T

1 S  empty stack
2 prev  null
3 push root onto S
4 while S is not empty do
curr  S.peek
5
if prev = null Or prev.leftChild = curr Or prev.rightChild = curr then

6

if curr.leftChild (cid:54)= null then
push curr.leftChild onto S
else if curr.rightChild (cid:54)= null then

push curr.rightChild onto S

end

else if curr.leftChild = prev then
if curr.rightChild (cid:54)= null then
push curr.rightChild onto S

end

else

process curr
S.pop

7

8

9

10

11

12

13

14

15

16

17

18

19

end
prev  curr

20
21 end

Algorithm 4: Stack-based Postorder Tree Traversal

11

3.5.4 BFS Implementation

Input : A binary tree, T
Output: A BFS traversal of the nodes in T

1 Q  empty queue
2 enqueue T s root into Q
3 while Q is not empty do
node  Q.dequeue
4
enqueue nodes left-child onto Q
enqueue nodes right-child onto Q
print node

6

5

7
8 end

Algorithm 5: Queue-based BFS Tree Traversal

3.5.5 Tree Walk Implementations

 Simple rules based on local information: where you are and where you came from
 No additional data structures required
 Traversal is a walk around the perimeter of the tree
 Can use similar rules to determine when the current node should be processed to

achieve pre, in, and postorder traversals

 Need to take care with corner cases (when current node is the root or children are

missing)

 Pseudocode presented Algorithm 6

3.6 Operations

Basic Operations:

 Search for a particular element/key
 Adding an element

 Add at most shallow available spot

 Add at a random leaf

12

Input : A binary tree, T
Output: A Tree Walk around T

1 curr  root
2 prevT ype  parent
3 while curr (cid:54)=null do

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

if prevT ype = parent then

//preorder: process curr here
if curr.lef tChild exists then
//Go to the left child:
curr  curr.lef tChild
prevT ype  parent

else

curr  curr
prevT ype  lef t

end

else if prevT ype = lef t then

//inorder: process curr here
if curr.rightChild exists then
//Go to the right child:
curr  curr.rightChild
prevT ype  parent

else

curr  curr
prevT ype  right

end

else if prevT ype = right then

//postorder: process curr here
if curr.parent = null then

//root has no parent, were done traversing
curr  curr.parent

//are we at the parents left or right child?
else if curr = curr.parent.lef tChild then

curr  curr.parent
prevT ype  lef t

else

curr  curr.parent
prevT ype  right

end

end

30
31 end

13

Algorithm 6: Tree Walk based Tree Traversal

 Add internally, shift nodes down by some criteria

 Removing elements

 Removing leaves

 Removing elements with one child

 Removing elements with two children

Other Operations:

 Compute the total number of nodes in a tree
 Compute the total number of leaves in a tree
 Given an item or node, compute its depth
 Compute the depth of a tree

4 Binary Search Trees

Regular binary search trees have little structure to their elements; search, insert, delete
operations are still linear with respect to the number of tree nodes, O(n). We want a data
structure with operations proportional to its depth, O(d). To this end, we add structure and
order to tree nodes.

 Each node has an associated key
 Binary Search Tree Property: For every node u with key uk in T

1. Every node in the left-sub-tree of u has keys less than uk
2. Every node in the right-sub-tree of u has keys greater than uk

 Duplicate keys can be handled, but you must be consistent and not guaranteed to be

contiguous

 Alternatively: do not allow duplicate keys or define a key scheme that ensures a total

order

 Inductive property: all sub-trees are also binary search trees
 A full example can be found in Figure 2

14

50

40

60

20

45

90

10

24

49

80

95

2

15

28

48

75

85

12

Figure 2: A Binary Search Tree

4.1 Basic Operations

Observation: a binary search tree has more structure: the key in each node provides infor-
mation on where a node is not located. We will exploit this structure to achieve O(log n)
operations.

Search/retrieve

 Goal: find a node (and its data) that matches a given key k
 Start at the node
 At each node u, compare k to us key, uk:

 If equal, element found, stop and return

 If k < uk, traverse to us left-child
 If k > uk, traverse to us right-child

 Traverse until the sub-tree is empty (element not found)
 Analysis: number of comparisons is bounded by the depth of the tree, O(d)

15

Input : A binary search tree, T , a key k
Output: The tree node u  T whose key, uk matches k
1 u  T s root
2 while u (cid:54)=  do

if uk = k then

output u

end
else if uk > k then
u  us left-child
else if uk < k then
u  us left-child

3

4

5

6

7

8

9
10 end
11 output 

Algorithm 7: Search algorithm for a binary search tree

Insert

 Insert new nodes as leaves
 To determine where it should be inserted: traverse the tree as above
 Insert at the first available spot (first missing child node)
 Analysis: finding the available location is O(d), inserting is just reference juggling,

O(1)

Delete

 Need to first find the node u to delete, traverse as above, O(d)
 If u is a leaf (no children): its safe to simply delete it
 If u has one child, then we can promote it to us spot (us parent will now point to

us child)

 If u has two children, we need to find a way to preserve the BST property

 Want to minimally change the trees structure

 Need the operation to be efficient

 Find the minimum element of the greater nodes (right sub-tree) or the maximal

element of the lesser nodes (left sub-tree)

 Such an element will have at most one child (which we know how to delete)

16

 Delete it and store off the key/data

 Replace us key/data with the contents of the minimum/maximum element

 Analysis:

 Search/Find: O(d)

 Finding the min/max: O(d)

 Swapping: O(1)

 In total: O(d)

 Examples illustrated in Figure 3

5 Balanced Binary Search Trees

Motivation:

 Ideally, a full binary tree has depth d = O(log n)
 General BSTs can degenerate such that d = O(n)
 Need a way to preserve the BST property while at the same time limiting the depth

to O(log n)

 Solution: Balanced Binary Search Trees
 BSTs that efficiently reorganize themselves such that the BST Property is preserved

and that the depth is restricted to O(log n)

 Some types of Balanced BSTs:

 B-trees (and 2-3 trees)

 AVL Trees

 Red-Black Trees

 Splay trees

5.1 2-3 Trees

See 310 Note Set.

5.2 AVL Trees

See 310 Note Set.

17

15

15

7

30

7

30

4

9

20

40

4

9

20

40

1

5

17

23

1

5

17

23

16

(a) A Binary Search Tree

(b) Insertion of a new node (16) into a Binary
Search Tree

15

9

7

30

7

30

4

9

20

40

4

20

40

1

5

17

23

1

5

17

23

(c) Deletion of a node with two children (15).
First step: find the maximum node in the left-
sub-tree (lesser elements).

(d) Node 15 is replaced with the extremal node,
preserving the BST property

9

9

7

30

4

30

4

20

40

1

5

20

40

1

5

17

23

17

23

(e) Deletion a node with only one child (7).

(f) Removal is achieved by simply promoting
the single child/subtree.

Figure 3: Binary Search Tree Operations. Figure 3(b) depicts the insertion of (16) into
the tree in Figure 3(a). Figures 3(c) and 3(d) depict the deletion of a node (15) with two
children. Figures 3(e) and 3(f) depict the deletion of a node with only one child (7).

18

5.3 Red-Black Trees

See 310 Note Set.

6 Optimal Binary Search Trees

See 310 Note Set.

7 Heaps

Definition 1. A heap is a binary tree that satisfies the following properties.

1. It is a full or complete binary tree: all nodes are present except possibly the last row

2. If the last row is not full, all nodes are full-to-the-left

3. It satisfies the Heap Property: every node has a key that is greater than both of its

children (max-heap)

 As a consequence of the Heap Property, the maximal element is always at the root
 Alternatively, we can define a min-heap
 Variations: 2-3 heaps, fibonacci heaps, etc.
 A min-heap example can be found in Figure 4

1

5

30

50

55

40

80

60

53

72

70

65

45

90

95

92

63

Applications

Figure 4: A min-heap

19

 Heaps are an optimal implementation of a priority queue
 Used extensively in other algorithms (Heap Sort, Prims, Dijkstras, Huffman Coding,

etc.) to ensure efficient operation

7.1 Operations

Insert

 Want to preserve the full-ness property and the Heap Property
 Preserve full-ness: add new element at the end of the heap (last row, first free spot on

the left)

 Insertion at the end may violate the Heap Property
 Heapify/fix: bubble up inserted element until Heap Property is satisfied
 Analysis: insert is O(1); heapify: O(d)

Remove from top

 Want to preserve the full-ness property and the Heap Property
 Preserve full-ness: swap root element with the last element in the heap (lowest row,

right-most element)

 Heap property may be violated
 Heapify/fix: bubble new root element down until Heap Property is satisfied
 Analysis: Swap is O(1); heapify: O(d)

Others

 Arbitrary remove
 Find
 Possible, but not ideal: Heaps are restricted-access data structures

Analysis

 All operations are O(d)
 Since Heaps are full, d = O(log n)
 Thus, all operations are O(log n)

20

7.2 Implementations

Array-Based

 Root is located at index 1
 If a node u is at index i, us left-child is at 2i, its right-child is at 2i + 1
2(cid:99)
 If node u is at index j, its parent is at index (cid:98) j
 Alternatively: 0-index array left/right children/parent are at 2n + 1, 2n + 2, and (cid:98) j1
2 (cid:99)
 Advantage: easy implementation, all items are contiguous in the array (in fact, a BFS

ordering!)

 Disadvantage:

Insert operation may force a reallocation, but this can be done in

amortized-constant time (though may still have wasted space)

Tree-Based

 Reference-based tree (nodes which contain references to children/parent)
 Parent reference is now required for efficiency
 For efficiency, we need to keep track of the last element in the tree
 For deletes/inserts: we need a way to find the last element and first open spot
 Well focus on finding the first available open spot as the same technique can be used

to find the last element with minor modifications

7.2.1 Finding the first available open spot in a Tree-based Heap

Technique A: numerical technique

 WLOG: assume we keep track of the number of nodes in the heap, n and thus the

depth d = (cid:98)log n(cid:99)

 If n = 2d+1  1 then the tree is full, the last element is all the way to the right, the

first available spot is all the way to the left

 Otherwise n < 2d+1  1 and the heap is not full (the first available spot is located at

level d, root is at level 0)

 Starting at the root, we want to know if the last element is in the left-subtree or the

right subtree

21

 Let m = n  (2d  1), the number of nodes present in level d
 If m  2d

2 then the left-sub tree is full at the last level and so the next open spot would

be in the right-sub tree

 Otherwise if m < 2d

open spot is in the left-sub tree

2 then the left-sub tree is not full at the last level and so the next

 Traverse down to the left or right respectively and repeat: the resulting sub-tree will
2 (if traversing right)

have depth d  1 with m = m (if traversing left) or m = m  2d

 Repeat until weve found the first available spot
 Analysis: in any case, its O(d) = O(log n) to traverse from the root to the first open

spot

22

Input : A tree-based heap H with n nodes
Output: The node whose child is the next available open spot in the heap

1 curr  T.head
2 d  (cid:98)log n(cid:99)
3 m  n
4 while curr has both children do
5

if m = 2d+1  1 then

//remaining tree is full, traverse all the way left
while curr has both children do

curr  curr.lef tChild

end

else

//remaining tree is not full, determine if the next open spot is

in the left or right sub-tree

then

if m  2d

2

//left sub-tree is full
d  (d  1)
m  (m  2d
2 )
curr  curr.rightChild

else

//left sub-tree is not full
d  (d  1)
m  m
curr  curr.lef tChild

end

6

7

8

9

10

11

12

13

14

15

16

17

18

end

19
20 end
21 output curr

Algorithm 8: Find Next Open Spot - Numerical Technique

Technique B: Walk Technique

 Alternatively, we can adapt the idea behind the tree walk algorithm to find the next

available open spot

 Well assume that weve kept track of the last node
 If the tree is full, we simply traverse all the way to the left and insert, O(d)

23

 If the last node is a left-child then its parents right child is the next available spot,

finding it is O(1)

 Otherwise, well need to traverse around the perimeter of the tree until we reach the

next open slot

Input : A tree-based heap H with n nodes
Output: The node whose (missing) child is the next available open spot in the heap

1 d  (cid:98)log n(cid:99)
2 if n = 2d+1  1 then

//The tree is full, traverse all the way to the left
curr  root
while curr.lef tChild (cid:54)= null do

curr  curr.lef tChild

3

4

5

end

6
7 else if last is a left-child then

//parents right child is open
curr  last.parent

8
9 else

//The open spot lies in a subtree to the right of the last node

//Walk the tree until we reach it
curr  last.parent
while curr is a right-child do

curr  curr.parent

end

//"turn" right
curr  curr.parent
curr  curr.rightChild
//traverse all the way left
while curr.lef tChild (cid:54)= null do

curr  curr.lef tChild

10

11

12

13

14

15

16

17

end

18
19 end

//current nodes missing child is the open spot

20 output curr

Algorithm 9: Find Next Open Spot - Walk Technique

24

7.3 Heap Sort

 If min/max element is always at the top; simply insert all elements, then remove them

all!

 Perfect illustration of Smart data structures and dumb code are a lot better than the

other way around

Input : A collection of elements A = {a1, . . . , an}
Output: A collection, A(cid:48) of elements in A, sorted

1 H  empty heap
2 A(cid:48)  empty collection
3 foreach x  A do
insert x into H

4
5 end
6 while H is not empty do
y  remove top from H
7
Add y to the end of A(cid:48)

8
9 end
10 output A(cid:48)

Analysis

 Amortized analysis:

rithm

Algorithm 10: Heap Sort

insert/remove operations are not constant throughout the algo-

 On first iteration: insert is d = O(1); on the i-th iteration, d = O(log i); only on the

last iteration is insertion O(log n)

 In total, the insert phase is:

n(cid:88)

i=1

log i = O(n log n)

 A similar lower bound can be shown
 Same analysis applies to the remove phase:

1(cid:88)

log i

 In total, O(n log n)

i=n

25

8 Java Collections Framework

Java has support for several data structures supported by underlying tree structures.

 java.util.PriorityQueue<E> is a binary-heap based priority queue

 Priority (keys) based on either natural ordering or a provided Comparator

 Guaranteed O(log n) time for insert (offer) and get top (poll)

 Supports O(n) arbitrary remove(Object) and search (contains) methods

 java.util.TreeSet<E>

 Implements the SortedSet interface; makes use of a Comparator

 Backed by TreeMap, a red-black tree balanced binary tree implementation

 Guaranteed O(log n) time for add, remove, contains operations

 Default iterator is an in-order traversal

9 Applications

9.1 Huffman Coding

Overview

 Coding Theory is the study and theory of codesschemes for transmitting data
 Coding theory involves efficiently padding out data with redundant information to

increase reliability (detect or even correct errors) over a noisy channel

 Coding theory also involves compressing data to save space

 MP3s (uses a form of Huffman coding, but is information lossy)

 jpegs, mpegs, even DVDs

 pack (straight Huffman coding)

 zip, gzip (uses a Ziv-Lempel and Huffman compression algorithm)

Basics

 Let  be a fixed alphabet of size n
 A coding is a mapping of this alphabet to a collection of binary codewords,

  {0, 1}

26

 A block encoding is a fixed length encoding scheme where all codewords have the same

length (example: ASCII); requires (cid:100)log2 n(cid:101) length codes

 Not all symbols have the same frequency, alternative: variable length encoding
 Intuitively: assign shorter codewords to more frequent symbols, longer to less frequent

symbols

 Reduction in the overall average codeword length
 Variable length encodings must be unambiguous
 Solution: prefix free codes: a code in which no whole codeword is the prefix of another

(other than itself of course).

 Examples:

 {0, 01, 101, 010} is not a prefix free code.
 {10, 010, 110, 0110} is a prefix free code.

 A simple way of building a prefix free code is to associate codewords with the leaves

of a binary tree (not necessarily full).

 Each edge corresponds to a bit, 0 if it is to the left sub-child and 1 to the right sub-child.
 Since no simple path from the root to any leaf can continue to another leaf, then we

are guaranteed a prefix free coding.

 Using this idea along with a greedy encoding forms the basis of Huffman Coding

Steps

 Consider a precomputed relative frequency function:
freq :   [0, 1]

 Build a collection of weighted trees Tx for each symbol x  Sigma with wt(Tx) = freq(x)
 Combine the two least weighted trees via a new node; associate a new weight (the sum

of the weights of the two subtrees)

 Keep combining until only one tree remains
 The tree constructed in Huffmans algorithm is known as a Huffman Tree and it defines

a Huffman Code

27

Input : An alphabet of symbols,  with relative frequencies, freq(x)
Output: A Huffman Tree

3

4

9

10

11

12

13

1 H  new min-heap
2 foreach x   do
Tx  single node tree
wt(Tx)  freq(x)
insert Tx into H

5
6 end
7 while size of H > 1 do
8

Tr  new tree root node
Ta  H.getM in
Tb  H.getM in
Tr.lef tChild  Ta
Tr.rightChild  Tb
wt(r)  wt(Ta) + wt(Tb)
insert Tr into H

14
15 end
16 output H.getM in

Algorithm 11: Huffman Coding

9.1.1 Example

Construct the Huffman Tree and Huffman Code for a file with the following content.

character
frequency 0.10

A

B

C

0.15

0.34

D
.05

E
.12

F
.21

G
.03

 Average codeword length:

.10  3 + .15  3 + .34  2 + .05  4 + .12  3 + .21  2 + .03  4 = 2.53

 Compression ratio:

(3  2.53)

3

= 15.67%

 In general, for text files, pack (Huffman Coding), claims an average compression ratio

of 25-40%.

 Degenerative cases:

28

.08

0

1

F : .21

1

0

.61

1

.27

0

1

C : .34

.39

0

.18

0

1

.08

A : .10

E : .12

B : .15

0

1

G : .03

D : .05

Figure 5: Huffman Tree

 When the probability distribution is uniform: p(x) = p(y) for all x, y  
 When the probability distribution follows a fibonacci sequence (the sum of each
of the two smallest probabilities is less than equal to the next highest probability
for all probabilities)

A Stack-based Traversal Simulations

A.1 Preorder

Simulation of a stack-based preorder traversal of the binary tree in Figure 1.

push a

(enter loop)
pop, node = a

push c
push b
print a

(enter loop)
pop, node = b

push e
push d
print b

(enter loop)
pop, node = d

push h
push g
print d

(enter loop)
pop, node = g

push m
push l
print g

(enter loop)
pop, node = l

(no push)
(no push)

print l

(enter loop)

pop, node = m

(no push)

push r
print m

(enter loop)
pop, node = r

(no push)
(no push)

print r

(enter loop)
pop, node = h

push n

(no push)

print h

(enter loop)
pop, node = n

(no push)
(no push)

print n

(enter loop)

29

pop, node = e

push i

(no push)

print e

(enter loop)
pop, node = i

(no push)

push o
print i

(enter loop)
pop, node = o

(no push)
(no push)

print o

(enter loop)
pop, node = c

push f

(no push)

print c

(enter loop)
pop, node = f

push k
push j
print f

(enter loop)
pop, node = j

push q
push p
print j

(enter loop)
pop, node = p

(no push)

(no push)

print p

(enter loop)
pop, node = q

(no push)
(no push)

print q

(enter loop)
pop, node = k

(no push)
(no push)

print k

A.2

Inorder

Simulation of a stack-based inorder traversal of the binary tree in Figure 1.

(enter loop, u = a)

push a

update u = b

(enter loop, u = b)

push b

update u = d

(enter loop, u = d)

push d

update u = g

(enter loop,
u = null)

pop r, update

u = r

process r

update u = null

(enter loop,
u = null)

update u = null

(enter loop,
u = null)

pop b, update

u = b

process b

update u = e

pop m, update

(enter loop, u = e)

u = m

process m

push e

update u = null

(enter loop, u = g)

update u = null

push g

update u = l

(enter loop,
u = null)

(enter loop,
u = null)

(enter loop, u = l)

pop d, update

push l

update u = null

(enter loop,
u = null)

u = d

process d

update u = h

(enter loop, u = h)

pop l, update u = l

push h

process l

update u = null

u = e

process e

update u = i

(enter loop, u = i)

push i

update u = o

(enter loop,
u = null)

pop i, update

u = i

process i

u = p

process p

update u = null

(enter loop,
u = null)

update u = null

pop j, update

(enter loop,
u = null)

pop a, update

u = a

process a

u = j

process j

update u = q

(enter loop, u = q)

push q

update u = c

update u = null

pop e, update

(enter loop, u = c)

push c

(enter loop,
u = null)

update u = null

pop q, update

(enter loop,
u = null)

pop c, update

u = c

process c

u = q

process q

update u = null

(enter loop,
u = null)

pop f , update

u = f

process f

update u = k

(enter loop, u = k)

update u = null

(enter loop,
u = null)

pop k, update

u = k

process k

update u = null

update u = null

(enter loop, u = o)

update u = f

(enter loop,
u = null)

pop g, update

u = g

process g

update u = m

(enter loop,
u = null)

pop h, update

u = h

process h

push o

update u = null

(enter loop, u = f )

(enter loop,
u = null)

push f

update u = j

update u = n

pop o, update

(enter loop, u = j)

push k

(enter loop, u = n)

u = o

process o

push j

update u = p

(enter loop, u = m)

push n

update u = null

push m

update u = r

update u = null

(enter loop, u = r)

push r

update u = null

(enter loop,
u = null)

pop n, update

u = n

process n

(enter loop,
u = null)

pop i, update

u = i

process i

(enter loop, u = p)

push p

update u = null

(enter loop,
u = null)

update u = null

pop p, update

(done)

30

A.3 Postorder

Simulation of a stack-based postorder traversal of the binary tree in Figure 1:

prev = null

push a

process l

update prev = l

(enter loop)

update curr = (a)
check: prev = null

push (b)

update prev = a

(enter loop)

update curr = (g)

check:

prev.rightChild = curr
(null.rightChild = (g))

update prev = m

update curr = (n)

check:

(enter loop)

update curr = (m)

prev.rightChild = curr
(null.rightChild = (n))

check:

prev.rightChild = curr
(null.rightChild = (m))

process n

update prev = n

process m

(enter loop)

check:

update prev = m

update curr = (h)

(enter loop)

update curr = (b)

check:

prev.lef tChild = curr
((b).lef tChild = (b))

push (d)

update prev = b

curr.lef tChild = prev
((l).lef tChild = (l))

push (m)

update prev = g

(enter loop)

update curr = (m)

check:

(enter loop)

update curr = (d)

prev.rightChild = curr
((m).rightChild = (m))

check:

prev.lef tChild = curr
((d).lef tChild = (d))

push (g)

update prev = d

push (r)

update prev = m

(enter loop)

update curr = (r)

check:

(enter loop)

update curr = (g)

prev.lef tChild = curr
((r).lef tChild = (r))

check:

prev.lef tChild = curr
((g).lef tChild = (g))

push (l)

update prev = g

(noop)

update prev = r

(enter loop)

update curr = (r)

check:

(enter loop)

update curr = (l)

prev.rightChild = curr
(null.rightChild = (r))

check:

prev.lef tChild = curr
((l).lef tChild = (l))

process r

update prev = r

(noop)

(enter loop)

(enter loop)

update curr = (g)

prev.rightChild = curr
(null.rightChild = (h))

check:

check:

prev.rightChild = curr
(null.rightChild = (g))

process g

update prev = g

process h

update prev = h

(enter loop)

update curr = (d)

check:

(enter loop)

update curr = (d)

prev.rightChild = curr
((n).rightChild = (d))

check:

prev.rightChild = curr
((m).rightChild = (d))

check:

curr.lef tChild = prev
((g).lef tChild = (g))

process d

update prev = d

(enter loop)

update curr = (b)

check:

push (h)

update prev = d

prev.rightChild = curr
((h).rightChild = (b))

(enter loop)

update curr = (h)

curr.lef tChild = prev
((d).lef tChild = (d))

check:

check:

prev.rightChild = curr
((h).rightChild = (h))

push (n)

update prev = h

push (e)

update prev = b

(enter loop)

update curr = (e)

check:

(enter loop)

update curr = (n)

prev.rightChild = curr
((e).rightChild = (e))

update prev = l

update curr = (m)

check:

(enter loop)

prev.rightChild = curr

31

check:

(enter loop)

update curr = (l)

prev.rightChild = curr
(null.rightChild = (m))

check:

check:

prev.rightChild = curr
(null.rightChild = (l))

curr.lef tChild = prev
((r).lef tChild = (r))

prev.rightChild = curr
((n).rightChild = (n))

(noop)

update prev = n

push (i)

update prev = e

(enter loop)

update curr = (i)

check:

((i).rightChild = (i))

check:

push (o)

update prev = i

prev.rightChild = curr
((i).rightChild = (b))

(enter loop)

update curr = (p)

(enter loop)

update curr = (o)

check:

prev.lef tChild = curr
((o).lef tChild = (o))

process b

update prev = b

(enter loop)

update curr = (a)

check:

check:

prev.rightChild = curr
(null.rightChild = (p))

push (k)

update prev = f

process p

update prev = p

((q).rightChild = (f ))

check:

curr.lef tChild = prev
((j).lef tChild = (j))

(enter loop)

update curr = (k)

check:

prev.rightChild = curr
((k).rightChild = (k))

(noop)

update prev = o

prev.rightChild = curr
((e).rightChild = (a))

(enter loop)

update curr = (j)

check:

check:

(enter loop)

update curr = (o)

check:

prev.rightChild = curr
(null.rightChild = (o))

process o

update prev = o

curr.lef tChild = prev
((b).lef tChild = (b))

prev.rightChild = curr
(null.rightChild = (j))

(noop)

update prev = k

push (c)

update prev = a

(enter loop)

update curr = (c)

check:

check:

curr.lef tChild = prev
((p).lef tChild = (p))

(enter loop)

update curr = (k)

push (q)

update prev = j

check:

prev.rightChild = curr
(null.rightChild = (k))

process k

update prev = k

(enter loop)

update curr = (i)

prev.rightChild = curr
((c).rightChild = (c))

(enter loop)

update curr = (q)

check:

prev.rightChild = curr
(null.rightChild = (i))

check:

curr.lef tChild = prev
((o).lef tChild = (o))

update prev = i

push (f )

update prev = c

check:

prev.rightChild = curr
((q).rightChild = (q))

(enter loop)

update curr = (f )

(enter loop)

(noop)

update curr = (f )

update prev = q

check:

prev.rightChild = curr
((f ).rightChild = (f ))

(enter loop)

update curr = (q)

check:

prev.rightChild = curr
(null.rightChild = (f ))

process f

update prev = f

(enter loop)

update curr = (i)

check:

push (j)

update prev = f

check:

prev.rightChild = curr
(null.rightChild = (q))

(enter loop)

update curr = (c)

prev.rightChild = curr
(null.rightChild = (i))

(enter loop)

update curr = (j)

process q

update prev = q

prev.rightChild = curr
(null.rightChild = (e))

(enter loop)

update curr = (p)

process j

update prev = j

process e

update prev = e

check:

prev.lef tChild = curr
((p).lef tChild = (p))

(enter loop)

process a

update curr = (f )

update prev = a

(enter loop)

update curr = (b)

(noop)

check:

update prev = p

prev.rightChild = curr

32

process i

update prev = i

(enter loop)

update curr = (e)

check:

check:

prev.lef tChild = curr
((j).lef tChild = (j))

push (p)

update prev = j

(enter loop)

update curr = (j)

check:

prev.rightChild = curr
(null.rightChild = (j))

(enter loop)

update curr = (a)

check:

prev.rightChild = curr
((k).rightChild = (c))

process c

update prev = c

check:

prev.rightChild = curr
((f ).rightChild = (a))

