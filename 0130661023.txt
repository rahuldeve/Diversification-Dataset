1
INTRODUCTION

Each of the past three centuries has been dominated by a single technology.
The 18th century was the era of the great mechanical systems accompanying the
Industrial Revolution. The 19th century was the age of the steam engine. During
the 20th century, the key technology was information gathering, processing, and
distribution. Among other developments, we saw the installation of worldwide
telephone networks, the invention of radio and television, the birth and unprecedented growth of the computer industry, and the launching of communication
satellites.
As a result of rapid technological progress, these areas are rapidly converging
and the differences between collecting, transporting, storing, and processing information are quickly disappearing. Organizations with hundreds of offices spread
over a wide geographical area routinely expect to be able to examine the current
status of even their most remote outpost at the push of a button. As our ability to
gather, process, and distribute information grows, the demand for ever more
sophisticated information processing grows even faster.
Although the computer industry is still young compared to other industries
(e.g., automobiles and air transportation), computers have made spectacular progress in a short time. During the first two decades of their existence, computer
systems were highly centralized, usually within a single large room. Not infrequently, this room had glass walls, through which visitors could gawk at the great
electronic wonder inside. A medium-sized company or university might have had
one or two computers, while large institutions had at most a few dozen. The idea
1

2

INTRODUCTION

CHAP. 1

that within twenty years equally powerful computers smaller than postage stamps
would be mass produced by the millions was pure science fiction.
The merging of computers and communications has had a profound influence
on the way computer systems are organized. The concept of the ‘‘computer
center’’ as a room with a large computer to which users bring their work for processing is now totally obsolete. The old model of a single computer serving all of
the organization’s computational needs has been replaced by one in which a large
number of separate but interconnected computers do the job. These systems are
called computer networks. The design and organization of these networks are
the subjects of this book.
Throughout the book we will use the term ‘‘computer network’’ to mean a collection of autonomous computers interconnected by a single technology. Two
computers are said to be interconnected if they are able to exchange information.
The connection need not be via a copper wire; fiber optics, microwaves, infrared,
and communication satellites can also be used. Networks come in many sizes,
shapes and forms, as we will see later. Although it may sound strange to some
people, neither the Internet nor the World Wide Web is a computer network. By
the end of this book, it should be clear why. The quick answer is: the Internet is
not a single network but a network of networks and the Web is a distributed system that runs on top of the Internet.
There is considerable confusion in the literature between a computer network
and a distributed system. The key distinction is that in a distributed system, a
collection of independent computers appears to its users as a single coherent system. Usually, it has a single model or paradigm that it presents to the users.
Often a layer of software on top of the operating system, called middleware, is
responsible for implementing this model. A well-known example of a distributed
system is the World Wide Web, in which everything looks like a document (Web
page).
In a computer network, this coherence, model, and software are absent. Users
are exposed to the actual machines, without any attempt by the system to make
the machines look and act in a coherent way. If the machines have different hardware and different operating systems, that is fully visible to the users. If a user
†
wants to run a program on a remote machine, he has to log onto that machine and
run it there.
In effect, a distributed system is a software system built on top of a network.
The software gives it a high degree of cohesiveness and transparency. Thus, the
distinction between a network and a distributed system lies with the software
(especially the operating system), rather than with the hardware.
Nevertheless, there is considerable overlap between the two subjects. For
example, both distributed systems and computer networks need to move files
around. The difference lies in who invokes the movement, the system or the user.
333333333333333333333333333333333333333333333333

† ‘‘He’’ should be read as ‘‘he or she’’ throughout this book.

SEC. 1.1

USES OF COMPUTER NETWORKS

3

Although this book primarily focuses on networks, many of the topics are also
important in distributed systems. For more information about distributed systems,
see (Tanenbaum and Van Steen, 2002).

1.1 USES OF COMPUTER NETWORKS
Before we start to examine the technical issues in detail, it is worth devoting
some time to pointing out why people are interested in computer networks and
what they can be used for. After all, if nobody were interested in computer networks, few of them would be built. We will start with traditional uses at companies and for individuals and then move on to recent developments regarding
mobile users and home networking.

1.1.1 Business Applications
Many companies have a substantial number of computers. For example, a
company may have separate computers to monitor production, keep track of
inventories, and do the payroll. Initially, each of these computers may have
worked in isolation from the others, but at some point, management may have
decided to connect them to be able to extract and correlate information about the
entire company.
Put in slightly more general form, the issue here is resource sharing, and the
goal is to make all programs, equipment, and especially data available to anyone
on the network without regard to the physical location of the resource and the
user. An obvious and widespread example is having a group of office workers
share a common printer. None of the individuals really needs a private printer,
and a high-volume networked printer is often cheaper, faster, and easier to maintain than a large collection of individual printers.
However, probably even more important than sharing physical resources such
as printers, scanners, and CD burners, is sharing information. Every large and
medium-sized company and many small companies are vitally dependent on computerized information. Most companies have customer records, inventories,
accounts receivable, financial statements, tax information, and much more online. If all of its computers went down, a bank could not last more than five minutes. A modern manufacturing plant, with a computer-controlled assembly line,
would not last even that long. Even a small travel agency or three-person law
firm is now highly dependent on computer networks for allowing employees to
access relevant information and documents instantly.
For smaller companies, all the computers are likely to be in a single office or
perhaps a single building, but for larger ones, the computers and employees may
be scattered over dozens of offices and plants in many countries. Nevertheless, a
sales person in New York might sometimes need access to a product inventory

4

INTRODUCTION

CHAP. 1

database in Singapore. In other words, the mere fact that a user happens to be
15,000 km away from his data should not prevent him from using the data as
though they were local. This goal may be summarized by saying that it is an
attempt to end the ‘‘tyranny of geography.’’
In the simplest of terms, one can imagine a company’s information system as
consisting of one or more databases and some number of employees who need to
access them remotely. In this model, the data are stored on powerful computers
called servers. Often these are centrally housed and maintained by a system
administrator. In contrast, the employees have simpler machines, called clients,
on their desks, with which they access remote data, for example, to include in
spreadsheets they are constructing. (Sometimes we will refer to the human user
of the client machine as the ‘‘client,’’ but it should be clear from the context
whether we mean the computer or its user.) The client and server machines are
connected by a network, as illustrated in Fig. 1-1. Note that we have shown the
network as a simple oval, without any detail. We will use this form when we
mean a network in the abstract sense. When more detail is required, it will be
provided.
Client
Server

Network

Figure 1-1. A network with two clients and one server.

This whole arrangement is called the client-server model. It is widely used
and forms the basis of much network usage. It is applicable when the client and
server are both in the same building (e.g., belong to the same company), but also
when they are far apart. For example, when a person at home accesses a page on
the World Wide Web, the same model is employed, with the remote Web server
being the server and the user’s personal computer being the client. Under most
conditions, one server can handle a large number of clients.
If we look at the client-server model in detail, we see that two processes are
involved, one on the client machine and one on the server machine. Communication takes the form of the client process sending a message over the network to the
server process. The client process then waits for a reply message. When the serv-

SEC. 1.1

5

USES OF COMPUTER NETWORKS

er process gets the request, it performs the requested work or looks up the requested data and sends back a reply. These messages are shown in Fig. 1-2.
Client machine

Server machine

Request
Network
Reply

Client process

Server process

Figure 1-2. The client-server model involves requests and replies.

A second goal of setting up a computer network has to do with people rather
than information or even computers. A computer network can provide a powerful
communication medium among employees. Virtually every company that has
two or more computers now has e-mail (electronic mail), which employees generally use for a great deal of daily communication. In fact, a common gripe
around the water cooler is how much e-mail everyone has to deal with, much of it
meaningless because bosses have discovered that they can send the same (often
content-free) message to all their subordinates at the push of a button.
But e-mail is not the only form of improved communication made possible by
computer networks. With a network, it is easy for two or more people who work
far apart to write a report together. When one worker makes a change to an online document, the others can see the change immediately, instead of waiting
several days for a letter. Such a speedup makes cooperation among far-flung
groups of people easy where it previously had been impossible.
Yet another form of computer-assisted communication is videoconferencing.
Using this technology, employees at distant locations can hold a meeting, seeing
and hearing each other and even writing on a shared virtual blackboard. Videoconferencing is a powerful tool for eliminating the cost and time previously
devoted to travel. It is sometimes said that communication and transportation are
having a race, and whichever wins will make the other obsolete.
A third goal for increasingly many companies is doing business electronically
with other companies, especially suppliers and customers. For example, manufacturers of automobiles, aircraft, and computers, among others, buy subsystems
from a variety of suppliers and then assemble the parts. Using computer networks, manufacturers can place orders electronically as needed. Being able to
place orders in real time (i.e., as needed) reduces the need for large inventories
and enhances efficiency.
A fourth goal that is starting to become more important is doing business with
consumers over the Internet. Airlines, bookstores, and music vendors have
discovered that many customers like the convenience of shopping from home.
Consequently, many companies provide catalogs of their goods and services online and take orders on-line. This sector is expected to grow quickly in the future.
It is called e-commerce (electronic commerce).

6

INTRODUCTION

CHAP. 1

1.1.2 Home Applications
In 1977, Ken Olsen was president of the Digital Equipment Corporation, then
the number two computer vendor in the world (after IBM). When asked why
Digital was not going after the personal computer market in a big way, he said:
‘‘There is no reason for any individual to have a computer in his home.’’ History
showed otherwise and Digital no longer exists. Why do people buy computers for
home use? Initially, for word processing and games, but in recent years that picture has changed radically. Probably the biggest reason now is for Internet access.
Some of the more popular uses of the Internet for home users are as follows:
1. Access to remote information.
2. Person-to-person communication.
3. Interactive entertainment.
4. Electronic commerce.
Access to remote information comes in many forms. It can be surfing the
World Wide Web for information or just for fun. Information available includes
the arts, business, cooking, government, health, history, hobbies, recreation, science, sports, travel, and many others. Fun comes in too many ways to mention,
plus some ways that are better left unmentioned.
Many newspapers have gone on-line and can be personalized. For example, it
is sometimes possible to tell a newspaper that you want everything about corrupt
politicians, big fires, scandals involving celebrities, and epidemics, but no football, thank you. Sometimes it is even possible to have the selected articles downloaded to your hard disk while you sleep or printed on your printer just before
breakfast. As this trend continues, it will cause massive unemployment among
12-year-old paperboys, but newspapers like it because distribution has always
been the weakest link in the whole production chain.
The next step beyond newspapers (plus magazines and scientific journals) is
the on-line digital library. Many professional organizations, such as the ACM
(www.acm.org) and the IEEE Computer Society (www.computer.org), already
have many journals and conference proceedings on-line. Other groups are following rapidly. Depending on the cost, size, and weight of book-sized notebook computers, printed books may become obsolete. Skeptics should take note of the
effect the printing press had on the medieval illuminated manuscript.
All of the above applications involve interactions between a person and a
remote database full of information. The second broad category of network use is
person-to-person communication, basically the 21st century’s answer to the 19th
century’s telephone. E-mail is already used on a daily basis by millions of people
all over the world and its use is growing rapidly. It already routinely contains
audio and video as well as text and pictures. Smell may take a while.

SEC. 1.1

USES OF COMPUTER NETWORKS

7

Any teenager worth his or her salt is addicted to instant messaging. This
facility, derived from the UNIX talk program in use since around 1970, allows two
people to type messages at each other in real time. A multiperson version of this
idea is the chat room, in which a group of people can type messages for all to see.
Worldwide newsgroups, with discussions on every conceivable topic, are
already commonplace among a select group of people, and this phenomenon will
grow to include the population at large. These discussions, in which one person
posts a message and all the other subscribers to the newsgroup can read it, run the
gamut from humorous to impassioned. Unlike chat rooms, newsgroups are not
real time and messages are saved so that when someone comes back from vacation, all messages that have been posted in the meanwhile are patiently waiting for
reading.
Another type of person-to-person communication often goes by the name of
peer-to-peer communication, to distinguish it from the client-server model
(Parameswaran et al., 2001). In this form, individuals who form a loose group can
communicate with others in the group, as shown in Fig. 1-3. Every person can, in
principle, communicate with one or more other people; there is no fixed division
into clients and servers.

Figure 1-3. In a peer-to-peer system there are no fixed clients and servers.

Peer-to-peer communication really hit the big time around 2000 with a service
called Napster, which at its peak had over 50 million music fans swapping music,
in what was probably the biggest copyright infringement in all of recorded history
(Lam and Tan, 2001; and Macedonia, 2000). The idea was fairly simple.
Members registered the music they had on their hard disks in a central database
maintained on the Napster server. If a member wanted a song, he checked the
database to see who had it and went directly there to get it. By not actually keeping any music on its machines, Napster argued that it was not infringing anyone’s
copyright. The courts did not agree and shut it down.

8

INTRODUCTION

CHAP. 1

However, the next generation of peer-to-peer systems eliminates the central
database by having each user maintain his own database locally, as well as providing a list of other nearby people who are members of the system. A new user
can then go to any existing member to see what he has and get a list of other
members to inspect for more music and more names. This lookup process can be
repeated indefinitely to build up a large local database of what is out there. It is
an activity that would get tedious for people but is one at which computers excel.
Legal applications for peer-to-peer communication also exist. For example,
fans sharing public domain music or sample tracks that new bands have released
for publicity purposes, families sharing photos, movies, and genealogical information, and teenagers playing multiperson on-line games. In fact, one of the most popular Internet applications of all, e-mail, is inherently peer-to-peer. This form of
communication is expected to grow considerably in the future.
Electronic crime is not restricted to copyright law. Another hot area is electronic gambling. Computers have been simulating things for decades. Why not
simulate slot machines, roulette wheels, blackjack dealers, and more gambling
equipment? Well, because it is illegal in a lot of places. The trouble is, gambling
is legal in a lot of other places (England, for example) and casino owners there
have grasped the potential for Internet gambling. What happens if the gambler
and the casino are in different countries, with conflicting laws? Good question.
Other communication-oriented applications include using the Internet to carry
telephone calls, video phone, and Internet radio, three rapidly growing areas. Another application is telelearning, meaning attending 8 A.M. classes without the inconvenience of having to get out of bed first. In the long run, the use of networks
to enhance human-to-human communication may prove more important than any
of the others.
Our third category is entertainment, which is a huge and growing industry.
The killer application here (the one that may drive all the rest) is video on
demand. A decade or so hence, it may be possible to select any movie or television program ever made, in any country, and have it displayed on your screen
instantly. New films may become interactive, where the user is occasionally
prompted for the story direction (should Macbeth murder Duncan or just bide his
time?) with alternative scenarios provided for all cases. Live television may also
become interactive, with the audience participating in quiz shows, choosing
among contestants, and so on.
On the other hand, maybe the killer application will not be video on demand.
Maybe it will be game playing. Already we have multiperson real-time simulation games, like hide-and-seek in a virtual dungeon, and flight simulators with the
players on one team trying to shoot down the players on the opposing team. If
games are played with goggles and three-dimensional real-time, photographicquality moving images, we have a kind of worldwide shared virtual reality.
Our fourth category is electronic commerce in the broadest sense of the term.
Home shopping is already popular and enables users to inspect the on-line cata-

SEC. 1.1

USES OF COMPUTER NETWORKS

9

logs of thousands of companies. Some of these catalogs will soon provide the
ability to get an instant video on any product by just clicking on the product’s
name. After the customer buys a product electronically but cannot figure out how
to use it, on-line technical support may be consulted.
Another area in which e-commerce is already happening is access to financial
institutions. Many people already pay their bills, manage their bank accounts, and
handle their investments electronically. This will surely grow as networks become more secure.
One area that virtually nobody foresaw is electronic flea markets (e-flea?).
On-line auctions of second-hand goods have become a massive industry. Unlike
traditional e-commerce, which follows the client-server model, on-line auctions
are more of a peer-to-peer system, sort of consumer-to-consumer. Some of these
forms of e-commerce have acquired cute little tags based on the fact that ‘‘to’’ and
‘‘2’’ are pronounced the same. The most popular ones are listed in Fig. 1-4.
222222222222222222222222222222222222222222222222222222222222222222222222222222
1 Tag 1
1
1
Full name
Example
2
1 22222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
B2C 1 Business-to-consumer
1222222222222222222222222222222222222222222222222222222222222222222222222222222
1 Ordering books on-line
1
1 B2B 1 Business-to-business
1 Car manufacturer ordering tires from supplier
1
1222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1222222222222222222222222222222222222222222222222222222222222222222222222222222
G2C 1 Government-to-consumer 1 Government distributing tax forms electronically 1
1
1
1
1
C2C 1 Consumer-to-consumer
1222222222222222222222222222222222222222222222222222222222222222222222222222222
1 Auctioning second-hand products on line
1
11222222222222222222222222222222222222222222222222222222222222222222222222222222
11 File sharing
11
P2P 11 Peer-to-peer

Figure 1-4. Some forms of e-commerce.

No doubt the range of uses of computer networks will grow rapidly in the
future, and probably in ways no one can now foresee. After all, how many people
in 1990 predicted that teenagers tediously typing short text messages on mobile
phones while riding buses would be an immense money maker for telephone companies in 10 years? But short message service is very profitable.
Computer networks may become hugely important to people who are geographically challenged, giving them the same access to services as people living
in the middle of a big city. Telelearning may radically affect education; universities may go national or international. Telemedicine is only now starting to catch
on (e.g., remote patient monitoring) but may become much more important. But
the killer application may be something mundane, like using the webcam in your
refrigerator to see if you have to buy milk on the way home from work.

1.1.3 Mobile Users
Mobile computers, such as notebook computers and personal digital assistants
(PDAs), are one of the fastest-growing segments of the computer industry. Many
owners of these computers have desktop machines back at the office and want to
be connected to their home base even when away from home or en route. Since

10

INTRODUCTION

CHAP. 1

having a wired connection is impossible in cars and airplanes, there is a lot of
interest in wireless networks. In this section we will briefly look at some of the
uses of wireless networks.
Why would anyone want one? A common reason is the portable office. People on the road often want to use their portable electronic equipment to send and
receive telephone calls, faxes, and electronic mail, surf the Web, access remote
files, and log on to remote machines. And they want to do this from anywhere on
land, sea, or air. For example, at computer conferences these days, the organizers
often set up a wireless network in the conference area. Anyone with a notebook
computer and a wireless modem can just turn the computer on and be connected
to the Internet, as though the computer were plugged into a wired network. Similarly, some universities have installed wireless networks on campus so students
can sit under the trees and consult the library’s card catalog or read their e-mail.
Wireless networks are of great value to fleets of trucks, taxis, delivery vehicles, and repairpersons for keeping in contact with home. For example, in many
cities, taxi drivers are independent businessmen, rather than being employees of a
taxi company. In some of these cities, the taxis have a display the driver can see.
When a customer calls up, a central dispatcher types in the pickup and destination
points. This information is displayed on the drivers’ displays and a beep sounds.
The first driver to hit a button on the display gets the call.
Wireless networks are also important to the military. If you have to be able to
fight a war anywhere on earth on short notice, counting on using the local networking infrastructure is probably not a good idea. It is better to bring your own.
Although wireless networking and mobile computing are often related, they
are not identical, as Fig. 1-5 shows. Here we see a distinction between fixed
wireless and mobile wireless. Even notebook computers are sometimes wired.
For example, if a traveler plugs a notebook computer into the telephone jack in a
hotel room, he has mobility without a wireless network.
222222222222222222222222222222222222222222222222222222222222
1
1 Wireless 1 Mobile 1
Applications
222222222222222222222222222222222222222222222222222222222222
1
1
1
1
No
1 Desktop computers in offices
1
1222222222222222222222222222222222222222222222222222222222222
1 No
1 A notebook computer used in a hotel room 1
1 No
1 Yes
222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1 Networks in older, unwired buildings
1
1222222222222222222222222222222222222222222222222222222222222
1 No
Yes
1
1
1
1
Yes
Yes
Portable
office;
PDA
for
store
inventory
1
222222222222222222222222222222222222222222222222222222222222
1
1
1

Figure 1-5. Combinations of wireless networks and mobile computing.

On the other hand, some wireless computers are not mobile. An important
example is a company that owns an older building lacking network cabling, and
which wants to connect its computers. Installing a wireless network may require
little more than buying a small box with some electronics, unpacking it, and plugging it in. This solution may be far cheaper than having workmen put in cable
ducts to wire the building.

SEC. 1.1

USES OF COMPUTER NETWORKS

11

But of course, there are also the true mobile, wireless applications, ranging
from the portable office to people walking around a store with a PDA doing
inventory. At many busy airports, car rental return clerks work in the parking lot
with wireless portable computers. They type in the license plate number of
returning cars, and their portable, which has a built-in printer, calls the main computer, gets the rental information, and prints out the bill on the spot.
As wireless technology becomes more widespread, numerous other applications are likely to emerge. Let us take a quick look at some of the possibilities.
Wireless parking meters have advantages for both users and city governments.
The meters could accept credit or debit cards with instant verification over the
wireless link. When a meter expires, it could check for the presence of a car (by
bouncing a signal off it) and report the expiration to the police. It has been estimated that city governments in the U.S. alone could collect an additional $10 billion this way (Harte et al., 2000). Furthermore, better parking enforcement would
help the environment, as drivers who knew their illegal parking was sure to be
caught might use public transport instead.
Food, drink, and other vending machines are found everywhere. However,
the food does not get into the machines by magic. Periodically, someone comes by
with a truck to fill them. If the vending machines issued a wireless report once a
day announcing their current inventories, the truck driver would know which
machines needed servicing and how much of which product to bring. This information could lead to more efficient route planning. Of course, this information
could be sent over a standard telephone line as well, but giving every vending
machine a fixed telephone connection for one call a day is expensive on account
of the fixed monthly charge.
Another area in which wireless could save money is utility meter reading. If
electricity, gas, water, and other meters in people’s homes were to report usage
over a wireless network, there would be no need to send out meter readers. Similarly, wireless smoke detectors could call the fire department instead of making a
big noise (which has little value if no one is home). As the cost of both the radio
devices and the air time drops, more and more measurement and reporting will be
done with wireless networks.
A whole different application area for wireless networks is the expected
merger of cell phones and PDAs into tiny wireless computers. A first attempt was
tiny wireless PDAs that could display stripped-down Web pages on their even
tinier screens. This system, called WAP 1.0 (Wireless Application Protocol)
failed, mostly due to the microscopic screens, low bandwidth, and poor service.
But newer devices and services will be better with WAP 2.0.
One area in which these devices may excel is called m-commerce (mobilecommerce) (Senn, 2000). The driving force behind this phenomenon consists of
an amalgam of wireless PDA manufacturers and network operators who are trying
hard to figure out how to get a piece of the e-commerce pie. One of their hopes is
to use wireless PDAs for banking and shopping. One idea is to use the wireless

12

INTRODUCTION

CHAP. 1

PDAs as a kind of electronic wallet, authorizing payments in stores, as a replacement for cash and credit cards. The charge then appears on the mobile phone bill.
From the store’s point of view, this scheme may save them most of the credit card
company’s fee, which can be several percent. Of course, this plan may backfire,
since customers in a store might use their PDAs to check out competitors’ prices
before buying. Worse yet, telephone companies might offer PDAs with bar code
readers that allow a customer to scan a product in a store and then instantaneously
get a detailed report on where else it can be purchased and at what price.
Since the network operator knows where the user is, some services are intentionally location dependent. For example, it may be possible to ask for a nearby
bookstore or Chinese restaurant. Mobile maps are another candidate. So are very
local weather forecasts (‘‘When is it going to stop raining in my backyard?’’). No
doubt many other applications appear as these devices become more widespread.
One huge thing that m-commerce has going for it is that mobile phone users
are accustomed to paying for everything (in contrast to Internet users, who expect
everything to be free). If an Internet Web site charged a fee to allow its customers
to pay by credit card, there would be an immense howling noise from the users. If
a mobile phone operator allowed people to pay for items in a store by using the
phone and then tacked on a fee for this convenience, it would probably be
accepted as normal. Time will tell.
A little further out in time are personal area networks and wearable computers. IBM has developed a watch that runs Linux (including the X11 windowing
system) and has wireless connectivity to the Internet for sending and receiving email (Narayanaswami et al., 2002). In the future, people may exchange business
cards just by exposing their watches to each other. Wearable wireless computers
may give people access to secure rooms the same way magnetic stripe cards do
now (possibly in combination with a PIN code or biometric measurement). These
watches may also be able to retrieve information relevant to the user’s current
location (e.g., local restaurants). The possibilities are endless.
Smart watches with radios have been part of our mental space since their
appearance in the Dick Tracy comic strip in 1946. But smart dust? Researchers
at Berkeley have packed a wireless computer into a cube 1 mm on edge (Warneke
et al., 2001). Potential applications include tracking inventory, packages, and
even small birds, rodents, and insects.

1.1.4 Social Issues
The widespread introduction of networking has introduced new social, ethical,
and political problems. Let us just briefly mention a few of them; a thorough
study would require a full book, at least. A popular feature of many networks are
newsgroups or bulletin boards whereby people can exchange messages with likeminded individuals. As long as the subjects are restricted to technical topics or
hobbies like gardening, not too many problems will arise.

SEC. 1.1

USES OF COMPUTER NETWORKS

13

The trouble comes when newsgroups are set up on topics that people actually
care about, like politics, religion, or sex. Views posted to such groups may be
deeply offensive to some people. Worse yet, they may not be politically correct.
Furthermore, messages need not be limited to text. High-resolution color photographs and even short video clips can now easily be transmitted over computer
networks. Some people take a live-and-let-live view, but others feel that posting
certain material (e.g., attacks on particular countries or religions, pornography,
etc.) is simply unacceptable and must be censored. Different countries have different and conflicting laws in this area. Thus, the debate rages.
People have sued network operators, claiming that they are responsible for the
contents of what they carry, just as newspapers and magazines are. The inevitable
response is that a network is like a telephone company or the post office and cannot be expected to police what its users say. Stronger yet, were network operators
to censor messages, they would likely delete everything containing even the
slightest possibility of them being sued, and thus violate their users’ rights to free
speech. It is probably safe to say that this debate will go on for a while.
Another fun area is employee rights versus employer rights. Many people
read and write e-mail at work. Many employers have claimed the right to read
and possibly censor employee messages, including messages sent from a home
computer after work. Not all employees agree with this.
Even if employers have power over employees, does this relationship also
govern universities and students? How about high schools and students? In 1994,
Carnegie-Mellon University decided to turn off the incoming message stream for
several newsgroups dealing with sex because the university felt the material was
inappropriate for minors (i.e., those few students under 18). The fallout from this
event took years to settle.
Another key topic is government versus citizen. The FBI has installed a system at many Internet service providers to snoop on all incoming and outgoing email for nuggets of interest to it (Blaze and Bellovin, 2000; Sobel, 2001; and
Zacks, 2001). The system was originally called Carnivore but bad publicity
caused it to be renamed to the more innocent-sounding DCS1000. But its goal is
still to spy on millions of people in the hope of finding information about illegal
activities. Unfortunately, the Fourth Amendment to the U.S. Constitution prohibits government searches without a search warrant. Whether these 54 words, written in the 18th century, still carry any weight in the 21st century is a matter that
may keep the courts busy until the 22nd century.
The government does not have a monopoly on threatening people’s privacy.
The private sector does its bit too. For example, small files called cookies that
Web browsers store on users’ computers allow companies to track users’ activities
in cyberspace and also may allow credit card numbers, social security numbers,
and other confidential information to leak all over the Internet (Berghel, 2001).
Computer networks offer the potential for sending anonymous messages. In
some situations, this capability may be desirable. For example, it provides a way

14

INTRODUCTION

CHAP. 1

for students, soldiers, employees, and citizens to blow the whistle on illegal behavior on the part of professors, officers, superiors, and politicians without fear of
reprisals. On the other hand, in the United States and most other democracies, the
law specifically permits an accused person the right to confront and challenge his
accuser in court. Anonymous accusations cannot be used as evidence.
In short, computer networks, like the printing press 500 years ago, allow ordinary citizens to distribute their views in different ways and to different audiences
than were previously possible. This new-found freedom brings with it many
unsolved social, political, and moral issues.
Along with the good comes the bad. Life seems to be like that. The Internet
makes it possible to find information quickly, but a lot of it is ill-informed,
misleading, or downright wrong. The medical advice you plucked from the Internet may have come from a Nobel Prize winner or from a high school dropout.
Computer networks have also introduced new kinds of antisocial and criminal
behavior. Electronic junk mail (spam) has become a part of life because people
have collected millions of e-mail addresses and sell them on CD-ROMs to
would-be marketeers. E-mail messages containing active content (basically programs or macros that execute on the receiver’s machine) can contain viruses that
wreak havoc.
Identity theft is becoming a serious problem as thieves collect enough information about a victim to obtain get credit cards and other documents in the victim’s name. Finally, being able to transmit music and video digitally has opened
the door to massive copyright violations that are hard to catch and enforce.
A lot of these problems could be solved if the computer industry took computer security seriously. If all messages were encrypted and authenticated, it
would be harder to commit mischief. This technology is well established and we
will study it in detail in Chap. 8. The problem is that hardware and software vendors know that putting in security features costs money and their customers are
not demanding such features. In addition, a substantial number of the problems
are caused by buggy software, which occurs because vendors keep adding more
and more features to their programs, which inevitably means more code and thus
more bugs. A tax on new features might help, but that is probably a tough sell in
some quarters. A refund for defective software might be nice, except it would
bankrupt the entire software industry in the first year.

1.2 NETWORK HARDWARE
It is now time to turn our attention from the applications and social aspects of
networking (the fun stuff) to the technical issues involved in network design (the
work stuff). There is no generally accepted taxonomy into which all computer
networks fit, but two dimensions stand out as important: transmission technology
and scale. We will now examine each of these in turn.

SEC. 1.2

NETWORK HARDWARE

15

Broadly speaking, there are two types of transmission technology that are in
widespread use. They are as follows:
1. Broadcast links.
2. Point-to-point links.
Broadcast networks have a single communication channel that is shared by all
the machines on the network. Short messages, called packets in certain contexts,
sent by any machine are received by all the others. An address field within the
packet specifies the intended recipient. Upon receiving a packet, a machine
checks the address field. If the packet is intended for the receiving machine, that
machine processes the packet; if the packet is intended for some other machine, it
is just ignored.
As an analogy, consider someone standing at the end of a corridor with many
rooms off it and shouting ‘‘Watson, come here. I want you.’’ Although the packet
may actually be received (heard) by many people, only Watson responds. The
others just ignore it. Another analogy is an airport announcement asking all flight
644 passengers to report to gate 12 for immediate boarding.
Broadcast systems generally also allow the possibility of addressing a packet
to all destinations by using a special code in the address field. When a packet
with this code is transmitted, it is received and processed by every machine on the
network. This mode of operation is called broadcasting. Some broadcast systems also support transmission to a subset of the machines, something known as
multicasting. One possible scheme is to reserve one bit to indicate multicasting.
The remaining n − 1 address bits can hold a group number. Each machine can
‘‘subscribe’’ to any or all of the groups. When a packet is sent to a certain group,
it is delivered to all machines subscribing to that group.
In contrast, point-to-point networks consist of many connections between individual pairs of machines. To go from the source to the destination, a packet on
this type of network may have to first visit one or more intermediate machines.
Often multiple routes, of different lengths, are possible, so finding good ones is
important in point-to-point networks. As a general rule (although there are many
exceptions), smaller, geographically localized networks tend to use broadcasting,
whereas larger networks usually are point-to-point. Point-to-point transmission
with one sender and one receiver is sometimes called unicasting.
An alternative criterion for classifying networks is their scale. In Fig. 1-6 we
classify multiple processor systems by their physical size. At the top are the personal area networks, networks that are meant for one person. For example, a
wireless network connecting a computer with its mouse, keyboard, and printer is a
personal area network. Also, a PDA that controls the user’s hearing aid or
pacemaker fits in this category. Beyond the personal area networks come longerrange networks. These can be divided into local, metropolitan, and wide area networks. Finally, the connection of two or more networks is called an internetwork.

16

INTRODUCTION
Interprocessor
distance

Processors
located in same

1m
10 m
100 m
1 km
10 km
100 km
1000 km
10,000 km

Square meter

CHAP. 1

Example
Personal area network

Room
Building

Local area network

Campus
City

Metropolitan area network

Country
Wide area network
Continent
Planet

The Internet

Figure 1-6. Classification of interconnected processors by scale.

The worldwide Internet is a well-known example of an internetwork. Distance is
important as a classification metric because different techniques are used at different scales. In this book we will be concerned with networks at all these scales.
Below we give a brief introduction to network hardware.

1.2.1 Local Area Networks
Local area networks, generally called LANs, are privately-owned networks
within a single building or campus of up to a few kilometers in size. They are
widely used to connect personal computers and workstations in company offices
and factories to share resources (e.g., printers) and exchange information. LANs
are distinguished from other kinds of networks by three characteristics: (1) their
size, (2) their transmission technology, and (3) their topology.
LANs are restricted in size, which means that the worst-case transmission
time is bounded and known in advance. Knowing this bound makes it possible to
use certain kinds of designs that would not otherwise be possible. It also simplifies network management.
LANs may use a transmission technology consisting of a cable to which all
the machines are attached, like the telephone company party lines once used in
rural areas. Traditional LANs run at speeds of 10 Mbps to 100 Mbps, have low
delay (microseconds or nanoseconds), and make very few errors. Newer LANs
operate at up to 10 Gbps. In this book, we will adhere to tradition and measure
line speeds in megabits/sec (1 Mbps is 1,000,000 bits/sec) and gigabits/sec (1
Gbps is 1,000,000,000 bits/sec).
Various topologies are possible for broadcast LANs. Figure 1-7 shows two of
them. In a bus (i.e., a linear cable) network, at any instant at most one machine is

SEC. 1.2

17

NETWORK HARDWARE

the master and is allowed to transmit. All other machines are required to refrain
from sending. An arbitration mechanism is needed to resolve conflicts when two
or more machines want to transmit simultaneously. The arbitration mechanism
may be centralized or distributed. IEEE 802.3, popularly called Ethernet, for
example, is a bus-based broadcast network with decentralized control, usually
operating at 10 Mbps to 10 Gbps. Computers on an Ethernet can transmit whenever they want to; if two or more packets collide, each computer just waits a random time and tries again later.

Computer

Cable
(a)

Computer

(b)

Figure 1-7. Two broadcast networks. (a) Bus. (b) Ring.

A second type of broadcast system is the ring. In a ring, each bit propagates
around on its own, not waiting for the rest of the packet to which it belongs. Typically, each bit circumnavigates the entire ring in the time it takes to transmit a few
bits, often before the complete packet has even been transmitted. As with all
other broadcast systems, some rule is needed for arbitrating simultaneous accesses
to the ring. Various methods, such as having the machines take turns, are in use.
IEEE 802.5 (the IBM token ring), is a ring-based LAN operating at 4 and 16
Mbps. FDDI is another example of a ring network.
Broadcast networks can be further divided into static and dynamic, depending
on how the channel is allocated. A typical static allocation would be to divide
time into discrete intervals and use a round-robin algorithm, allowing each machine to broadcast only when its time slot comes up. Static allocation wastes
channel capacity when a machine has nothing to say during its allocated slot, so
most systems attempt to allocate the channel dynamically (i.e., on demand).
Dynamic allocation methods for a common channel are either centralized or
decentralized. In the centralized channel allocation method, there is a single entity, for example, a bus arbitration unit, which determines who goes next. It might
do this by accepting requests and making a decision according to some internal
algorithm. In the decentralized channel allocation method, there is no central
entity; each machine must decide for itself whether to transmit. You might think
that this always leads to chaos, but it does not. Later we will study many algorithms designed to bring order out of the potential chaos.

18

INTRODUCTION

CHAP. 1

1.2.2 Metropolitan Area Networks
A metropolitan area network, or MAN, covers a city. The best-known
example of a MAN is the cable television network available in many cities. This
system grew from earlier community antenna systems used in areas with poor
over-the-air television reception. In these early systems, a large antenna was
placed on top of a nearby hill and signal was then piped to the subscribers’ houses.
At first, these were locally-designed, ad hoc systems. Then companies began
jumping into the business, getting contracts from city governments to wire up an
entire city. The next step was television programming and even entire channels
designed for cable only. Often these channels were highly specialized, such as all
news, all sports, all cooking, all gardening, and so on. But from their inception
until the late 1990s, they were intended for television reception only.
Starting when the Internet attracted a mass audience, the cable TV network
operators began to realize that with some changes to the system, they could provide two-way Internet service in unused parts of the spectrum. At that point, the
cable TV system began to morph from a way to distribute television to a metropolitan area network. To a first approximation, a MAN might look something like
the system shown in Fig. 1-8. In this figure we see both television signals and
Internet being fed into the centralized head end for subsequent distribution to
people’s homes. We will come back to this subject in detail in Chap. 2.

Junction
box
Antenna

Head end

Internet

Figure 1-8. A metropolitan area network based on cable TV.

Cable television is not the only MAN. Recent developments in high-speed
wireless Internet access resulted in another MAN, which has been standardized as
IEEE 802.16. We will look at this area in Chap. 2.

SEC. 1.2

19

NETWORK HARDWARE

1.2.3 Wide Area Networks
A wide area network, or WAN, spans a large geographical area, often a
country or continent. It contains a collection of machines intended for running
user (i.e., application) programs. We will follow traditional usage and call these
machines hosts. The hosts are connected by a communication subnet, or just
subnet for short. The hosts are owned by the customers (e.g., people’s personal
computers), whereas the communication subnet is typically owned and operated
by a telephone company or Internet service provider. The job of the subnet is to
carry messages from host to host, just as the telephone system carries words from
speaker to listener. Separation of the pure communication aspects of the network
(the subnet) from the application aspects (the hosts), greatly simplifies the complete network design.
In most wide area networks, the subnet consists of two distinct components:
transmission lines and switching elements. Transmission lines move bits between machines. They can be made of copper wire, optical fiber, or even radio
links. Switching elements are specialized computers that connect three or more
transmission lines. When data arrive on an incoming line, the switching element
must choose an outgoing line on which to forward them. These switching computers have been called by various names in the past; the name router is now
most commonly used. Unfortunately, some people pronounce it ‘‘rooter’’ and others have it rhyme with ‘‘doubter.’’ Determining the correct pronunciation will be
left as an exercise for the reader. (Note: the perceived correct answer may depend
on where you live.)
In this model, shown in Fig. 1-9, each host is frequently connected to a LAN
on which a router is present, although in some cases a host can be connected
directly to a router. The collection of communication lines and routers (but not
the hosts) form the subnet.
Subnet

Router

Host

LAN

Figure 1-9. Relation between hosts on LANs and the subnet.

A short comment about the term ‘‘subnet’’ is in order here. Originally, its
only meaning was the collection of routers and communication lines that moved

20

INTRODUCTION

CHAP. 1

packets from the source host to the destination host. However, some years later, it
also acquired a second meaning in conjunction with network addressing (which
we will discuss in Chap. 5). Unfortunately, no widely-used alternative exists for
its initial meaning, so with some hesitation we will use it in both senses. From the
context, it will always be clear which is meant.
In most WANs, the network contains numerous transmission lines, each one
connecting a pair of routers. If two routers that do not share a transmission line
wish to communicate, they must do this indirectly, via other routers. When a
packet is sent from one router to another via one or more intermediate routers, the
packet is received at each intermediate router in its entirety, stored there until the
required output line is free, and then forwarded. A subnet organized according to
this principle is called a store-and-forward or packet-switched subnet. Nearly
all wide area networks (except those using satellites) have store-and-forward subnets. When the packets are small and all the same size, they are often called cells.
The principle of a packet-switched WAN is so important that it is worth
devoting a few more words to it. Generally, when a process on some host has a
message to be sent to a process on some other host, the sending host first cuts the
message into packets, each one bearing its number in the sequence. These packets are then injected into the network one at a time in quick succession. The packets are transported individually over the network and deposited at the receiving
host, where they are reassembled into the original message and delivered to the
receiving process. A stream of packets resulting from some initial message is
illustrated in Fig. 1-10.
Router

Subnet

Sending host

Receiving host
B

D

A

E
C

Sending process

Packet

Router C makes a
choice to forward
packets to E and
not to D

Receiving process

Figure 1-10. A stream of packets from sender to receiver.

In this figure, all the packets follow the route ACE, rather than ABDE or
ACDE. In some networks all packets from a given message must follow the same
route; in others each packet is routed separately. Of course, if ACE is the best
route, all packets may be sent along it, even if each packet is individually routed.
Routing decisions are made locally. When a packet arrives at router A, it is
up to A to decide if this packet should be sent on the line to B or the line to C.
How A makes that decision is called the routing algorithm. Many of them exist.
We will study some of them in detail in Chap. 5.

SEC. 1.2

NETWORK HARDWARE

21

Not all WANs are packet switched. A second possibility for a WAN is a
satellite system. Each router has an antenna through which it can send and
receive. All routers can hear the output from the satellite, and in some cases they
can also hear the upward transmissions of their fellow routers to the satellite as
well. Sometimes the routers are connected to a substantial point-to-point subnet,
with only some of them having a satellite antenna. Satellite networks are inherently broadcast and are most useful when the broadcast property is important.

1.2.4 Wireless Networks
Digital wireless communication is not a new idea. As early as 1901, the Italian physicist Guglielmo Marconi demonstrated a ship-to-shore wireless telegraph,
using Morse Code (dots and dashes are binary, after all). Modern digital wireless
systems have better performance, but the basic idea is the same.
To a first approximation, wireless networks can be divided into three main
categories:
1. System interconnection.
2. Wireless LANs.
3. Wireless WANs.
System interconnection is all about interconnecting the components of a computer
using short-range radio. Almost every computer has a monitor, keyboard, mouse,
and printer connected to the main unit by cables. So many new users have a hard
time plugging all the cables into the right little holes (even though they are usually
color coded) that most computer vendors offer the option of sending a technician
to the user’s home to do it. Consequently, some companies got together to design
a short-range wireless network called Bluetooth to connect these components
without wires. Bluetooth also allows digital cameras, headsets, scanners, and
other devices to connect to a computer by merely being brought within range. No
cables, no driver installation, just put them down, turn them on, and they work.
For many people, this ease of operation is a big plus.
In the simplest form, system interconnection networks use the master-slave
paradigm of Fig. 1-11(a). The system unit is normally the master, talking to the
mouse, keyboard, etc., as slaves. The master tells the slaves what addresses to
use, when they can broadcast, how long they can transmit, what frequencies they
can use, and so on. We will discuss Bluetooth in more detail in Chap. 4.
The next step up in wireless networking are the wireless LANs. These are
systems in which every computer has a radio modem and antenna with which it
can communicate with other systems. Often there is an antenna on the ceiling that
the machines talk to, as shown in Fig. 1-11(b). However, if the systems are close
enough, they can communicate directly with one another in a peer-to-peer configuration. Wireless LANs are becoming increasingly common in small offices and

22

INTRODUCTION

CHAP. 1

Base
station

(a)

To wired network

(b)

Figure 1-11. (a) Bluetooth configuration. (b) Wireless LAN.

homes, where installing Ethernet is considered too much trouble, as well as in
older office buildings, company cafeterias, conference rooms, and other places.
There is a standard for wireless LANs, called IEEE 802.11, which most systems
implement and which is becoming very widespread. We will discuss it in Chap. 4.
The third kind of wireless network is used in wide area systems. The radio
network used for cellular telephones is an example of a low-bandwidth wireless
system. This system has already gone through three generations. The first generation was analog and for voice only. The second generation was digital and for
voice only. The third generation is digital and is for both voice and data. In a certain sense, cellular wireless networks are like wireless LANs, except that the distances involved are much greater and the bit rates much lower. Wireless LANs
can operate at rates up to about 50 Mbps over distances of tens of meters. Cellular systems operate below 1 Mbps, but the distance between the base station and
the computer or telephone is measured in kilometers rather than in meters. We
will have a lot to say about these networks in Chap. 2.
In addition to these low-speed networks, high-bandwidth wide area wireless
networks are also being developed. The initial focus is high-speed wireless Internet access from homes and businesses, bypassing the telephone system. This
service is often called local multipoint distribution service. We will study it later
in the book. A standard for it, called IEEE 802.16, has also been developed. We
will examine the standard in Chap. 4.
Almost all wireless networks hook up to the wired network at some point to
provide access to files, databases, and the Internet. There are many ways these
connections can be realized, depending on the circumstances. For example, in
Fig. 1-12(a), we depict an airplane with a number of people using modems and
seat-back telephones to call the office. Each call is independent of the other ones.
A much more efficient option, however, is the flying LAN of Fig. 1-12(b). Here

SEC. 1.2

23

NETWORK HARDWARE

each seat comes equipped with an Ethernet connector into which passengers can
plug their computers. A single router on the aircraft maintains a radio link with
some router on the ground, changing routers as it flies along. This configuration
is just a traditional LAN, except that its connection to the outside world happens
to be a radio link instead of a hardwired line.
Flying router

Wired
LAN

Portable
computer

One telephone
call per computer
(a)

(b)

Figure 1-12. (a) Individual mobile computers. (b) A flying LAN.

Many people believe wireless is the wave of the future (e.g., Bi et al., 2001;
Leeper, 2001; Varshey and Vetter, 2000) but at least one dissenting voice has
been heard. Bob Metcalfe, the inventor of Ethernet, has written: ‘‘Mobile wireless computers are like mobile pipeless bathrooms—portapotties. They will be
common on vehicles, and at construction sites, and rock concerts. My advice is to
wire up your home and stay there’’ (Metcalfe, 1995). History may record this
remark in the same category as IBM’s chairman T.J. Watson’s 1945 explanation
of why IBM was not getting into the computer business: ‘‘Four or five computers
should be enough for the entire world until the year 2000.’’

1.2.5 Home Networks
Home networking is on the horizon. The fundamental idea is that in the future most homes will be set up for networking. Every device in the home will be
capable of communicating with every other device, and all of them will be accessible over the Internet. This is one of those visionary concepts that nobody
asked for (like TV remote controls or mobile phones), but once they arrived
nobody can imagine how they lived without them.
Many devices are capable of being networked. Some of the more obvious
categories (with examples) are as follows:
1. Computers (desktop PC, notebook PC, PDA, shared peripherals).
2. Entertainment (TV, DVD, VCR, camcorder, camera, stereo, MP3).
3. Telecommunications (telephone, mobile telephone, intercom, fax).
4. Appliances (microwave, refrigerator, clock, furnace, airco, lights).
5. Telemetry (utility meter, smoke/burglar alarm, thermostat, babycam).

24

INTRODUCTION

CHAP. 1

Home computer networking is already here in a limited way. Many homes already have a device to connect multiple computers to a fast Internet connection.
Networked entertainment is not quite here, but as more and more music and
movies can be downloaded from the Internet, there will be a demand to connect
stereos and televisions to it. Also, people will want to share their own videos with
friends and family, so the connection will need to go both ways. Telecommunications gear is already connected to the outside world, but soon it will be digital and
go over the Internet. The average home probably has a dozen clocks (e.g., in
appliances), all of which have to be reset twice a year when daylight saving time
(summer time) comes and goes. If all the clocks were on the Internet, that resetting could be done automatically. Finally, remote monitoring of the home and its
contents is a likely winner. Probably many parents would be willing to spend
some money to monitor their sleeping babies on their PDAs when they are eating
out, even with a rented teenager in the house. While one can imagine a separate
network for each application area, integrating all of them into a single network is
probably a better idea.
Home networking has some fundamentally different properties than other network types. First, the network and devices have to be easy to install. The author
has installed numerous pieces of hardware and software on various computers
over the years, with mixed results. A series of phone calls to the vendor’s helpdesk typically resulted in answers like (1) Read the manual, (2) Reboot the computer, (3) Remove all hardware and software except ours and try again, (4) Download the newest driver from our Web site, and if all else fails, (5) Reformat the
hard disk and then reinstall Windows from the CD-ROM. Telling the purchaser
of an Internet refrigerator to download and install a new version of the refrigerator’s operating system is not going to lead to happy customers. Computer
users are accustomed to putting up with products that do not work; the car-,
television-, and refrigerator-buying public is far less tolerant. They expect products to work for 100% from the word go.
Second, the network and devices have to be foolproof in operation. Air conditioners used to have one knob with four settings: OFF, LOW, MEDIUM, and
HIGH. Now they have 30-page manuals. Once they are networked, expect the
chapter on security alone to be 30 pages. This will be beyond the comprehension
of virtually all the users.
Third, low price is essential for success. People will not pay a $50 premium
for an Internet thermostat because few people regard monitoring their home temperature from work that important. For $5 extra, it might sell, though.
Fourth, the main application is likely to involve multimedia, so the network
needs sufficient capacity. There is no market for Internet-connected televisions
that show shaky movies at 320 × 240 pixel resolution and 10 frames/sec. Fast
Ethernet, the workhorse in most offices, is not good enough for multimedia. Consequently, home networks will need better performance than that of existing
office networks and at lower prices before they become mass market items.

SEC. 1.2

NETWORK HARDWARE

25

Fifth, it must be possible to start out with one or two devices and expand the
reach of the network gradually. This means no format wars. Telling consumers
to buy peripherals with IEEE 1394 (FireWire) interfaces and a few years later
retracting that and saying USB 2.0 is the interface-of-the-month is going to make
consumers skittish. The network interface will have to remain stable for many
years; the wiring (if any) will have to remain stable for decades.
Sixth, security and reliability will be very important. Losing a few files to an
e-mail virus is one thing; having a burglar disarm your security system from his
PDA and then plunder your house is something quite different.
An interesting question is whether home networks will be wired or wireless.
Most homes already have six networks installed: electricity, telephone, cable television, water, gas, and sewer. Adding a seventh one during construction is not
difficult, but retrofitting existing houses is expensive. Cost favors wireless networking, but security favors wired networking. The problem with wireless is that
the radio waves they use are quite good at going through fences. Not everyone is
overjoyed at the thought of having the neighbors piggybacking on their Internet
connection and reading their e-mail on its way to the printer. In Chap. 8 we will
study how encryption can be used to provide security, but in the context of a home
network, security has to be foolproof, even with inexperienced users. This is
easier said than done, even with highly sophisticated users.
In short, home networking offers many opportunities and challenges. Most of
them relate to the need to be easy to manage, dependable, and secure, especially
in the hands of nontechnical users, while at the same time delivering high performance at low cost.

1.2.6 Internetworks
Many networks exist in the world, often with different hardware and software.
People connected to one network often want to communicate with people attached
to a different one. The fulfillment of this desire requires that different, and frequently incompatible networks, be connected, sometimes by means of machines
called gateways to make the connection and provide the necessary translation,
both in terms of hardware and software. A collection of interconnected networks
is called an internetwork or internet. These terms will be used in a generic
sense, in contrast to the worldwide Internet (which is one specific internet), which
we will always capitalize.
A common form of internet is a collection of LANs connected by a WAN. In
fact, if we were to replace the label ‘‘subnet’’ in Fig. 1-9 by ‘‘WAN,’’ nothing else
in the figure would have to change. The only real technical distinction between a
subnet and a WAN in this case is whether hosts are present. If the system within
the gray area contains only routers, it is a subnet; if it contains both routers and
hosts, it is a WAN. The real differences relate to ownership and use.

26

INTRODUCTION

CHAP. 1

Subnets, networks, and internetworks are often confused. Subnet makes the
most sense in the context of a wide area network, where it refers to the collection
of routers and communication lines owned by the network operator. As an analogy, the telephone system consists of telephone switching offices connected to
one another by high-speed lines, and to houses and businesses by low-speed lines.
These lines and equipment, owned and managed by the telephone company, form
the subnet of the telephone system. The telephones themselves (the hosts in this
analogy) are not part of the subnet. The combination of a subnet and its hosts
forms a network. In the case of a LAN, the cable and the hosts form the network.
There really is no subnet.
An internetwork is formed when distinct networks are interconnected. In our
view, connecting a LAN and a WAN or connecting two LANs forms an internetwork, but there is little agreement in the industry over terminology in this area.
One rule of thumb is that if different organizations paid to construct different parts
of the network and each maintains its part, we have an internetwork rather than a
single network. Also, if the underlying technology is different in different parts
(e.g., broadcast versus point-to-point), we probably have two networks.

1.3 NETWORK SOFTWARE
The first computer networks were designed with the hardware as the main
concern and the software as an afterthought. This strategy no longer works. Network software is now highly structured. In the following sections we examine the
software structuring technique in some detail. The method described here forms
the keystone of the entire book and will occur repeatedly later on.

1.3.1 Protocol Hierarchies
To reduce their design complexity, most networks are organized as a stack of
layers or levels, each one built upon the one below it. The number of layers, the
name of each layer, the contents of each layer, and the function of each layer
differ from network to network. The purpose of each layer is to offer certain services to the higher layers, shielding those layers from the details of how the offered
services are actually implemented. In a sense, each layer is a kind of virtual
machine, offering certain services to the layer above it.
This concept is actually a familiar one and used throughout computer science,
where it is variously known as information hiding, abstract data types, data encapsulation, and object-oriented programming. The fundamental idea is that a particular piece of software (or hardware) provides a service to its users but keeps the
details of its internal state and algorithms hidden from them.
Layer n on one machine carries on a conversation with layer n on another machine. The rules and conventions used in this conversation are collectively known

SEC. 1.3

27

NETWORK SOFTWARE

as the layer n protocol. Basically, a protocol is an agreement between the communicating parties on how communication is to proceed. As an analogy, when a
woman is introduced to a man, she may choose to stick out her hand. He, in turn,
may decide either to shake it or kiss it, depending, for example, on whether she is
an American lawyer at a business meeting or a European princess at a formal ball.
Violating the protocol will make communication more difficult, if not completely
impossible.
A five-layer network is illustrated in Fig. 1-13. The entities comprising the
corresponding layers on different machines are called peers. The peers may be
processes, hardware devices, or even human beings. In other words, it is the peers
that communicate by using the protocol.
Host 1

Host 2
Layer 5 protocol

Layer 5

Layer 5

Layer 4/5 interface
Layer 4

Layer 4 protocol

Layer 4

Layer 3/4 interface
Layer 3

Layer 3 protocol

Layer 3

Layer 2/3 interface
Layer 2

Layer 2 protocol

Layer 2

Layer 1/2 interface
Layer 1

Layer 1 protocol

Layer 1

Physical medium

Figure 1-13. Layers, protocols, and interfaces.

In reality, no data are directly transferred from layer n on one machine to
layer n on another machine. Instead, each layer passes data and control information to the layer immediately below it, until the lowest layer is reached. Below
layer 1 is the physical medium through which actual communication occurs. In
Fig. 1-13, virtual communication is shown by dotted lines and physical communication by solid lines.
Between each pair of adjacent layers is an interface. The interface defines
which primitive operations and services the lower layer makes available to the
upper one. When network designers decide how many layers to include in a network and what each one should do, one of the most important considerations is
defining clean interfaces between the layers. Doing so, in turn, requires that each

28

INTRODUCTION

CHAP. 1

layer perform a specific collection of well-understood functions. In addition to
minimizing the amount of information that must be passed between layers, clearcut interfaces also make it simpler to replace the implementation of one layer with
a completely different implementation (e.g., all the telephone lines are replaced
by satellite channels) because all that is required of the new implementation is
that it offer exactly the same set of services to its upstairs neighbor as the old
implementation did. In fact, it is common that different hosts use different implementations.
A set of layers and protocols is called a network architecture. The specification of an architecture must contain enough information to allow an implementer to write the program or build the hardware for each layer so that it will
correctly obey the appropriate protocol. Neither the details of the implementation
nor the specification of the interfaces is part of the architecture because these are
hidden away inside the machines and not visible from the outside. It is not even
necessary that the interfaces on all machines in a network be the same, provided
that each machine can correctly use all the protocols. A list of protocols used by a
certain system, one protocol per layer, is called a protocol stack. The subjects of
network architectures, protocol stacks, and the protocols themselves are the principal topics of this book.
An analogy may help explain the idea of multilayer communication. Imagine
two philosophers (peer processes in layer 3), one of whom speaks Urdu and
English and one of whom speaks Chinese and French. Since they have no common language, they each engage a translator (peer processes at layer 2), each of
whom in turn contacts a secretary (peer processes in layer 1). Philosopher 1
wishes to convey his affection for oryctolagus cuniculus to his peer. To do so, he
passes a message (in English) across the 2/3 interface to his translator, saying ‘‘I
like rabbits,’’ as illustrated in Fig. 1-14. The translators have agreed on a neutral
language known to both of them, Dutch, so the message is converted to ‘‘Ik vind
konijnen leuk.’’ The choice of language is the layer 2 protocol and is up to the
layer 2 peer processes.
The translator then gives the message to a secretary for transmission, by, for
example, fax (the layer 1 protocol). When the message arrives, it is translated
into French and passed across the 2/3 interface to philosopher 2. Note that each
protocol is completely independent of the other ones as long as the interfaces are
not changed. The translators can switch from Dutch to say, Finnish, at will, provided that they both agree, and neither changes his interface with either layer 1 or
layer 3. Similarly, the secretaries can switch from fax to e-mail or telephone
without disturbing (or even informing) the other layers. Each process may add
some information intended only for its peer. This information is not passed
upward to the layer above.
Now consider a more technical example: how to provide communication to
the top layer of the five-layer network in Fig. 1-15. A message, M, is produced by
an application process running in layer 5 and given to layer 4 for transmission.

SEC. 1.3

29

NETWORK SOFTWARE

Location A
I like
rabbits

Location B

Message

Philosopher

J'aime
bien les
lapins

3

2

1

3

L: Dutch
Ik vind
konijnen
leuk

Fax #--L: Dutch
Ik vind
konijnen
leuk

Information
for the remote
translator

Information
for the remote
secretary

Translator

Secretary

L: Dutch
Ik vind
konijnen
leuk

Fax #--L: Dutch
Ik vind
konijnen
leuk

2

1

Figure 1-14. The philosopher-translator-secretary architecture.

Layer 4 puts a header in front of the message to identify the message and passes
the result to layer 3. The header includes control information, such as sequence
numbers, to allow layer 4 on the destination machine to deliver messages in the
right order if the lower layers do not maintain sequence. In some layers, headers
can also contain sizes, times, and other control fields.
In many networks, there is no limit to the size of messages transmitted in the
layer 4 protocol, but there is nearly always a limit imposed by the layer 3 protocol. Consequently, layer 3 must break up the incoming messages into smaller
units, packets, prepending a layer 3 header to each packet. In this example, M is
split into two parts, M 1 and M 2 .
Layer 3 decides which of the outgoing lines to use and passes the packets to
layer 2. Layer 2 adds not only a header to each piece, but also a trailer, and gives
the resulting unit to layer 1 for physical transmission. At the receiving machine
the message moves upward, from layer to layer, with headers being stripped off as
it progresses. None of the headers for layers below n are passed up to layer n.

30

INTRODUCTION

CHAP. 1

Layer
Layer 5 protocol

M

5

H4

4

Layer 4 protocol

M

M

H4

M

Layer 3
protocol
3

H3 H4 M1

H3 M2

H3 H4 M1

H3 M2

H2 H3 H4 M1 T2

H2 H3 M2 T2

Layer 2
protocol
2 H2 H3 H4 M1 T2

H2 H3 M2 T2

1

Source machine

Destination machine

Figure 1-15. Example information flow supporting virtual communication in layer 5.

The important thing to understand about Fig. 1-15 is the relation between the
virtual and actual communication and the difference between protocols and interfaces. The peer processes in layer 4, for example, conceptually think of their
communication as being ‘‘horizontal,’’ using the layer 4 protocol. Each one is
likely to have a procedure called something like SendToOtherSide and GetFromOtherSide, even though these procedures actually communicate with lower layers
across the 3/4 interface, not with the other side.
The peer process abstraction is crucial to all network design. Using it, the
unmanageable task of designing the complete network can be broken into several
smaller, manageable design problems, namely, the design of the individual layers.
Although Sec. 1.3 is called ‘‘Network Software,’’ it is worth pointing out that
the lower layers of a protocol hierarchy are frequently implemented in hardware
or firmware. Nevertheless, complex protocol algorithms are involved, even if
they are embedded (in whole or in part) in hardware.

1.3.2 Design Issues for the Layers
Some of the key design issues that occur in computer networks are present in
several layers. Below, we will briefly mention some of the more important ones.
Every layer needs a mechanism for identifying senders and receivers. Since a
network normally has many computers, some of which have multiple processes, a

SEC. 1.3

NETWORK SOFTWARE

31

means is needed for a process on one machine to specify with whom it wants to
talk. As a consequence of having multiple destinations, some form of addressing
is needed in order to specify a specific destination.
Another set of design decisions concerns the rules for data transfer. In some
systems, data only travel in one direction; in others, data can go both ways. The
protocol must also determine how many logical channels the connection corresponds to and what their priorities are. Many networks provide at least two logical channels per connection, one for normal data and one for urgent data.
Error control is an important issue because physical communication circuits
are not perfect. Many error-detecting and error-correcting codes are known, but
both ends of the connection must agree on which one is being used. In addition,
the receiver must have some way of telling the sender which messages have been
correctly received and which have not.
Not all communication channels preserve the order of messages sent on them.
To deal with a possible loss of sequencing, the protocol must make explicit provision for the receiver to allow the pieces to be reassembled properly. An obvious
solution is to number the pieces, but this solution still leaves open the question of
what should be done with pieces that arrive out of order.
An issue that occurs at every level is how to keep a fast sender from swamping a slow receiver with data. Various solutions have been proposed and will be
discussed later. Some of them involve some kind of feedback from the receiver to
the sender, either directly or indirectly, about the receiver’s current situation.
Others limit the sender to an agreed-on transmission rate. This subject is called
flow control.
Another problem that must be solved at several levels is the inability of all
processes to accept arbitrarily long messages. This property leads to mechanisms
for disassembling, transmitting, and then reassembling messages. A related issue
is the problem of what to do when processes insist on transmitting data in units
that are so small that sending each one separately is inefficient. Here the solution
is to gather several small messages heading toward a common destination into a
single large message and dismember the large message at the other side.
When it is inconvenient or expensive to set up a separate connection for each
pair of communicating processes, the underlying layer may decide to use the same
connection for multiple, unrelated conversations. As long as this multiplexing
and demultiplexing is done transparently, it can be used by any layer. Multiplexing is needed in the physical layer, for example, where all the traffic for all connections has to be sent over at most a few physical circuits.
When there are multiple paths between source and destination, a route must
be chosen. Sometimes this decision must be split over two or more layers. For
example, to send data from London to Rome, a high-level decision might have to
be made to transit France or Germany based on their respective privacy laws.
Then a low-level decision might have to made to select one of the available circuits based on the current traffic load. This topic is called routing.

32

INTRODUCTION

CHAP. 1

1.3.3 Connection-Oriented and Connectionless Services
Layers can offer two different types of service to the layers above them:
connection-oriented and connectionless. In this section we will look at these two
types and examine the differences between them.
Connection-oriented service is modeled after the telephone system. To talk
to someone, you pick up the phone, dial the number, talk, and then hang up. Similarly, to use a connection-oriented network service, the service user first establishes a connection, uses the connection, and then releases the connection. The
essential aspect of a connection is that it acts like a tube: the sender pushes objects
(bits) in at one end, and the receiver takes them out at the other end. In most
cases the order is preserved so that the bits arrive in the order they were sent.
In some cases when a connection is established, the sender, receiver, and subnet conduct a negotiation about parameters to be used, such as maximum message size, quality of service required, and other issues. Typically, one side makes
a proposal and the other side can accept it, reject it, or make a counterproposal.
In contrast, connectionless service is modeled after the postal system. Each
message (letter) carries the full destination address, and each one is routed
through the system independent of all the others. Normally, when two messages
are sent to the same destination, the first one sent will be the first one to arrive.
However, it is possible that the first one sent can be delayed so that the second
one arrives first.
Each service can be characterized by a quality of service. Some services are
reliable in the sense that they never lose data. Usually, a reliable service is implemented by having the receiver acknowledge the receipt of each message so the
sender is sure that it arrived. The acknowledgement process introduces overhead
and delays, which are often worth it but are sometimes undesirable.
A typical situation in which a reliable connection-oriented service is appropriate is file transfer. The owner of the file wants to be sure that all the bits arrive
correctly and in the same order they were sent. Very few file transfer customers
would prefer a service that occasionally scrambles or loses a few bits, even if it is
much faster.
Reliable connection-oriented service has two minor variations: message sequences and byte streams. In the former variant, the message boundaries are
preserved. When two 1024-byte messages are sent, they arrive as two distinct
1024-byte messages, never as one 2048-byte message. In the latter, the connection is simply a stream of bytes, with no message boundaries. When 2048 bytes
arrive at the receiver, there is no way to tell if they were sent as one 2048-byte
message, two 1024-byte messages, or 2048 1-byte messages. If the pages of a
book are sent over a network to a phototypesetter as separate messages, it might
be important to preserve the message boundaries. On the other hand, when a user
logs into a remote server, a byte stream from the user’s computer to the server is
all that is needed. Message boundaries are not relevant.

SEC. 1.3

33

NETWORK SOFTWARE

As mentioned above, for some applications, the transit delays introduced by
acknowledgements are unacceptable. One such application is digitized voice
traffic. It is preferable for telephone users to hear a bit of noise on the line from
time to time than to experience a delay waiting for acknowledgements. Similarly,
when transmitting a video conference, having a few pixels wrong is no problem,
but having the image jerk along as the flow stops to correct errors is irritating.
Not all applications require connections. For example, as electronic mail becomes more common, electronic junk is becoming more common too. The electronic junk-mail sender probably does not want to go to the trouble of setting up
and later tearing down a connection just to send one item. Nor is 100 percent reliable delivery essential, especially if it costs more. All that is needed is a way to
send a single message that has a high probability of arrival, but no guarantee.
Unreliable (meaning not acknowledged) connectionless service is often called
datagram service, in analogy with telegram service, which also does not return
an acknowledgement to the sender.
In other situations, the convenience of not having to establish a connection to
send one short message is desired, but reliability is essential. The acknowledged
datagram service can be provided for these applications. It is like sending a
registered letter and requesting a return receipt. When the receipt comes back, the
sender is absolutely sure that the letter was delivered to the intended party and not
lost along the way.
Still another service is the request-reply service. In this service the sender
transmits a single datagram containing a request; the reply contains the answer.
For example, a query to the local library asking where Uighur is spoken falls into
this category. Request-reply is commonly used to implement communication in
the client-server model: the client issues a request and the server responds to it.
Figure 1-16 summarizes the types of services discussed above.
Service
Connectionoriented

Connectionless

Example

Reliable message stream

Sequence of pages

Reliable byte stream

Remote login

Unreliable connection

Digitized voice

Unreliable datagram

Electronic junk mail

Acknowledged datagram

Registered mail

Request-reply

Database query

Figure 1-16. Six different types of service.

The concept of using unreliable communication may be confusing at first.
After all, why would anyone actually prefer unreliable communication to reliable

34

INTRODUCTION

CHAP. 1

communication? First of all, reliable communication (in our sense, that is, acknowledged) may not be available. For example, Ethernet does not provide reliable communication. Packets can occasionally be damaged in transit. It is up to
higher protocol levels to deal with this problem. Second, the delays inherent in
providing a reliable service may be unacceptable, especially in real-time applications such as multimedia. For these reasons, both reliable and unreliable communication coexist.

1.3.4 Service Primitives
A service is formally specified by a set of primitives (operations) available to
a user process to access the service. These primitives tell the service to perform
some action or report on an action taken by a peer entity. If the protocol stack is
located in the operating system, as it often is, the primitives are normally system
calls. These calls cause a trap to kernel mode, which then turns control of the
machine over to the operating system to send the necessary packets.
The set of primitives available depends on the nature of the service being provided. The primitives for connection-oriented service are different from those of
connectionless service. As a minimal example of the service primitives that
might be provided to implement a reliable byte stream in a client-server environment, consider the primitives listed in Fig. 1-17.
22222222222222222222222222222222222222222222222222222222
1
1
1
Primitive
Meaning
22222222222222222222222222222222222222222222222222222222
1
1
1
LISTEN
122222222222222222222222222222222222222222222222222222222
1 Block waiting for an incoming connection 1
1 CONNECT
1 Establish a connection with a waiting peer 1
22222222222222222222222222222222222222222222222222222222
1
1
1
122222222222222222222222222222222222222222222222222222222
1 Block waiting for an incoming message
1
RECEIVE
1
1
1
SEND
Send
a
message
to
the
peer
122222222222222222222222222222222222222222222222222222222
1
1
1122222222222222222222222222222222222222222222222222222222
11
DISCONNECT 11 Terminate a connection

Figure 1-17. Five service primitives for implementing a simple connectionoriented service.

These primitives might be used as follows. First, the server executes LISTEN
to indicate that it is prepared to accept incoming connections. A common way to
implement LISTEN is to make it a blocking system call. After executing the primitive, the server process is blocked until a request for connection appears.
Next, the client process executes CONNECT to establish a connection with the
server. The CONNECT call needs to specify who to connect to, so it might have a
parameter giving the server’s address. The operating system then typically sends
a packet to the peer asking it to connect, as shown by (1) in Fig. 1-18. The client
process is suspended until there is a response. When the packet arrives at the
server, it is processed by the operating system there. When the system sees that
the packet is requesting a connection, it checks to see if there is a listener. If so, it

SEC. 1.3

35

NETWORK SOFTWARE

does two things: unblocks the listener and sends back an acknowledgement (2).
The arrival of this acknowledgement then releases the client. At this point the
client and server are both running and they have a connection established. It is
important to note that the acknowledgement (2) is generated by the protocol code
itself, not in response to a user-level primitive. If a connection request arrives and
there is no listener, the result is undefined. In some systems the packet may be
queued for a short time in anticipation of a LISTEN .
The obvious analogy between this protocol and real life is a customer (client)
calling a company’s customer service manager. The service manager starts out by
being near the telephone in case it rings. Then the client places the call. When the
manager picks up the phone, the connection is established.
Client machine
Client
process

Operating
system

System
calls
Protocol
Kernel
Drivers
stack

(1) Connect request
(2) ACK

Server machine

Server
process

(3) Request for data
(4) Reply
(5) Disconnect
(6) Disconnect

Kernel Protocol Drivers
stack

Figure 1-18. Packets sent in a simple client-server interaction on a connectionoriented network.

The next step is for the server to execute RECEIVE to prepare to accept the first
request. Normally, the server does this immediately upon being released from the
LISTEN , before the acknowledgement can get back to the client. The RECEIVE call
blocks the server.
Then the client executes SEND to transmit its request (3) followed by the execution of RECEIVE to get the reply.
The arrival of the request packet at the server machine unblocks the server
process so it can process the request. After it has done the work, it uses SEND to
return the answer to the client (4). The arrival of this packet unblocks the client,
which can now inspect the answer. If the client has additional requests, it can
make them now. If it is done, it can use DISCONNECT to terminate the connection.
Usually, an initial DISCONNECT is a blocking call, suspending the client and sending a packet to the server saying that the connection is no longer needed (5).
When the server gets the packet, it also issues a DISCONNECT of its own, acknowledging the client and releasing the connection. When the server’s packet
(6) gets back to the client machine, the client process is released and the connection is broken. In a nutshell, this is how connection-oriented communication
works.
Of course, life is not so simple. Many things can go wrong here. The timing
can be wrong (e.g., the CONNECT is done before the LISTEN ), packets can get lost,

36

INTRODUCTION

CHAP. 1

and much more. We will look at these issues in great detail later, but for the
moment, Fig. 1-18 briefly summarizes how client-server communication might
work over a connection-oriented network.
Given that six packets are required to complete this protocol, one might
wonder why a connectionless protocol is not used instead. The answer is that in a
perfect world it could be, in which case only two packets would be needed: one
for the request and one for the reply. However, in the face of large messages in
either direction (e.g., a megabyte file), transmission errors, and lost packets, the
situation changes. If the reply consisted of hundreds of packets, some of which
could be lost during transmission, how would the client know if some pieces were
missing? How would the client know whether the last packet actually received
was really the last packet sent? Suppose that the client wanted a second file.
How could it tell packet 1 from the second file from a lost packet 1 from the first
file that suddenly found its way to the client? In short, in the real world, a simple
request-reply protocol over an unreliable network is often inadequate. In Chap. 3
we will study a variety of protocols in detail that overcome these and other problems. For the moment, suffice it to say that having a reliable, ordered byte stream
between processes is sometimes very convenient.

1.3.5 The Relationship of Services to Protocols
Services and protocols are distinct concepts, although they are frequently confused. This distinction is so important, however, that we emphasize it again here.
A service is a set of primitives (operations) that a layer provides to the layer
above it. The service defines what operations the layer is prepared to perform on
behalf of its users, but it says nothing at all about how these operations are implemented. A service relates to an interface between two layers, with the lower layer
being the service provider and the upper layer being the service user.
A protocol, in contrast, is a set of rules governing the format and meaning of
the packets, or messages that are exchanged by the peer entities within a layer.
Entities use protocols to implement their service definitions. They are free to
change their protocols at will, provided they do not change the service visible to
their users. In this way, the service and the protocol are completely decoupled.
In other words, services relate to the interfaces between layers, as illustrated
in Fig. 1-19. In contrast, protocols relate to the packets sent between peer entities
on different machines. It is important not to confuse the two concepts.
An analogy with programming languages is worth making. A service is like
an abstract data type or an object in an object-oriented language. It defines operations that can be performed on an object but does not specify how these operations
are implemented. A protocol relates to the implementation of the service and as
such is not visible to the user of the service.
Many older protocols did not distinguish the service from the protocol. In
effect, a typical layer might have had a service primitive SEND PACKET with the

SEC. 1.3

37

NETWORK SOFTWARE

Layer k + 1

Layer k + 1

Service provided by layer k
Layer k

Layer k - 1

Protocol

Layer k

Layer k - 1

Figure 1-19. The relationship between a service and a protocol.

user providing a pointer to a fully assembled packet. This arrangement meant that
all changes to the protocol were immediately visible to the users. Most network
designers now regard such a design as a serious blunder.

1.4 REFERENCE MODELS
Now that we have discussed layered networks in the abstract, it is time to look
at some examples. In the next two sections we will discuss two important network architectures, the OSI reference model and the TCP/IP reference model.
Although the protocols associated with the OSI model are rarely used any more,
the model itself is actually quite general and still valid, and the features discussed
at each layer are still very important. The TCP/IP model has the opposite properties: the model itself is not of much use but the protocols are widely used. For this
reason we will look at both of them in detail. Also, sometimes you can learn
more from failures than from successes.

1.4.1 The OSI Reference Model
The OSI model (minus the physical medium) is shown in Fig. 1-20. This model is based on a proposal developed by the International Standards Organization
(ISO) as a first step toward international standardization of the protocols used in
the various layers (Day and Zimmermann, 1983). It was revised in 1995 (Day,
1995). The model is called the ISO OSI (Open Systems Interconnection) Reference Model because it deals with connecting open systems—that is, systems
that are open for communication with other systems. We will just call it the OSI
model for short.
The OSI model has seven layers. The principles that were applied to arrive at
the seven layers can be briefly summarized as follows:

38

INTRODUCTION

CHAP. 1

1. A layer should be created where a different abstraction is needed.
2. Each layer should perform a well-defined function.
3. The function of each layer should be chosen with an eye toward
defining internationally standardized protocols.
4. The layer boundaries should be chosen to minimize the information
flow across the interfaces.
5. The number of layers should be large enough that distinct functions
need not be thrown together in the same layer out of necessity and
small enough that the architecture does not become unwieldy.
Below we will discuss each layer of the model in turn, starting at the bottom
layer. Note that the OSI model itself is not a network architecture because it does
not specify the exact services and protocols to be used in each layer. It just tells
what each layer should do. However, ISO has also produced standards for all the
layers, although these are not part of the reference model itself. Each one has
been published as a separate international standard.
The Physical Layer
The physical layer is concerned with transmitting raw bits over a communication channel. The design issues have to do with making sure that when one side
sends a 1 bit, it is received by the other side as a 1 bit, not as a 0 bit. Typical
questions here are how many volts should be used to represent a 1 and how many
for a 0, how many nanoseconds a bit lasts, whether transmission may proceed
simultaneously in both directions, how the initial connection is established and
how it is torn down when both sides are finished, and how many pins the network
connector has and what each pin is used for. The design issues here largely deal
with mechanical, electrical, and timing interfaces, and the physical transmission
medium, which lies below the physical layer.
The Data Link Layer
The main task of the data link layer is to transform a raw transmission facility into a line that appears free of undetected transmission errors to the network
layer. It accomplishes this task by having the sender break up the input data into
data frames (typically a few hundred or a few thousand bytes) and transmit the
frames sequentially. If the service is reliable, the receiver confirms correct receipt
of each frame by sending back an acknowledgement frame.
Another issue that arises in the data link layer (and most of the higher layers
as well) is how to keep a fast transmitter from drowning a slow receiver in data.
Some traffic regulation mechanism is often needed to let the transmitter know

SEC. 1.4

39

REFERENCE MODELS

Name of unit
exchanged

Layer

7

Application

Application protocol

Application

APDU

Presentation

PPDU

Session

SPDU

Transport

TPDU

Network

Network

Packet

Interface
6

Presentation

5

Session

4

Transport

Presentation protocol

Session protocol

Transport protocol
Communication subnet boundary
Internal subnet protocol

3

Network

2

Data link

Data link

Data link

Data link

Frame

1

Physical

Physical

Physical

Physical

Bit

Host A

Router

Router

Host B

Network

Network layer host-router protocol
Data link layer host-router protocol
Physical layer host-router protocol

Figure 1-20. The OSI reference model.

how much buffer space the receiver has at the moment. Frequently, this flow
regulation and the error handling are integrated.
Broadcast networks have an additional issue in the data link layer: how to
control access to the shared channel. A special sublayer of the data link layer, the
medium access control sublayer, deals with this problem.
The Network Layer
The network layer controls the operation of the subnet. A key design issue is
determining how packets are routed from source to destination. Routes can be
based on static tables that are ‘‘wired into’’ the network and rarely changed. They
can also be determined at the start of each conversation, for example, a terminal
session (e.g., a login to a remote machine). Finally, they can be highly dynamic,
being determined anew for each packet, to reflect the current network load.

40

INTRODUCTION

CHAP. 1

If too many packets are present in the subnet at the same time, they will get in
one another’s way, forming bottlenecks. The control of such congestion also
belongs to the network layer. More generally, the quality of service provided
(delay, transit time, jitter, etc.) is also a network layer issue.
When a packet has to travel from one network to another to get to its destination, many problems can arise. The addressing used by the second network may
be different from the first one. The second one may not accept the packet at all
because it is too large. The protocols may differ, and so on. It is up to the network layer to overcome all these problems to allow heterogeneous networks to be
interconnected.
In broadcast networks, the routing problem is simple, so the network layer is
often thin or even nonexistent.
The Transport Layer
The basic function of the transport layer is to accept data from above, split it
up into smaller units if need be, pass these to the network layer, and ensure that
the pieces all arrive correctly at the other end. Furthermore, all this must be done
efficiently and in a way that isolates the upper layers from the inevitable changes
in the hardware technology.
The transport layer also determines what type of service to provide to the session layer, and, ultimately, to the users of the network. The most popular type of
transport connection is an error-free point-to-point channel that delivers messages
or bytes in the order in which they were sent. However, other possible kinds of
transport service are the transporting of isolated messages, with no guarantee
about the order of delivery, and the broadcasting of messages to multiple destinations. The type of service is determined when the connection is established. (As
an aside, an error-free channel is impossible to achieve; what people really mean
by this term is that the error rate is low enough to ignore in practice.)
The transport layer is a true end-to-end layer, all the way from the source to
the destination. In other words, a program on the source machine carries on a
conversation with a similar program on the destination machine, using the message headers and control messages. In the lower layers, the protocols are between
each machine and its immediate neighbors, and not between the ultimate source
and destination machines, which may be separated by many routers. The difference between layers 1 through 3, which are chained, and layers 4 through 7,
which are end-to-end, is illustrated in Fig. 1-20.
The Session Layer
The session layer allows users on different machines to establish sessions
between them. Sessions offer various services, including dialog control (keeping
track of whose turn it is to transmit), token management (preventing two parties

SEC. 1.4

REFERENCE MODELS

41

from attempting the same critical operation at the same time), and synchronization (checkpointing long transmissions to allow them to continue from where they
were after a crash).
The Presentation Layer
Unlike lower layers, which are mostly concerned with moving bits around, the
presentation layer is concerned with the syntax and semantics of the information
transmitted. In order to make it possible for computers with different data representations to communicate, the data structures to be exchanged can be defined
in an abstract way, along with a standard encoding to be used ‘‘on the wire.’’ The
presentation layer manages these abstract data structures and allows higher-level
data structures (e.g., banking records), to be defined and exchanged.
The Application Layer
The application layer contains a variety of protocols that are commonly
needed by users. One widely-used application protocol is HTTP (HyperText
Transfer Protocol), which is the basis for the World Wide Web. When a browser wants a Web page, it sends the name of the page it wants to the server using
HTTP. The server then sends the page back. Other application protocols are used
for file transfer, electronic mail, and network news.

1.4.2 The TCP/IP Reference Model
Let us now turn from the OSI reference model to the reference model used in
the grandparent of all wide area computer networks, the ARPANET, and its successor, the worldwide Internet. Although we will give a brief history of the
ARPANET later, it is useful to mention a few key aspects of it now. The
ARPANET was a research network sponsored by the DoD (U.S. Department of
Defense). It eventually connected hundreds of universities and government installations, using leased telephone lines. When satellite and radio networks were added later, the existing protocols had trouble interworking with them, so a new reference architecture was needed. Thus, the ability to connect multiple networks in
a seamless way was one of the major design goals from the very beginning. This
architecture later became known as the TCP/IP Reference Model, after its two
primary protocols. It was first defined in (Cerf and Kahn, 1974). A later perspective is given in (Leiner et al., 1985). The design philosophy behind the model is
discussed in (Clark, 1988).
Given the DoD’s worry that some of its precious hosts, routers, and internetwork gateways might get blown to pieces at a moment’s notice, another major
goal was that the network be able to survive loss of subnet hardware, with existing
conversations not being broken off. In other words, DoD wanted connections to

42

INTRODUCTION

CHAP. 1

remain intact as long as the source and destination machines were functioning,
even if some of the machines or transmission lines in between were suddenly put
out of operation. Furthermore, a flexible architecture was needed since applications with divergent requirements were envisioned, ranging from transferring files
to real-time speech transmission.
The Internet Layer
All these requirements led to the choice of a packet-switching network based
on a connectionless internetwork layer. This layer, called the internet layer, is
the linchpin that holds the whole architecture together. Its job is to permit hosts to
inject packets into any network and have them travel independently to the destination (potentially on a different network). They may even arrive in a different
order than they were sent, in which case it is the job of higher layers to rearrange
them, if in-order delivery is desired. Note that ‘‘internet’’ is used here in a generic
sense, even though this layer is present in the Internet.
The analogy here is with the (snail) mail system. A person can drop a
sequence of international letters into a mail box in one country, and with a little
luck, most of them will be delivered to the correct address in the destination country. Probably the letters will travel through one or more international mail gateways along the way, but this is transparent to the users. Furthermore, that each
country (i.e., each network) has its own stamps, preferred envelope sizes, and
delivery rules is hidden from the users.
The internet layer defines an official packet format and protocol called IP
(Internet Protocol). The job of the internet layer is to deliver IP packets where
they are supposed to go. Packet routing is clearly the major issue here, as is
avoiding congestion. For these reasons, it is reasonable to say that the TCP/IP
internet layer is similar in functionality to the OSI network layer. Figure 1-21
shows this correspondence.
The Transport Layer
The layer above the internet layer in the TCP/IP model is now usually called
the transport layer. It is designed to allow peer entities on the source and destination hosts to carry on a conversation, just as in the OSI transport layer. Two
end-to-end transport protocols have been defined here. The first one, TCP
(Transmission Control Protocol), is a reliable connection-oriented protocol that
allows a byte stream originating on one machine to be delivered without error on
any other machine in the internet. It fragments the incoming byte stream into
discrete messages and passes each one on to the internet layer. At the destination,
the receiving TCP process reassembles the received messages into the output
stream. TCP also handles flow control to make sure a fast sender cannot swamp a
slow receiver with more messages than it can handle.

SEC. 1.4

43

REFERENCE MODELS

OSI

TCP/IP
Application

7

Application

6

Presentation

5

Session

4

Transport

Transport

3

Network

Internet

2

Data link

Host-to-network

1

Physical

Not present
in the model

Figure 1-21. The TCP/IP reference model.

The second protocol in this layer, UDP (User Datagram Protocol), is an unreliable, connectionless protocol for applications that do not want TCP’s sequencing or flow control and wish to provide their own. It is also widely used for oneshot, client-server-type request-reply queries and applications in which prompt
delivery is more important than accurate delivery, such as transmitting speech or
video. The relation of IP, TCP, and UDP is shown in Fig. 1-22. Since the model
was developed, IP has been implemented on many other networks.
Layer (OSI names)
TELNET

FTP

SMTP

TCP

Protocols

DNS

UDP

Transport

IP

Networks

ARPANET

SATNET

Application

Network
Packet
radio

LAN

Physical +
data link

Figure 1-22. Protocols and networks in the TCP/IP model initially.

The Application Layer
The TCP/IP model does not have session or presentation layers. No need for
them was perceived, so they were not included. Experience with the OSI model
has proven this view correct: they are of little use to most applications.
On top of the transport layer is the application layer. It contains all the
higher-level protocols. The early ones included virtual terminal (TELNET), file

44

INTRODUCTION

CHAP. 1

transfer (FTP), and electronic mail (SMTP), as shown in Fig. 1-22. The virtual
terminal protocol allows a user on one machine to log onto a distant machine and
work there. The file transfer protocol provides a way to move data efficiently
from one machine to another. Electronic mail was originally just a kind of file
transfer, but later a specialized protocol (SMTP) was developed for it. Many
other protocols have been added to these over the years: the Domain Name System (DNS) for mapping host names onto their network addresses, NNTP, the protocol for moving USENET news articles around, and HTTP, the protocol for
fetching pages on the World Wide Web, and many others.
The Host-to-Network Layer
Below the internet layer is a great void. The TCP/IP reference model does
not really say much about what happens here, except to point out that the host has
to connect to the network using some protocol so it can send IP packets to it. This
protocol is not defined and varies from host to host and network to network.
Books and papers about the TCP/IP model rarely discuss it.

1.4.3 A Comparison of the OSI and TCP/IP Reference Models
The OSI and TCP/IP reference models have much in common. Both are
based on the concept of a stack of independent protocols. Also, the functionality
of the layers is roughly similar. For example, in both models the layers up
through and including the transport layer are there to provide an end-to-end,
network-independent transport service to processes wishing to communicate.
These layers form the transport provider. Again in both models, the layers above
transport are application-oriented users of the transport service.
Despite these fundamental similarities, the two models also have many differences. In this section we will focus on the key differences between the two reference models. It is important to note that we are comparing the reference models
here, not the corresponding protocol stacks. The protocols themselves will be discussed later. For an entire book comparing and contrasting TCP/IP and OSI, see
(Piscitello and Chapin, 1993).
Three concepts are central to the OSI model:
1. Services.
2. Interfaces.
3. Protocols.
Probably the biggest contribution of the OSI model is to make the distinction
between these three concepts explicit. Each layer performs some services for the
layer above it. The service definition tells what the layer does, not how entities
above it access it or how the layer works. It defines the layer’s semantics.

SEC. 1.4

REFERENCE MODELS

45

A layer’s interface tells the processes above it how to access it. It specifies
what the parameters are and what results to expect. It, too, says nothing about
how the layer works inside.
Finally, the peer protocols used in a layer are the layer’s own business. It can
use any protocols it wants to, as long as it gets the job done (i.e., provides the
offered services). It can also change them at will without affecting software in
higher layers.
These ideas fit very nicely with modern ideas about object-oriented programming. An object, like a layer, has a set of methods (operations) that processes outside the object can invoke. The semantics of these methods define the set of services that the object offers. The methods’ parameters and results form the object’s
interface. The code internal to the object is its protocol and is not visible or of
any concern outside the object.
The TCP/IP model did not originally clearly distinguish between service,
interface, and protocol, although people have tried to retrofit it after the fact to
make it more OSI-like. For example, the only real services offered by the internet
layer are SEND IP PACKET and RECEIVE IP PACKET .
As a consequence, the protocols in the OSI model are better hidden than in the
TCP/IP model and can be replaced relatively easily as the technology changes.
Being able to make such changes is one of the main purposes of having layered
protocols in the first place.
The OSI reference model was devised before the corresponding protocols
were invented. This ordering means that the model was not biased toward one
particular set of protocols, a fact that made it quite general. The downside of this
ordering is that the designers did not have much experience with the subject and
did not have a good idea of which functionality to put in which layer.
For example, the data link layer originally dealt only with point-to-point networks. When broadcast networks came around, a new sublayer had to be hacked
into the model. When people started to build real networks using the OSI model
and existing protocols, it was discovered that these networks did not match the
required service specifications (wonder of wonders), so convergence sublayers
had to be grafted onto the model to provide a place for papering over the differences. Finally, the committee originally expected that each country would have
one network, run by the government and using the OSI protocols, so no thought
was given to internetworking. To make a long story short, things did not turn out
that way.
With TCP/IP the reverse was true: the protocols came first, and the model was
really just a description of the existing protocols. There was no problem with the
protocols fitting the model. They fit perfectly. The only trouble was that the
model did not fit any other protocol stacks. Consequently, it was not especially
useful for describing other, non-TCP/IP networks.
Turning from philosophical matters to more specific ones, an obvious difference between the two models is the number of layers: the OSI model has seven

46

INTRODUCTION

CHAP. 1

layers and the TCP/IP has four layers. Both have (inter)network, transport, and
application layers, but the other layers are different.
Another difference is in the area of connectionless versus connection-oriented
communication. The OSI model supports both connectionless and connectionoriented communication in the network layer, but only connection-oriented communication in the transport layer, where it counts (because the transport service is
visible to the users). The TCP/IP model has only one mode in the network layer
(connectionless) but supports both modes in the transport layer, giving the users a
choice. This choice is especially important for simple request-response protocols.

1.4.4 A Critique of the OSI Model and Protocols
Neither the OSI model and its protocols nor the TCP/IP model and its protocols are perfect. Quite a bit of criticism can be, and has been, directed at both of
them. In this section and the next one, we will look at some of these criticisms.
We will begin with OSI and examine TCP/IP afterward.
At the time the second edition of this book was published (1989), it appeared
to many experts in the field that the OSI model and its protocols were going to
take over the world and push everything else out of their way. This did not happen. Why? A look back at some of the lessons may be useful. These lessons can
be summarized as:
1. Bad timing.
2. Bad technology.
3. Bad implementations.
4. Bad politics.

Bad Timing
First let us look at reason one: bad timing. The time at which a standard is
established is absolutely critical to its success. David Clark of M.I.T. has a theory
of standards that he calls the apocalypse of the two elephants, which is illustrated
in Fig. 1-23.
This figure shows the amount of activity surrounding a new subject. When
the subject is first discovered, there is a burst of research activity in the form of
discussions, papers, and meetings. After a while this activity subsides, corporations discover the subject, and the billion-dollar wave of investment hits.
It is essential that the standards be written in the trough in between the two
‘‘elephants.’’ If the standards are written too early, before the research is finished,
the subject may still be poorly understood; the result is bad standards. If they are
written too late, so many companies may have already made major investments in

SEC. 1.4

REFERENCE MODELS

Billion dollar
investment

Research

Activity

47

Standards

Time

Figure 1-23. The apocalypse of the two elephants.

different ways of doing things that the standards are effectively ignored. If the
interval between the two elephants is very short (because everyone is in a hurry to
get started), the people developing the standards may get crushed.
It now appears that the standard OSI protocols got crushed. The competing
TCP/IP protocols were already in widespread use by research universities by the
time the OSI protocols appeared. While the billion-dollar wave of investment had
not yet hit, the academic market was large enough that many vendors had begun
cautiously offering TCP/IP products. When OSI came around, they did not want
to support a second protocol stack until they were forced to, so there were no initial offerings. With every company waiting for every other company to go first,
no company went first and OSI never happened.
Bad Technology
The second reason that OSI never caught on is that both the model and the
protocols are flawed. The choice of seven layers was more political than technical, and two of the layers (session and presentation) are nearly empty, whereas
two other ones (data link and network) are overfull.
The OSI model, along with the associated service definitions and protocols, is
extraordinarily complex. When piled up, the printed standards occupy a significant fraction of a meter of paper. They are also difficult to implement and inefficient in operation. In this context, a riddle posed by Paul Mockapetris and cited in
(Rose, 1993) comes to mind:
Q: What do you get when you cross a mobster with an international standard?
A: Someone who makes you an offer you can’t understand.
In addition to being incomprehensible, another problem with OSI is that some
functions, such as addressing, flow control, and error control, reappear again and

48

INTRODUCTION

CHAP. 1

again in each layer. Saltzer et al. (1984), for example, have pointed out that to be
effective, error control must be done in the highest layer, so that repeating it over
and over in each of the lower layers is often unnecessary and inefficient.
Bad Implementations
Given the enormous complexity of the model and the protocols, it will come
as no surprise that the initial implementations were huge, unwieldy, and slow.
Everyone who tried them got burned. It did not take long for people to associate
‘‘OSI’’ with ‘‘poor quality.’’ Although the products improved in the course of
time, the image stuck.
In contrast, one of the first implementations of TCP/IP was part of Berkeley
UNIX and was quite good (not to mention, free). People began using it quickly,
which led to a large user community, which led to improvements, which led to an
even larger community. Here the spiral was upward instead of downward.
Bad Politics
On account of the initial implementation, many people, especially in academia, thought of TCP/IP as part of UNIX, and UNIX in the 1980s in academia was
not unlike parenthood (then incorrectly called motherhood) and apple pie.
OSI, on the other hand, was widely thought to be the creature of the European
telecommunication ministries, the European Community, and later the U.S.
Government. This belief was only partly true, but the very idea of a bunch of
government bureaucrats trying to shove a technically inferior standard down the
throats of the poor researchers and programmers down in the trenches actually
developing computer networks did not help much. Some people viewed this
development in the same light as IBM announcing in the 1960s that PL/I was the
language of the future, or DoD correcting this later by announcing that it was
actually Ada.

1.4.5 A Critique of the TCP/IP Reference Model
The TCP/IP model and protocols have their problems too. First, the model
does not clearly distinguish the concepts of service, interface, and protocol. Good
software engineering practice requires differentiating between the specification
and the implementation, something that OSI does very carefully, and TCP/IP does
not. Consequently, the TCP/IP model is not much of a guide for designing new
networks using new technologies.
Second, the TCP/IP model is not at all general and is poorly suited to describing any protocol stack other than TCP/IP. Trying to use the TCP/IP model to
describe Bluetooth, for example, is completely impossible.
Third, the host-to-network layer is not really a layer at all in the normal sense
of the term as used in the context of layered protocols. It is an interface (between

SEC. 1.4

REFERENCE MODELS

49

the network and data link layers). The distinction between an interface and a
layer is crucial, and one should not be sloppy about it.
Fourth, the TCP/IP model does not distinguish (or even mention) the physical
and data link layers. These are completely different. The physical layer has to do
with the transmission characteristics of copper wire, fiber optics, and wireless
communication. The data link layer’s job is to delimit the start and end of frames
and get them from one side to the other with the desired degree of reliability. A
proper model should include both as separate layers. The TCP/IP model does not
do this.
Finally, although the IP and TCP protocols were carefully thought out and
well implemented, many of the other protocols were ad hoc, generally produced
by a couple of graduate students hacking away until they got tired. The protocol
implementations were then distributed free, which resulted in their becoming
widely used, deeply entrenched, and thus hard to replace. Some of them are a bit
of an embarrassment now. The virtual terminal protocol, TELNET, for example,
was designed for a ten-character per second mechanical Teletype terminal. It
knows nothing of graphical user interfaces and mice. Nevertheless, 25 years later,
it is still in widespread use.
In summary, despite its problems, the OSI model (minus the session and
presentation layers) has proven to be exceptionally useful for discussing computer
networks. In contrast, the OSI protocols have not become popular. The reverse is
true of TCP/IP: the model is practically nonexistent, but the protocols are widely
used. Since computer scientists like to have their cake and eat it, too, in this book
we will use a modified OSI model but concentrate primarily on the TCP/IP and
related protocols, as well as newer ones such as 802, SONET, and Bluetooth. In
effect, we will use the hybrid model of Fig. 1-24 as the framework for this book.
222222222222222222
5 1222222222222222222
Application layer 1
1
1
4 1222222222222222222
Transport layer 1
1
3 1222222222222222222
Network layer
1
1
1
2 1222222222222222222
Data link layer
1
1
1 1222222222222222222
Physical layer
1

Figure 1-24. The hybrid reference model to be used in this book.

1.5 EXAMPLE NETWORKS
The subject of computer networking covers many different kinds of networks,
large and small, well known and less well known. They have different goals,
scales, and technologies. In the following sections, we will look at some examples, to get an idea of the variety one finds in the area of computer networking.

50

INTRODUCTION

CHAP. 1

We will start with the Internet, probably the best known network, and look at
its history, evolution, and technology. Then we will consider ATM, which is often used within the core of large (telephone) networks. Technically, it is quite different from the Internet, contrasting nicely with it. Next we will introduce Ethernet, the dominant local area network. Finally, we will look at IEEE 802.11, the
standard for wireless LANs.

1.5.1 The Internet
The Internet is not a network at all, but a vast collection of different networks
that use certain common protocols and provide certain common services. It is an
unusual system in that it was not planned by anyone and is not controlled by anyone. To better understand it, let us start from the beginning and see how it has
developed and why. For a wonderful history of the Internet, John Naughton’s
(2000) book is highly recommended. It is one of those rare books that is not only
fun to read, but also has 20 pages of ibid.’s and op. cit.’s for the serious historian.
Some of the material below is based on this book.
Of course, countless technical books have been written about the Internet and
its protocols as well. For more information, see, for example, (Maufer, 1999).
The ARPANET
The story begins in the late 1950s. At the height of the Cold War, the DoD
wanted a command-and-control network that could survive a nuclear war. At that
time, all military communications used the public telephone network, which was
considered vulnerable. The reason for this belief can be gleaned from Fig. 125(a). Here the black dots represent telephone switching offices, each of which
was connected to thousands of telephones. These switching offices were, in turn,
connected to higher-level switching offices (toll offices), to form a national hierarchy with only a small amount of redundancy. The vulnerability of the system
was that the destruction of a few key toll offices could fragment the system into
many isolated islands.
Around 1960, the DoD awarded a contract to the RAND Corporation to find a
solution. One of its employees, Paul Baran, came up with the highly distributed
and fault-tolerant design of Fig. 1-25(b). Since the paths between any two switching offices were now much longer than analog signals could travel without distortion, Baran proposed using digital packet-switching technology throughout the
system. Baran wrote several reports for the DoD describing his ideas in detail.
Officials at the Pentagon liked the concept and asked AT&T, then the U.S. national telephone monopoly, to build a prototype. AT&T dismissed Baran’s ideas
out of hand. The biggest and richest corporation in the world was not about to
allow some young whippersnapper tell it how to build a telephone system. They
said Baran’s network could not be built and the idea was killed.

SEC. 1.5

51

EXAMPLE NETWORKS
Switching
office

Toll
office

(a)

(b)

Figure 1-25. (a) Structure of the telephone system. (b) Baran’s proposed distributed switching system.

Several years went by and still the DoD did not have a better command-andcontrol system. To understand what happened next, we have to go back to October 1957, when the Soviet Union beat the U.S. into space with the launch of the
first artificial satellite, Sputnik. When President Eisenhower tried to find out who
was asleep at the switch, he was appalled to find the Army, Navy, and Air Force
squabbling over the Pentagon’s research budget. His immediate response was to
create a single defense research organization, ARPA, the Advanced Research
Projects Agency. ARPA had no scientists or laboratories; in fact, it had nothing
more than an office and a small (by Pentagon standards) budget. It did its work
by issuing grants and contracts to universities and companies whose ideas looked
promising to it.
For the first few years, ARPA tried to figure out what its mission should be,
but in 1967, the attention of ARPA’s then director, Larry Roberts, turned to networking. He contacted various experts to decide what to do. One of them, Wesley Clark, suggested building a packet-switched subnet, giving each host its own
router, as illustrated in Fig. 1-10.
After some initial skepticism, Roberts bought the idea and presented a somewhat vague paper about it at the ACM SIGOPS Symposium on Operating System
Principles held in Gatlinburg, Tennessee in late 1967 (Roberts, 1967). Much to
Roberts’ surprise, another paper at the conference described a similar system that
had not only been designed but actually implemented under the direction of Donald Davies at the National Physical Laboratory in England. The NPL system was
not a national system (it just connected several computers on the NPL campus),
but it demonstrated that packet switching could be made to work. Furthermore, it

52

INTRODUCTION

CHAP. 1

cited Baran’s now discarded earlier work. Roberts came away from Gatlinburg
determined to build what later became known as the ARPANET.
The subnet would consist of minicomputers called IMPs (Interface Message
Processors) connected by 56-kbps transmission lines. For high reliability, each
IMP would be connected to at least two other IMPs. The subnet was to be a
datagram subnet, so if some lines and IMPs were destroyed, messages could be
automatically rerouted along alternative paths.
Each node of the network was to consist of an IMP and a host, in the same
room, connected by a short wire. A host could send messages of up to 8063 bits
to its IMP, which would then break these up into packets of at most 1008 bits and
forward them independently toward the destination. Each packet was received in
its entirety before being forwarded, so the subnet was the first electronic storeand-forward packet-switching network.
ARPA then put out a tender for building the subnet. Twelve companies bid
for it. After evaluating all the proposals, ARPA selected BBN, a consulting firm
in Cambridge, Massachusetts, and in December 1968, awarded it a contract to
build the subnet and write the subnet software. BBN chose to use specially modified Honeywell DDP-316 minicomputers with 12K 16-bit words of core memory
as the IMPs. The IMPs did not have disks, since moving parts were considered
unreliable. The IMPs were interconnected by 56-kbps lines leased from telephone
companies. Although 56 kbps is now the choice of teenagers who cannot afford
ADSL or cable, it was then the best money could buy.
The software was split into two parts: subnet and host. The subnet software
consisted of the IMP end of the host-IMP connection, the IMP-IMP protocol, and
a source IMP to destination IMP protocol designed to improve reliability. The
original ARPANET design is shown in Fig. 1-26.
Host-host protocol

Host

Host-IMP
protocol

Source IMP

IMP-IMP protocol

ol

n IMP protoc

to destinatio

-IMP
IMP col
o
o
pr t

Subnet

IMP

Figure 1-26. The original ARPANET design.

Outside the subnet, software was also needed, namely, the host end of the
host-IMP connection, the host-host protocol, and the application software. It soon

SEC. 1.5

53

EXAMPLE NETWORKS

became clear that BBN felt that when it had accepted a message on a host-IMP
wire and placed it on the host-IMP wire at the destination, its job was done.
Roberts had a problem: the hosts needed software too. To deal with it, he
convened a meeting of network researchers, mostly graduate students, at Snowbird, Utah, in the summer of 1969. The graduate students expected some network
expert to explain the grand design of the network and its software to them and
then to assign each of them the job of writing part of it. They were astounded
when there was no network expert and no grand design. They had to figure out
what to do on their own.
Nevertheless, somehow an experimental network went on the air in December
1969 with four nodes: at UCLA, UCSB, SRI, and the University of Utah. These
four were chosen because all had a large number of ARPA contracts, and all had
different and completely incompatible host computers (just to make it more fun).
The network grew quickly as more IMPs were delivered and installed; it soon
spanned the United States. Figure 1-27 shows how rapidly the ARPANET grew
in the first 3 years.
SRI

SRI

UTAH

UTAH

UCSB

UCSB

MIT

SRI

SDC

UTAH ILLINOIS MIT

UCSB

LINCOLN CASE

CARN

SDC
STAN

UCLA

UCLA

(a)

RAND

BBN

UCLA

MCCLELLAN
UTAH

GWC

LINCOLN CASE

STAN

USC
MIT
SDC

CARN
LINC
MITRE

RAND

STANFORD
FNWC RAND
TINKER

ETAC
TINKER

BBN

HARVARD

UCSD

NBS

UCLA

(d)

SDC

ILLINOIS

MIT

CCA
BBN
HARVARD
LINC
ABERDEEN
NBS
ETAC

AMES IMP

UCSB
UCLA

UTAH

X-PARC

RADC

ILLINOIS
AMES
UCSB

LBL MCCLELLAN
AMES TIP

NCAR

HARVARD BURROUGHS

(c)

(b)

SRI

SRI

BBN

RAND

USC

ARPA
MITRE
RADC
SAAC
BELVOIR
CMU

NOAA

GWC

CASE

(e)

Figure 1-27. Growth of the ARPANET. (a) December 1969. (b) July 1970.
(c) March 1971. (d) April 1972. (e) September 1972.

In addition to helping the fledgling ARPANET grow, ARPA also funded research on the use of satellite networks and mobile packet radio networks. In one

54

INTRODUCTION

CHAP. 1

now famous demonstration, a truck driving around in California used the packet
radio network to send messages to SRI, which were then forwarded over the
ARPANET to the East Coast, where they were shipped to University College in
London over the satellite network. This allowed a researcher in the truck to use a
computer in London while driving around in California.
This experiment also demonstrated that the existing ARPANET protocols
were not suitable for running over multiple networks. This observation led to
more research on protocols, culminating with the invention of the TCP/IP model
and protocols (Cerf and Kahn, 1974). TCP/IP was specifically designed to handle
communication over internetworks, something becoming increasingly important
as more and more networks were being hooked up to the ARPANET.
To encourage adoption of these new protocols, ARPA awarded several contracts to BBN and the University of California at Berkeley to integrate them into
Berkeley UNIX. Researchers at Berkeley developed a convenient program interface to the network (sockets) and wrote many application, utility, and management programs to make networking easier.
The timing was perfect. Many universities had just acquired a second or third
VAX computer and a LAN to connect them, but they had no networking software.
When 4.2BSD came along, with TCP/IP, sockets, and many network utilities, the
complete package was adopted immediately. Furthermore, with TCP/IP, it was
easy for the LANs to connect to the ARPANET, and many did.
During the 1980s, additional networks, especially LANs, were connected to
the ARPANET. As the scale increased, finding hosts became increasingly expensive, so DNS (Domain Name System) was created to organize machines into
domains and map host names onto IP addresses. Since then, DNS has become a
generalized, distributed database system for storing a variety of information
related to naming. We will study it in detail in Chap. 7.
NSFNET
By the late 1970s, NSF (the U.S. National Science Foundation) saw the enormous impact the ARPANET was having on university research, allowing scientists across the country to share data and collaborate on research projects. However, to get on the ARPANET, a university had to have a research contract with
the DoD, which many did not have. NSF’s response was to design a successor to
the ARPANET that would be open to all university research groups. To have
something concrete to start with, NSF decided to build a backbone network to
connect its six supercomputer centers, in San Diego, Boulder, Champaign, Pittsburgh, Ithaca, and Princeton. Each supercomputer was given a little brother, consisting of an LSI-11 microcomputer called a fuzzball. The fuzzballs were connected with 56-kbps leased lines and formed the subnet, the same hardware technology as the ARPANET used. The software technology was different however:
the fuzzballs spoke TCP/IP right from the start, making it the first TCP/IP WAN.

SEC. 1.5

55

EXAMPLE NETWORKS

NSF also funded some (eventually about 20) regional networks that connected
to the backbone to allow users at thousands of universities, research labs, libraries,
and museums to access any of the supercomputers and to communicate with one
another. The complete network, including the backbone and the regional networks, was called NSFNET. It connected to the ARPANET through a link
between an IMP and a fuzzball in the Carnegie-Mellon machine room. The first
NSFNET backbone is illustrated in Fig. 1-28.

NSF Supercomputer center
NSF Midlevel network
Both

Figure 1-28. The NSFNET backbone in 1988.

NSFNET was an instantaneous success and was overloaded from the word go.
NSF immediately began planning its successor and awarded a contract to the
Michigan-based MERIT consortium to run it. Fiber optic channels at 448 kbps
were leased from MCI (since merged with WorldCom) to provide the version 2
backbone. IBM PC-RTs were used as routers. This, too, was soon overwhelmed,
and by 1990, the second backbone was upgraded to 1.5 Mbps.
As growth continued, NSF realized that the government could not continue
financing networking forever. Furthermore, commercial organizations wanted to
join but were forbidden by NSF’s charter from using networks NSF paid for.
Consequently, NSF encouraged MERIT, MCI, and IBM to form a nonprofit corporation, ANS (Advanced Networks and Services), as the first step along the
road to commercialization. In 1990, ANS took over NSFNET and upgraded the
1.5-Mbps links to 45 Mbps to form ANSNET. This network operated for 5 years
and was then sold to America Online. But by then, various companies were offering commercial IP service and it was clear the government should now get out of
the networking business.
To ease the transition and make sure every regional network could communicate with every other regional network, NSF awarded contracts to four different

56

INTRODUCTION

CHAP. 1

network operators to establish a NAP (Network Access Point). These operators
were PacBell (San Francisco), Ameritech (Chicago), MFS (Washington, D.C.),
and Sprint (New York City, where for NAP purposes, Pennsauken, New Jersey
counts as New York City). Every network operator that wanted to provide backbone service to the NSF regional networks had to connect to all the NAPs.
This arrangement meant that a packet originating on any regional network had
a choice of backbone carriers to get from its NAP to the destination’s NAP. Consequently, the backbone carriers were forced to compete for the regional networks’ business on the basis of service and price, which was the idea, of course.
As a result, the concept of a single default backbone was replaced by a commercially-driven competitive infrastructure. Many people like to criticize the Federal
Government for not being innovative, but in the area of networking, it was DoD
and NSF that created the infrastructure that formed the basis for the Internet and
then handed it over to industry to operate.
During the 1990s, many other countries and regions also built national research networks, often patterned on the ARPANET and NSFNET. These included EuropaNET and EBONE in Europe, which started out with 2-Mbps lines
and then upgraded to 34-Mbps lines. Eventually, the network infrastructure in
Europe was handed over to industry as well.
Internet Usage
The number of networks, machines, and users connected to the ARPANET
grew rapidly after TCP/IP became the only official protocol on January 1, 1983.
When NSFNET and the ARPANET were interconnected, the growth became exponential. Many regional networks joined up, and connections were made to networks in Canada, Europe, and the Pacific.
Sometime in the mid-1980s, people began viewing the collection of networks
as an internet, and later as the Internet, although there was no official dedication
with some politician breaking a bottle of champagne over a fuzzball.
The glue that holds the Internet together is the TCP/IP reference model and
TCP/IP protocol stack. TCP/IP makes universal service possible and can be compared to the adoption of standard gauge by the railroads in the 19th century or the
adoption of common signaling protocols by all the telephone companies.
What does it actually mean to be on the Internet? Our definition is that a
machine is on the Internet if it runs the TCP/IP protocol stack, has an IP address,
and can send IP packets to all the other machines on the Internet. The mere ability to send and receive electronic mail is not enough, since e-mail is gatewayed to
many networks outside the Internet. However, the issue is clouded somewhat by
the fact that millions of personal computers can call up an Internet service provider using a modem, be assigned a temporary IP address, and send IP packets to
other Internet hosts. It makes sense to regard such machines as being on the Internet for as long as they are connected to the service provider’s router.

SEC. 1.5

EXAMPLE NETWORKS

57

Traditionally (meaning 1970 to about 1990), the Internet and its predecessors
had four main applications:
1. E-mail. The ability to compose, send, and receive electronic mail
has been around since the early days of the ARPANET and is enormously popular. Many people get dozens of messages a day and
consider it their primary way of interacting with the outside world,
far outdistancing the telephone and snail mail. E-mail programs are
available on virtually every kind of computer these days.
2. News. Newsgroups are specialized forums in which users with a
common interest can exchange messages. Thousands of newsgroups
exist, devoted to technical and nontechnical topics, including computers, science, recreation, and politics. Each newsgroup has its own
etiquette, style, and customs, and woe betide anyone violating them.
3. Remote login. Using the telnet, rlogin, or ssh programs, users anywhere on the Internet can log on to any other machine on which they
have an account.
4. File transfer. Using the FTP program, users can copy files from one
machine on the Internet to another. Vast numbers of articles, databases, and other information are available this way.
Up until the early 1990s, the Internet was largely populated by academic,
government, and industrial researchers. One new application, the WWW (World
Wide Web) changed all that and brought millions of new, nonacademic users to
the net. This application, invented by CERN physicist Tim Berners-Lee, did not
change any of the underlying facilities but made them easier to use. Together
with the Mosaic browser, written by Marc Andreessen at the National Center for
Supercomputer Applications in Urbana, Illinois, the WWW made it possible for a
site to set up a number of pages of information containing text, pictures, sound,
and even video, with embedded links to other pages. By clicking on a link, the
user is suddenly transported to the page pointed to by that link. For example,
many companies have a home page with entries pointing to other pages for product information, price lists, sales, technical support, communication with employees, stockholder information, and more.
Numerous other kinds of pages have come into existence in a very short time,
including maps, stock market tables, library card catalogs, recorded radio programs, and even a page pointing to the complete text of many books whose copyrights have expired (Mark Twain, Charles Dickens, etc.). Many people also have
personal pages (home pages).
Much of this growth during the 1990s was fueled by companies called ISPs
(Internet Service Providers). These are companies that offer individual users at
home the ability to call up one of their machines and connect to the Internet, thus

58

INTRODUCTION

CHAP. 1

gaining access to e-mail, the WWW, and other Internet services. These companies signed up tens of millions of new users a year during the late 1990s, completely changing the character of the network from an academic and military playground to a public utility, much like the telephone system. The number of Internet users now is unknown, but is certainly hundreds of millions worldwide and
will probably hit 1 billion fairly soon.
Architecture of the Internet
In this section we will attempt to give a brief overview of the Internet today.
Due to the many mergers between telephone companies (telcos) and ISPs, the
waters have become muddied and it is often hard to tell who is doing what. Consequently, this description will be of necessity somewhat simpler than reality.
The big picture is shown in Fig. 1-29. Let us examine this figure piece by piece
now.
Regional ISP
Backbone

POP

NAP
Telephone
system

Client

Server farm

Corporate
LAN
Router

Figure 1-29. Overview of the Internet.

A good place to start is with a client at home. Let us assume our client calls
his or her ISP over a dial-up telephone line, as shown in Fig. 1-29. The modem is
a card within the PC that converts the digital signals the computer produces to
analog signals that can pass unhindered over the telephone system. These signals
are transferred to the ISP’s POP (Point of Presence), where they are removed
from the telephone system and injected into the ISP’s regional network. From this
point on, the system is fully digital and packet switched. If the ISP is the local

SEC. 1.5

EXAMPLE NETWORKS

59

telco, the POP will probably be located in the telephone switching office where
the telephone wire from the client terminates. If the ISP is not the local telco, the
POP may be a few switching offices down the road.
The ISP’s regional network consists of interconnected routers in the various
cities the ISP serves. If the packet is destined for a host served directly by the
ISP, the packet is delivered to the host. Otherwise, it is handed over to the ISP’s
backbone operator.
At the top of the food chain are the major backbone operators, companies like
AT&T and Sprint. They operate large international backbone networks, with
thousands of routers connected by high-bandwidth fiber optics. Large corporations and hosting services that run server farms (machines that can serve thousands of Web pages per second) often connect directly to the backbone. Backbone operators encourage this direct connection by renting space in what are
called carrier hotels, basically equipment racks in the same room as the router to
allow short, fast connections between server farms and the backbone.
If a packet given to the backbone is destined for an ISP or company served by
the backbone, it is sent to the closest router and handed off there. However, many
backbones, of varying sizes, exist in the world, so a packet may have to go to a
competing backbone. To allow packets to hop between backbones, all the major
backbones connect at the NAPs discussed earlier. Basically, a NAP is a room full
of routers, at least one per backbone. A LAN in the room connects all the routers,
so packets can be forwarded from any backbone to any other backbone. In addition to being interconnected at NAPs, the larger backbones have numerous direct
connections between their routers, a technique known as private peering. One of
the many paradoxes of the Internet is that ISPs who publicly compete with one
another for customers often privately cooperate to do private peering (Metz,
2001).
This ends our quick tour of the Internet. We will have a great deal to say
about the individual components and their design, algorithms, and protocols in
subsequent chapters. Also worth mentioning in passing is that some companies
have interconnected all their existing internal networks, often using the same technology as the Internet. These intranets are typically accessible only within the
company but otherwise work the same way as the Internet.

1.5.2 Connection-Oriented Networks: X.25, Frame Relay, and ATM
Since the beginning of networking, a war has been going on between the people who support connectionless (i.e., datagram) subnets and the people who support connection-oriented subnets. The main proponents of the connectionless subnets come from the ARPANET/Internet community. Remember that DoD’s original desire in funding and building the ARPANET was to have a network that
would continue functioning even after multiple direct hits by nuclear weapons
wiped out numerous routers and transmission lines. Thus, fault tolerance was

60

INTRODUCTION

CHAP. 1

high on their priority list; billing customers was not. This approach led to a connectionless design in which every packet is routed independently of every other
packet. As a consequence, if some routers go down during a session, no harm is
done as long as the system can reconfigure itself dynamically so that subsequent
packets can find some route to the destination, even if it is different from that
which previous packets used.
The connection-oriented camp comes from the world of telephone companies.
In the telephone system, a caller must dial the called party’s number and wait for
a connection before talking or sending data. This connection setup establishes a
route through the telephone system that is maintained until the call is terminated.
All words or packets follow the same route. If a line or switch on the path goes
down, the call is aborted. This property is precisely what the DoD did not like
about it.
Why do the telephone companies like it then? There are two reasons:
1. Quality of service.
2. Billing.
By setting up a connection in advance, the subnet can reserve resources such as
buffer space and router CPU capacity. If an attempt is made to set up a call and
insufficient resources are available, the call is rejected and the caller gets a kind
of busy signal. In this way, once a connection has been set up, the connection will
get good service. With a connectionless network, if too many packets arrive at
the same router at the same moment, the router will choke and probably lose
packets. The sender will eventually notice this and resend them, but the quality of
service will be jerky and unsuitable for audio or video unless the network is very
lightly loaded. Needless to say, providing adequate audio quality is something
telephone companies care about very much, hence their preference for connections.
The second reason the telephone companies like connection-oriented service
is that they are accustomed to charging for connect time. When you make a long
distance call (or even a local call outside North America) you are charged by the
minute. When networks came around, they just automatically gravitated toward a
model in which charging by the minute was easy to do. If you have to set up a
connection before sending data, that is when the billing clock starts running. If
there is no connection, they cannot charge for it.
Ironically, maintaining billing records is very expensive. If a telephone company were to adopt a flat monthly rate with unlimited calling and no billing or
record keeping, it would probably save a huge amount of money, despite the increased calling this policy would generate. Political, regulatory, and other factors
weigh against doing this, however. Interestingly enough, flat rate service exists in
other sectors. For example, cable TV is billed at a flat rate per month, no matter
how many programs you watch. It could have been designed with pay-per-view

SEC. 1.5

EXAMPLE NETWORKS

61

as the basic concept, but it was not, due in part to the expense of billing (and given the quality of most television, the embarrassment factor cannot be totally discounted either). Also, many theme parks charge a daily admission fee for unlimited rides, in contrast to traveling carnivals, which charge by the ride.
That said, it should come as no surprise that all networks designed by the telephone industry have had connection-oriented subnets. What is perhaps surprising,
is that the Internet is also drifting in that direction, in order to provide a better
quality of service for audio and video, a subject we will return to in Chap. 5. But
now let us examine some connection-oriented networks.
X.25 and Frame Relay
Our first example of a connection-oriented network is X.25, which was the
first public data network. It was deployed in the 1970s at a time when telephone
service was a monopoly everywhere and the telephone company in each country
expected there to be one data network per country—theirs. To use X.25, a computer first established a connection to the remote computer, that is, placed a telephone call. This connection was given a connection number to be used in data
transfer packets (because multiple connections could be open at the same time).
Data packets were very simple, consisting of a 3-byte header and up to 128 bytes
of data. The header consisted of a 12-bit connection number, a packet sequence
number, an acknowledgement number, and a few miscellaneous bits. X.25 networks operated for about a decade with mixed success.
In the 1980s, the X.25 networks were largely replaced by a new kind of network called frame relay. The essence of frame relay is that it is a connectionoriented network with no error control and no flow control. Because it was
connection-oriented, packets were delivered in order (if they were delivered at
all). The properties of in-order delivery, no error control, and no flow control
make frame relay akin to a wide area LAN. Its most important application is
interconnecting LANs at multiple company offices. Frame relay enjoyed a modest success and is still in use in places today.
Asynchronous Transfer Mode
Yet another, and far more important, connection-oriented network is ATM
(Asynchronous Transfer Mode). The reason for the somewhat strange name is
that in the telephone system, most transmission is synchronous (closely tied to a
clock), and ATM is not.
ATM was designed in the early 1990s and launched amid truly incredible
hype (Ginsburg, 1996; Goralski, 1995; Ibe, 1997; Kim et al., 1994; and Stallings,
2000). ATM was going to solve all the world’s networking and telecommunications problems by merging voice, data, cable television, telex, telegraph, carrier
pigeon, tin cans connected by strings, tom-toms, smoke signals, and everything

62

INTRODUCTION

CHAP. 1

else into a single integrated system that could do everything for everyone. It did
not happen. In large part, the problems were similar to those we described earlier
concerning OSI, that is, bad timing, technology, implementation, and politics.
Having just beaten back the telephone companies in round 1, many in the Internet
community saw ATM as Internet versus the Telcos: the Sequel. But it really was
not, and this time around even diehard datagram fanatics were aware that the
Internet’s quality of service left a lot to be desired. To make a long story short,
ATM was much more successful than OSI, and it is now widely used deep within
the telephone system, often for moving IP packets. Because it is now mostly used
by carriers for internal transport, users are often unaware of its existence, but it is
definitely alive and well.
ATM Virtual Circuits
Since ATM networks are connection-oriented, sending data requires first
sending a packet to set up the connection. As the setup packet wends its way
through the subnet, all the routers on the path make an entry in their internal tables noting the existence of the connection and reserving whatever resources are
needed for it. Connections are often called virtual circuits, in analogy with the
physical circuits used within the telephone system. Most ATM networks also support permanent virtual circuits, which are permanent connections between two
(distant) hosts. They are similar to leased lines in the telephone world. Each connection, temporary or permanent, has a unique connection identifier. A virtual
circuit is illustrated in Fig. 1-30.
Router

Subnet

Sending host

Sending process

Receiving host

Virtual circuit

Receiving process

Figure 1-30. A virtual circuit.

Once a connection has been established, either side can begin transmitting
data. The basic idea behind ATM is to transmit all information in small, fixedsize packets called cells. The cells are 53 bytes long, of which 5 bytes are header
and 48 bytes are payload, as shown in Fig. 1-31. Part of the header is the connection identifier, so the sending and receiving hosts and all the intermediate routers
can tell which cells belong to which connections. This information allows each
router to know how to route each incoming cell. Cell routing is done in hardware,
at high speed. In fact, the main argument for having fixed-size cells is that it is
easy to build hardware routers to handle short, fixed-length cells. Variable-length

SEC. 1.5

EXAMPLE NETWORKS

63

IP packets have to be routed by software, which is a slower process. Another plus
of ATM is that the hardware can be set up to copy one incoming cell to multiple
output lines, a property that is required for handling a television program that is
being broadcast to many receivers. Finally, small cells do not block any line for
very long, which makes guaranteeing quality of service easier.
All cells follow the same route to the destination. Cell delivery is not
guaranteed, but their order is. If cells 1 and 2 are sent in that order, then if both
arrive, they will arrive in that order, never first 2 then 1. But either or both of
them can be lost along the way. It is up to higher protocol levels to recover from
lost cells. Note that although this guarantee is not perfect, it is better than what
the Internet provides. There packets can not only be lost, but delivered out of
order as well. ATM, in contrast, guarantees never to deliver cells out of order.
Bytes

5
Header

48
User data

Figure 1-31. An ATM cell.

ATM networks are organized like traditional WANs, with lines and switches
(routers). The most common speeds for ATM networks are 155 Mbps and 622
Mbps, although higher speeds are also supported. The 155-Mbps speed was
chosen because this is about what is needed to transmit high definition television.
The exact choice of 155.52 Mbps was made for compatibility with AT&T’s
SONET transmission system, something we will study in Chap. 2. The 622 Mbps
speed was chosen so that four 155-Mbps channels could be sent over it.
The ATM Reference Model
ATM has its own reference model, different from the OSI model and also different from the TCP/IP model. This model is shown in Fig. 1-32. It consists of
three layers, the physical, ATM, and ATM adaptation layers, plus whatever users
want to put on top of that.
The physical layer deals with the physical medium: voltages, bit timing, and
various other issues. ATM does not prescribe a particular set of rules but instead
says that ATM cells can be sent on a wire or fiber by themselves, but they can
also be packaged inside the payload of other carrier systems. In other words,
ATM has been designed to be independent of the transmission medium.
The ATM layer deals with cells and cell transport. It defines the layout of a
cell and tells what the header fields mean. It also deals with establishment and
release of virtual circuits. Congestion control is also located here.
Because most applications do not want to work directly with cells (although
some may), a layer above the ATM layer has been defined to allow users to send

64

INTRODUCTION

CHAP. 1

Plane management
Layer management
Control plane
Upper layers
CS
SAR

User plane
Upper layers

ATM adaptation layer
ATM layer

TC
PMD

Physical layer

r
ye
bla er
u
y
S la
b
Su
r
ye
bla er
u
S lay
b
Su

CS: Convergence sublayer
SAR: Segmentation and
reassembly sublayer
TC: Transmission convergence
sublayer
PMD: Physical medium
dependent sublayer

Figure 1-32. The ATM reference model.

packets larger than a cell. The ATM interface segments these packets, transmits
the cells individually, and reassembles them at the other end. This layer is the
AAL (ATM Adaptation Layer).
Unlike the earlier two-dimensional reference models, the ATM model is
defined as being three-dimensional, as shown in Fig. 1-32. The user plane deals
with data transport, flow control, error correction, and other user functions. In
contrast, the control plane is concerned with connection management. The layer
and plane management functions relate to resource management and interlayer
coordination.
The physical and AAL layers are each divided into two sublayers, one at the
bottom that does the work and a convergence sublayer on top that provides the
proper interface to the layer above it. The functions of the layers and sublayers
are given in Fig. 1-33.
The PMD (Physical Medium Dependent) sublayer interfaces to the actual
cable. It moves the bits on and off and handles the bit timing. For different carriers and cables, this layer will be different.
The other sublayer of the physical layer is the TC (Transmission Convergence) sublayer. When cells are transmitted, the TC layer sends them as a string
of bits to the PMD layer. Doing this is easy. At the other end, the TC sublayer
gets a pure incoming bit stream from the PMD sublayer. Its job is to convert this
bit stream into a cell stream for the ATM layer. It handles all the issues related to
telling where cells begin and end in the bit stream. In the ATM model, this functionality is in the physical layer. In the OSI model and in pretty much all other
networks, the job of framing, that is, turning a raw bit stream into a sequence of
frames or cells, is the data link layer’s task.
As we mentioned earlier, the ATM layer manages cells, including their generation and transport. Most of the interesting aspects of ATM are located here. It
is a mixture of the OSI data link and network layers; it is not split into sublayers.

SEC. 1.5

EXAMPLE NETWORKS

OSI
layer

ATM
layer

3/4

AAL

ATM
sublayer
CS
SAR

2/3

Functionality
Providing the standard interface (convergence)
Segmentation and reassembly
Flow control
Cell header generation/extraction
Virtual circuit/path management
Cell multiplexing/demultiplexing

ATM

TC

2

65

Cell rate decoupling
Header checksum generation and verification
Cell generation
Packing/unpacking cells from the enclosing envelope
Frame generation

Physical
PMD

1

Bit timing
Physical network access

Figure 1-33. The ATM layers and sublayers, and their functions.

The AAL layer is split into a SAR (Segmentation And Reassembly) sublayer and a CS (Convergence Sublayer). The lower sublayer breaks up packets
into cells on the transmission side and puts them back together again at the destination. The upper sublayer makes it possible to have ATM systems offer different
kinds of services to different applications (e.g., file transfer and video on demand
have different requirements concerning error handling, timing, etc.).
As it is probably mostly downhill for ATM from now on, we will not discuss
it further in this book. Nevertheless, since it has a substantial installed base, it
will probably be around for at least a few more years. For more information about
ATM, see (Dobrowski and Grise, 2001; and Gadecki and Heckart, 1997).

1.5.3 Ethernet
Both the Internet and ATM were designed for wide area networking. However, many companies, universities, and other organizations have large numbers
of computers that must be connected. This need gave rise to the local area network. In this section we will say a little bit about the most popular LAN, Ethernet.
The story starts out in pristine Hawaii in the early 1970s. In this case, ‘‘pristine’’ can be interpreted as ‘‘not having a working telephone system.’’ While not
being interrupted by the phone all day long makes life more pleasant for vacationers, it did not make life more pleasant for researcher Norman Abramson and his

66

INTRODUCTION

CHAP. 1

colleagues at the University of Hawaii who were trying to connect users on remote islands to the main computer in Honolulu. Stringing their own cables under
the Pacific Ocean was not in the cards, so they looked for a different solution.
The one they found was short-range radios. Each user terminal was equipped
with a small radio having two frequencies: upstream (to the central computer) and
downstream (from the central computer). When the user wanted to contact the
computer, it just transmitted a packet containing the data in the upstream channel.
If no one else was transmitting at that instant, the packet probably got through and
was acknowledged on the downstream channel. If there was contention for the
upstream channel, the terminal noticed the lack of acknowledgement and tried
again. Since there was only one sender on the downstream channel (the central
computer), there were never collisions there. This system, called ALOHANET,
worked fairly well under conditions of low traffic but bogged down badly when
the upstream traffic was heavy.
About the same time, a student named Bob Metcalfe got his bachelor’s degree
at M.I.T. and then moved up the river to get his Ph.D. at Harvard. During his studies, he was exposed to Abramson’s work. He became so interested in it that after
graduating from Harvard, he decided to spend the summer in Hawaii working
with Abramson before starting work at Xerox PARC (Palo Alto Research Center).
When he got to PARC, he saw that the researchers there had designed and built
what would later be called personal computers. But the machines were isolated.
Using his knowledge of Abramson’s work, he, together with his colleague David
Boggs, designed and implemented the first local area network (Metcalfe and
Boggs, 1976).
They called the system Ethernet after the luminiferous ether, through which
electromagnetic radiation was once thought to propagate. (When the 19th century
British physicist James Clerk Maxwell discovered that electromagnetic radiation
could be described by a wave equation, scientists assumed that space must be
filled with some ethereal medium in which the radiation was propagating. Only
after the famous Michelson-Morley experiment in 1887 did physicists discover
that electromagnetic radiation could propagate in a vacuum.)
The transmission medium here was not a vacuum, but a thick coaxial cable
(the ether) up to 2.5 km long (with repeaters every 500 meters). Up to 256 machines could be attached to the system via transceivers screwed onto the cable. A
cable with multiple machines attached to it in parallel is called a multidrop
cable. The system ran at 2.94 Mbps. A sketch of its architecture is given in
Fig. 1-34. Ethernet had a major improvement over ALOHANET: before transmitting, a computer first listened to the cable to see if someone else was already
transmitting. If so, the computer held back until the current transmission finished.
Doing so avoided interfering with existing transmissions, giving a much higher
efficiency. ALOHANET did not work like this because it was impossible for a
terminal on one island to sense the transmission of a terminal on a distant island.
With a single cable, this problem does not exist.

SEC. 1.5

EXAMPLE NETWORKS

67

Transceiver
Interface
cable
Ether

Figure 1-34. Architecture of the original Ethernet.

Despite the computer listening before transmitting, a problem still arises: what
happens if two or more computers all wait until the current transmission completes and then all start at once? The solution is to have each computer listen during its own transmission and if it detects interference, jam the ether to alert all
senders. Then back off and wait a random time before retrying. If a second collision happens, the random waiting time is doubled, and so on, to spread out the
competing transmissions and give one of them a chance to go first.
The Xerox Ethernet was so successful that DEC, Intel, and Xerox drew up a
standard in 1978 for a 10-Mbps Ethernet, called the DIX standard. With two
minor changes, the DIX standard became the IEEE 802.3 standard in 1983.
Unfortunately for Xerox, it already had a history of making seminal inventions (such as the personal computer) and then failing to commercialize on them,
a story told in Fumbling the Future (Smith and Alexander, 1988). When Xerox
showed little interest in doing anything with Ethernet other than helping standardize it, Metcalfe formed his own company, 3Com, to sell Ethernet adapters for
PCs. It has sold over 100 million of them.
Ethernet continued to develop and is still developing. New versions at 100
Mbps, 1000 Mbps, and still higher have come out. Also the cabling has improved,
and switching and other features have been added. We will discuss Ethernet in
detail in Chap. 4.
In passing, it is worth mentioning that Ethernet (IEEE 802.3) is not the only
LAN standard. The committee also standardized a token bus (802.4) and a token
ring (802.5). The need for three more-or-less incompatible standards has little to
do with technology and everything to do with politics. At the time of standardization, General Motors was pushing a LAN in which the topology was the same as
Ethernet (a linear cable) but computers took turns in transmitting by passing a
short packet called a token from computer to computer. A computer could only
send if it possessed the token, thus avoiding collisions. General Motors announced that this scheme was essential for manufacturing cars and was not prepared to
budge from this position. This announcement notwithstanding, 802.4 has basically vanished from sight.

68

INTRODUCTION

CHAP. 1

Similarly, IBM had its own favorite: its proprietary token ring. The token was
passed around the ring and whichever computer held the token was allowed to
transmit before putting the token back on the ring. Unlike 802.4, this scheme,
standardized as 802.5, is still in use at some IBM sites, but virtually nowhere outside of IBM sites. However, work is progressing on a gigabit version (802.5v),
but it seems unlikely that it will ever catch up with Ethernet. In short, there was a
war between Ethernet, token bus, and token ring, and Ethernet won, mostly
because it was there first and the challengers were not as good.

1.5.4 Wireless LANs: 802.11
Almost as soon as notebook computers appeared, many people had a dream of
walking into an office and magically having their notebook computer be connected to the Internet. Consequently, various groups began working on ways to
accomplish this goal. The most practical approach is to equip both the office and
the notebook computers with short-range radio transmitters and receivers to allow
them to communicate. This work rapidly led to wireless LANs being marketed by
a variety of companies.
The trouble was that no two of them were compatible. This proliferation of
standards meant that a computer equipped with a brand X radio would not work in
a room equipped with a brand Y base station. Finally, the industry decided that a
wireless LAN standard might be a good idea, so the IEEE committee that standardized the wired LANs was given the task of drawing up a wireless LAN standard. The standard it came up with was named 802.11. A common slang name
for it is WiFi. It is an important standard and deserves respect, so we will call it
by its proper name, 802.11.
The proposed standard had to work in two modes:
1. In the presence of a base station.
2. In the absence of a base station.
In the former case, all communication was to go through the base station, called
an access point in 802.11 terminology. In the latter case, the computers would
just send to one another directly. This mode is now sometimes called ad hoc networking. A typical example is two or more people sitting down together in a
room not equipped with a wireless LAN and having their computers just communicate directly. The two modes are illustrated in Fig. 1-35.
The first decision was the easiest: what to call it. All the other LAN standards
had numbers like 802.1, 802.2, 802.3, up to 802.10, so the wireless LAN standard
was dubbed 802.11. The rest was harder.
In particular, some of the many challenges that had to be met were: finding a
suitable frequency band that was available, preferably worldwide; dealing with
the fact that radio signals have a finite range; ensuring that users’ privacy was

SEC. 1.5

69

EXAMPLE NETWORKS

Base To wired network
station

(a)

(b)

Figure 1-35. (a) Wireless networking with a base station. (b) Ad hoc networking.

maintained; taking limited battery life into account; worrying about human safety
(do radio waves cause cancer?); understanding the implications of computer mobility; and finally, building a system with enough bandwidth to be economically
viable.
At the time the standardization process started (mid-1990s), Ethernet had
already come to dominate local area networking, so the committee decided to
make 802.11 compatible with Ethernet above the data link layer. In particular, it
should be possible to send an IP packet over the wireless LAN the same way a
wired computer sent an IP packet over Ethernet. Nevertheless, in the physical and
data link layers, several inherent differences with Ethernet exist and had to be
dealt with by the standard.
First, a computer on Ethernet always listens to the ether before transmitting.
Only if the ether is idle does the computer begin transmitting. With wireless
LANs, that idea does not work so well. To see why, examine Fig. 1-36. Suppose
that computer A is transmitting to computer B, but the radio range of A’s
transmitter is too short to reach computer C. If C wants to transmit to B it can
listen to the ether before starting, but the fact that it does not hear anything does
not mean that its transmission will succeed. The 802.11 standard had to solve this
problem.
The second problem that had to be solved is that a radio signal can be reflected off solid objects, so it may be received multiple times (along multiple paths).
This interference results in what is called multipath fading.
The third problem is that a great deal of software is not aware of mobility.
For example, many word processors have a list of printers that users can choose
from to print a file. When the computer on which the word processor runs is
taken into a new environment, the built-in list of printers becomes invalid.
The fourth problem is that if a notebook computer is moved away from the
ceiling-mounted base station it is using and into the range of a different base station, some way of handing it off is needed. Although this problem occurs with
cellular telephones, it does not occur with Ethernet and needed to be solved. In

70

INTRODUCTION

Range
of A's
radio

A

CHAP. 1

Range
of C's
radio

B

C

Figure 1-36. The range of a single radio may not cover the entire system.

particular, the network envisioned consists of multiple cells, each with its own
base station, but with the base stations connected by Ethernet, as shown in Fig. 137. From the outside, the entire system should look like a single Ethernet. The
connection between the 802.11 system and the outside world is called a portal.
Ethernet

Base station
Portal

Cell

Figure 1-37. A multicell 802.11 network.

After some work, the committee came up with a standard in 1997 that
addressed these and other concerns. The wireless LAN it described ran at either 1
Mbps or 2 Mbps. Almost immediately, people complained that it was too slow, so
work began on faster standards. A split developed within the committee, resulting
in two new standards in 1999. The 802.11a standard uses a wider frequency band
and runs at speeds up to 54 Mbps. The 802.11b standard uses the same frequency
band as 802.11, but uses a different modulation technique to achieve 11 Mbps.
Some people see this as psychologically important since 11 Mbps is faster than
the original wired Ethernet. It is likely that the original 1-Mbps 802.11 will die
off quickly, but it is not yet clear which of the new standards will win out.
To make matters even more complicated than they already were, the 802
committee has come up with yet another variant, 802.11g, which uses the modulation technique of 802.11a but the frequency band of 802.11b. We will come back
to 802.11 in detail in Chap. 4.

SEC. 1.5

EXAMPLE NETWORKS

71

That 802.11 is going to cause a revolution in computing and Internet access is
now beyond any doubt. Airports, train stations, hotels, shopping malls, and
universities are rapidly installing it. Even upscale coffee shops are installing
802.11 so that the assembled yuppies can surf the Web while drinking their lattes.
It is likely that 802.11 will do to the Internet what notebook computers did to
computing: make it mobile.

1.6 NETWORK STANDARDIZATION
Many network vendors and suppliers exist, each with its own ideas of how
things should be done. Without coordination, there would be complete chaos, and
users would get nothing done. The only way out is to agree on some network
standards.
Not only do standards allow different computers to communicate, but they
also increase the market for products adhering to the standard. A larger market
leads to mass production, economies of scale in manufacturing, VLSI implementations, and other benefits that decrease price and further increase acceptance. In
the following sections we will take a quick look at the important, but little-known,
world of international standardization.
Standards fall into two categories: de facto and de jure. De facto (Latin for
‘‘from the fact’’) standards are those that have just happened, without any formal
plan. The IBM PC and its successors are de facto standards for small-office and
home computers because dozens of manufacturers chose to copy IBM’s machines
very closely. Similarly, UNIX is the de facto standard for operating systems in
university computer science departments.
De jure (Latin for ‘‘by law’’) standards, in contrast, are formal, legal standards adopted by some authorized standardization body. International standardization authorities are generally divided into two classes: those established by
treaty among national governments, and those comprising voluntary, nontreaty
organizations. In the area of computer network standards, there are several organizations of each type, which are discussed below.

1.6.1 Who’s Who in the Telecommunications World
The legal status of the world’s telephone companies varies considerably from
country to country. At one extreme is the United States, which has 1500 separate,
privately owned telephone companies. Before it was broken up in 1984, AT&T,
at that time the world’s largest corporation, completely dominated the scene. It
provided telephone service to about 80 percent of America’s telephones, spread
throughout half of its geographical area, with all the other companies combined

72

INTRODUCTION

CHAP. 1

servicing the remaining (mostly rural) customers. Since the breakup, AT&T continues to provide long-distance service, although now in competition with other
companies. The seven Regional Bell Operating Companies that were split off
from AT&T and numerous independents provide local and cellular telephone
service. Due to frequent mergers and other changes, the industry is in a constant
state of flux.
Companies in the United States that provide communication services to the
public are called common carriers. Their offerings and prices are described by a
document called a tariff, which must be approved by the Federal Communications Commission for the interstate and international traffic and by the state public
utilities commissions for intrastate traffic.
At the other extreme are countries in which the national government has a
complete monopoly on all communication, including the mail, telegraph, telephone, and often, radio and television. Most of the world falls in this category. In
some cases the telecommunication authority is a nationalized company, and in
others it is simply a branch of the government, usually known as the PTT (Post,
Telegraph & Telephone administration). Worldwide, the trend is toward liberalization and competition and away from government monopoly. Most European
countries have now (partially) privatized their PTTs, but elsewhere the process is
still slowly gaining steam.
With all these different suppliers of services, there is clearly a need to provide
compatibility on a worldwide scale to ensure that people (and computers) in one
country can call their counterparts in another one. Actually, this need has existed
for a long time. In 1865, representatives from many European governments met
to form the predecessor to today’s ITU (International Telecommunication
Union). Its job was standardizing international telecommunications, which in
those days meant telegraphy. Even then it was clear that if half the countries used
Morse code and the other half used some other code, there was going to be a problem. When the telephone was put into international service, ITU took over the job
of standardizing telephony (pronounced te-LEF-ony) as well. In 1947, ITU became an agency of the United Nations.
ITU has three main sectors:
1. Radiocommunications Sector (ITU-R).
2. Telecommunications Standardization Sector (ITU-T).
3. Development Sector (ITU-D).
ITU-R is concerned with allocating radio frequencies worldwide to the competing
interest groups. We will focus primarily on ITU-T, which is concerned with telephone and data communication systems. From 1956 to 1993, ITU-T was known
as CCITT, an acronym for its French name: Comité Consultatif International
Télégraphique et Téléphonique. On March 1, 1993, CCITT was reorganized to
make it less bureaucratic and renamed to reflect its new role. Both ITU-T and

SEC. 1.6

NETWORK STANDARDIZATION

73

CCITT issued recommendations in the area of telephone and data communications. One still frequently runs into CCITT recommendations, such as CCITT
X.25, although since 1993 recommendations bear the ITU-T label.
ITU-T has four classes of members:
1. National governments.
2. Sector members.
3. Associate members.
4. Regulatory agencies.
ITU-T has about 200 governmental members, including almost every member of
the United Nations. Since the United States does not have a PTT, somebody else
had to represent it in ITU-T. This task fell to the State Department, probably on
the grounds that ITU-T had to do with foreign countries, the State Department’s
specialty. There are approximately 500 sector members, including telephone
companies (e.g., AT&T, Vodafone, WorldCom), telecom equipment manufacturers (e.g., Cisco, Nokia, Nortel), computer vendors (e.g., Compaq, Sun, Toshiba),
chip manufacturers (e.g., Intel, Motorola, TI), media companies (e.g., AOL Time
Warner, CBS, Sony), and other interested companies (e.g., Boeing, Samsung,
Xerox). Various nonprofit scientific organizations and industry consortia are also
sector members (e.g., IFIP and IATA). Associate members are smaller organizations that are interested in a particular Study Group. Regulatory agencies are the
folks who watch over the telecom business, such as the U.S. Federal Communications Commission.
ITU-T’s task is to make technical recommendations about telephone, telegraph, and data communication interfaces. These often become internationally
recognized standards, for example, V.24 (also known as EIA RS-232 in the
United States), which specifies the placement and meaning of the various pins on
the connector used by most asynchronous terminals and external modems.
It should be noted that ITU-T recommendations are technically only suggestions that governments can adopt or ignore, as they wish (because governments
are like 13-year-old boys—they do not take kindly to being given orders). In
practice, a country that wishes to adopt a telephone standard different from that
used by the rest of the world is free to do so, but at the price of cutting itself off
from everyone else. This might work for North Korea, but elsewhere it would be
a real problem. The fiction of calling ITU-T standards ‘‘recommendations’’ was
and is necessary to keep nationalist forces in many countries placated.
The real work of ITU-T is done in its 14 Study Groups, often as large as 400
people. There are currently 14 Study Groups, covering topics ranging from telephone billing to multimedia services. In order to make it possible to get anything
at all done, the Study Groups are divided into Working Parties, which are in turn

74

INTRODUCTION

CHAP. 1

divided into Expert Teams, which are in turn divided into ad hoc groups. Once a
bureaucracy, always a bureaucracy.
Despite all this, ITU-T actually gets things done. Since its inception, it has
produced close to 3000 recommendations occupying about 60,000 pages of paper.
Many of these are widely used in practice. For example, the popular V.90 56kbps modem standard is an ITU recommendation.
As telecommunications completes the transition started in the 1980s from
being entirely national to being entirely global, standards will become increasingly important, and more and more organizations will want to become involved
in setting them. For more information about ITU, see (Irmer, 1994).

1.6.2 Who’s Who in the International Standards World
International standards are produced and published by ISO (International
†
Standards Organization ), a voluntary nontreaty organization founded in 1946.
Its members are the national standards organizations of the 89 member countries.
These members include ANSI (U.S.), BSI (Great Britain), AFNOR (France), DIN
(Germany), and 85 others.
ISO issues standards on a truly vast number of subjects, ranging from nuts and
bolts (literally) to telephone pole coatings [not to mention cocoa beans (ISO
2451), fishing nets (ISO 1530), women’s underwear (ISO 4416) and quite a few
other subjects one might not think were subject to standardization]. Over 13,000
standards have been issued, including the OSI standards. ISO has almost 200
Technical Committees, numbered in the order of their creation, each dealing with
a specific subject. TC1 deals with the nuts and bolts (standardizing screw thread
pitches). TC97 deals with computers and information processing. Each TC has
subcommittees (SCs) divided into working groups (WGs).
The real work is done largely in the WGs by over 100,000 volunteers worldwide. Many of these ‘‘volunteers’’ are assigned to work on ISO matters by their
employers, whose products are being standardized. Others are government officials keen on having their country’s way of doing things become the international
standard. Academic experts also are active in many of the WGs.
On issues of telecommunication standards, ISO and ITU-T often cooperate
(ISO is a member of ITU-T) to avoid the irony of two official and mutually
incompatible international standards.
The U.S. representative in ISO is ANSI (American National Standards
Institute), which despite its name, is a private, nongovernmental, nonprofit organization. Its members are manufacturers, common carriers, and other interested
parties. ANSI standards are frequently adopted by ISO as international standards.
The procedure used by ISO for adopting standards has been designed to
achieve as broad a consensus as possible. The process begins when one of the

333333333333333333333333333333333333333333333333

† For the purist, ISO’s true name is the International Organization for Standardization.

SEC. 1.6

NETWORK STANDARDIZATION

75

national standards organizations feels the need for an international standard in
some area. A working group is then formed to come up with a CD (Committee
Draft). The CD is then circulated to all the member bodies, which get 6 months
to criticize it. If a substantial majority approves, a revised document, called a DIS
(Draft International Standard) is produced and circulated for comments and
voting. Based on the results of this round, the final text of the IS (International
Standard) is prepared, approved, and published. In areas of great controversy, a
CD or DIS may have to go through several versions before acquiring enough
votes, and the whole process can take years.
NIST (National Institute of Standards and Technology) is part of the U.S.
Department of Commerce. It used to be the National Bureau of Standards. It
issues standards that are mandatory for purchases made by the U.S. Government,
except for those of the Department of Defense, which has its own standards.
Another major player in the standards world is IEEE (Institute of Electrical
and Electronics Engineers), the largest professional organization in the world.
In addition to publishing scores of journals and running hundreds of conferences
each year, IEEE has a standardization group that develops standards in the area of
electrical engineering and computing. IEEE’s 802 committee has standardized
many kinds of LANs. We will study some of its output later in this book. The
actual work is done by a collection of working groups, which are listed in Fig. 138. The success rate of the various 802 working groups has been low; having an
802.x number is no guarantee of success. But the impact of the success stories
(especially 802.3 and 802.11) has been enormous.

1.6.3 Who’s Who in the Internet Standards World
The worldwide Internet has its own standardization mechanisms, very different from those of ITU-T and ISO. The difference can be crudely summed up
by saying that the people who come to ITU or ISO standardization meetings wear
suits. The people who come to Internet standardization meetings wear jeans
(except when they meet in San Diego, when they wear shorts and T-shirts).
ITU-T and ISO meetings are populated by corporate officials and government
civil servants for whom standardization is their job. They regard standardization
as a Good Thing and devote their lives to it. Internet people, on the other hand,
prefer anarchy as a matter of principle. However, with hundreds of millions of
people all doing their own thing, little communication can occur. Thus, standards,
however regrettable, are sometimes needed.
When the ARPANET was set up, DoD created an informal committee to
oversee it. In 1983, the committee was renamed the IAB (Internet Activities
Board) and was given a slighter broader mission, namely, to keep the researchers
involved with the ARPANET and the Internet pointed more-or-less in the same
direction, an activity not unlike herding cats. The meaning of the acronym
‘‘IAB’’ was later changed to Internet Architecture Board.

76

INTRODUCTION

CHAP. 1

222222222222222222222222222222222222222222222222222222222222222222222
1 Number 1
1
Topic
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
802.1
1222222222222222222222222222222222222222222222222222222222222222222222
1 Overview and architecture of LANs
1
1 802.2 ↓ 1 Logical link control
1
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
1222222222222222222222222222222222222222222222222222222222222222222222
1
802.3 * 1 Ethernet
1
1
1
802.4
↓
Token
bus
(was
briefly
used
in
manufacturing
plants)
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
1222222222222222222222222222222222222222222222222222222222222222222222
1 Token ring (IBM’s entry into the LAN world)
1
802.5
1
1
1
802.6 ↓ 1 Dual queue dual bus (early metropolitan area network)
1222222222222222222222222222222222222222222222222222222222222222222222
1
1 802.7 ↓ 1 Technical advisory group on broadband technologies
1
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
802.8 † 1 Technical advisory group on fiber optic technologies
1222222222222222222222222222222222222222222222222222222222222222222222
1
1 802.9 ↓ 1 Isochronous LANs (for real-time applications)
1
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
1222222222222222222222222222222222222222222222222222222222222222222222
1
802.10 ↓ 1 Virtual LANs and security
1
1
1
802.11 * 1 Wireless LANs
1222222222222222222222222222222222222222222222222222222222222222222222
1
1 802.12 ↓ 1 Demand priority (Hewlett-Packard’s AnyLAN)
1
21 22222222222222222222222222222222222222222222222222222222222222222222
1
1
802.13
1222222222222222222222222222222222222222222222222222222222222222222222
1 Unlucky number. Nobody wanted it
1
1 802.14 ↓ 1 Cable modems (defunct: an industry consortium got there first) 1
21 22222222222222222222222222222222222222222222222222222222222222222222
1
1
802.15 * 1 Personal area networks (Bluetooth)
1222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
802.16 * 1 Broadband wireless
21 22222222222222222222222222222222222222222222222222222222222222222222
1
11222222222222222222222222222222222222222222222222222222222222222222222
11 Resilient packet ring
11
802.17

Figure 1-38. The 802 working groups. The important ones are marked with *.
The ones marked with ↓ are hibernating. The one marked with † gave up and
disbanded itself.

Each of the approximately ten members of the IAB headed a task force on
some issue of importance. The IAB met several times a year to discuss results
and to give feedback to the DoD and NSF, which were providing most of the
funding at this time. When a standard was needed (e.g., a new routing algorithm),
the IAB members would thrash it out and then announce the change so the graduate students who were the heart of the software effort could implement it. Communication was done by a series of technical reports called RFCs (Request For
Comments). RFCs are stored on-line and can be fetched by anyone interested in
them from www.ietf.org/rfc. They are numbered in chronological order of creation. Over 3000 now exist. We will refer to many RFCs in this book.
By 1989, the Internet had grown so large that this highly informal style no
longer worked. Many vendors by then offered TCP/IP products and did not want
to change them just because ten researchers had thought of a better idea. In the
summer of 1989, the IAB was reorganized again. The researchers were moved to
the IRTF (Internet Research Task Force), which was made subsidiary to IAB,
along with the IETF (Internet Engineering Task Force). The IAB was repopulated with people representing a broader range of organizations than just the

SEC. 1.6

NETWORK STANDARDIZATION

77

research community. It was initially a self-perpetuating group, with members
serving for a 2-year term and new members being appointed by the old ones.
Later, the Internet Society was created, populated by people interested in the
Internet. The Internet Society is thus in a sense comparable to ACM or IEEE. It
is governed by elected trustees who appoint the IAB members.
The idea of this split was to have the IRTF concentrate on long-term research
while the IETF dealt with short-term engineering issues. The IETF was divided
up into working groups, each with a specific problem to solve. The chairmen of
these working groups initially met as a steering committee to direct the engineering effort. The working group topics include new applications, user information,
OSI integration, routing and addressing, security, network management, and standards. Eventually, so many working groups were formed (more than 70) that they
were grouped into areas and the area chairmen met as the steering committee.
In addition, a more formal standardization process was adopted, patterned
after ISOs. To become a Proposed Standard, the basic idea must be completely
explained in an RFC and have sufficient interest in the community to warrant consideration. To advance to the Draft Standard stage, a working implementation
must have been rigorously tested by at least two independent sites for at least 4
months. If the IAB is convinced that the idea is sound and the software works, it
can declare the RFC to be an Internet Standard. Some Internet Standards have
become DoD standards (MIL-STD), making them mandatory for DoD suppliers.
David Clark once made a now-famous remark about Internet standardization consisting of ‘‘rough consensus and running code.’’

1.7 METRIC UNITS
To avoid any confusion, it is worth stating explicitly that in this book, as in
computer science in general, metric units are used instead of traditional English
units (the furlong-stone-fortnight system). The principal metric prefixes are listed
in Fig. 1-39. The prefixes are typically abbreviated by their first letters, with the
units greater than 1 capitalized (KB, MB, etc.). One exception (for historical reasons) is kbps for kilobits/sec. Thus, a 1-Mbps communication line transmits 106
bits/sec and a 100 psec (or 100 ps) clock ticks every 10−10 seconds. Since milli
and micro both begin with the letter ‘‘m,’’ a choice had to be made. Normally,
‘‘m’’ is for milli and ‘‘µ’’ (the Greek letter mu) is for micro.
It is also worth pointing out that for measuring memory, disk, file, and database sizes, in common industry practice, the units have slightly different meanings. There, kilo means 210 (1024) rather than 103 (1000) because memories are
always a power of two. Thus, a 1-KB memory contains 1024 bytes, not 1000
bytes. Similarly, a 1-MB memory contains 220 (1,048,576) bytes, a 1-GB
memory contains 230 (1,073,741,824) bytes, and a 1-TB database contains 240

78

INTRODUCTION

CHAP. 1

222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1 Exp. 1
1 Prefix 1 Exp. 1
1 Prefix 1
Explicit
Explicit
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
1 10−3 1 0.001
1 milli
1 103 1
1,000 1 Kilo 1
222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
1
6
10−6 1 0.000001
1,000,000 1 Mega 1
1 micro 1 10
1
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
1
9
10−9 1 0.000000001
1,000,000,000 1 Giga 1
1 nano 1 10
1
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1 10−12 1 0.000000000001
1 pico 1 1012 1
1,000,000,000,000 1 Tera 1
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
1 10−15 1 0.000000000000001
1 femto 1 1015 1
1,000,000,000,000,000 1 Peta 1
222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
1
18
10−18 1 0.0000000000000000001
1,000,000,000,000,000,000 1 Exa 1
1 atto
1 10
1
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1
21 1
10−21 1 0.0000000000000000000001
1,000,000,000,000,000,000,000 1 Zetta 1
1 zepto 1 10
1
21 22222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1 10−24 1 0.0000000000000000000000001 1 yocto 1 1024 1 1,000,000,000,000,000,000,000,000 1 Yotta 1
1222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222222
1
1
1
1
1
1

Figure 1-39. The principal metric prefixes.

(1,099,511,627,776) bytes. However, a 1-kbps communication line transmits
1000 bits per second and a 10-Mbps LAN runs at 10,000,000 bits/sec because
these speeds are not powers of two. Unfortunately, many people tend to mix up
these two systems, especially for disk sizes. To avoid ambiguity, in this book, we
will use the symbols KB, MB, and GB for 210 , 220 , and 230 bytes, respectively,
and the symbols kbps, Mbps, and Gbps for 103 , 106 , and 109 bits/sec, respectively.

1.8 OUTLINE OF THE REST OF THE BOOK
This book discusses both the principles and practice of computer networking.
Most chapters start with a discussion of the relevant principles, followed by a
number of examples that illustrate these principles. These examples are usually
taken from the Internet and wireless networks since these are both important and
very different. Other examples will be given where relevant.
The book is structured according to the hybrid model of Fig. 1-24. Starting
with Chap. 2, we begin working our way up the protocol hierarchy beginning at
the bottom. The second chapter provides some background in the field of data
communication. It covers wired, wireless, and satellite transmission systems.
This material is concerned with the physical layer, although we cover only the
architectural rather than the hardware aspects. Several examples of the physical
layer, such as the public switched telephone network, mobile telephones, and the
cable television network are also discussed.
Chapter 3 discusses the data link layer and its protocols by means of a number
of increasingly complex examples. The analysis of these protocols is also covered. After that, some important real-world protocols are discussed, including
HDLC (used in low- and medium-speed networks) and PPP (used in the Internet).

SEC. 1.8

OUTLINE OF THE REST OF THE BOOK

79

Chapter 4 concerns the medium access sublayer, which is part of the data link
layer. The basic question it deals with is how to determine who may use the network next when the network consists of a single shared channel, as in most LANs
and some satellite networks. Many examples are given from the areas of wired
LANs, wireless LANs (especially Ethernet), wireless MANs, Bluetooth, and
satellite networks. Bridges and data link switches, which are used to connect
LANs, are also discussed here.
Chapter 5 deals with the network layer, especially routing, with many routing
algorithms, both static and dynamic, being covered. Even with good routing algorithms though, if more traffic is offered than the network can handle, congestion
can develop, so we discuss congestion and how to prevent it. Even better than
just preventing congestion is guaranteeing a certain quality of service. We will
discuss that topic as well here. Connecting heterogeneous networks to form internetworks leads to numerous problems that are discussed here. The network layer
in the Internet is given extensive coverage.
Chapter 6 deals with the transport layer. Much of the emphasis is on connection-oriented protocols, since many applications need these. An example
transport service and its implementation are discussed in detail. The actual code
is given for this simple example to show how it could be implemented. Both
Internet transport protocols, UDP and TCP, are covered in detail, as are their performance issues. Issues concerning wireless networks are also covered.
Chapter 7 deals with the application layer, its protocols and applications. The
first topic is DNS, which is the Internet’s telephone book. Next comes e-mail,
including a discussion of its protocols. Then we move onto the Web, with detailed discussions of the static content, dynamic content, what happens on the client
side, what happens on the server side, protocols, performance, the wireless Web,
and more. Finally, we examine networked multimedia, including streaming
audio, Internet radio, and video on demand.
Chapter 8 is about network security. This topic has aspects that relate to all
layers, so it is easiest to treat it after all the layers have been thoroughly explained. The chapter starts with an introduction to cryptography. Later, it shows
how cryptography can be used to secure communication, e-mail, and the Web.
The book ends with a discussion of some areas in which security hits privacy,
freedom of speech, censorship, and other social issues collide head on.
Chapter 9 contains an annotated list of suggested readings arranged by
chapter. It is intended to help those readers who would like to pursue their study
of networking further. The chapter also has an alphabetical bibliography of all
references cited in this book.
The author’s Web site at Prentice Hall:
http://www.prenhall.com/tanenbaum
has a page with links to many tutorials, FAQs, companies, industry consortia, professional organizations, standards organizations, technologies, papers, and more.

80

INTRODUCTION

CHAP. 1

1.9 SUMMARY
Computer networks can be used for numerous services, both for companies
and for individuals. For companies, networks of personal computers using shared
servers often provide access to corporate information. Typically they follow the
client-server model, with client workstations on employee desktops accessing
powerful servers in the machine room. For individuals, networks offer access to a
variety of information and entertainment resources. Individuals often access the
Internet by calling up an ISP using a modem, although increasingly many people
have a fixed connection at home. An up-and-coming area is wireless networking
with new applications such as mobile e-mail access and m-commerce.
Roughly speaking, networks can be divided up into LANs, MANs, WANs,
and internetworks, with their own characteristics, technologies, speeds, and
niches. LANs cover a building and operate at high speeds. MANs cover a city,
for example, the cable television system, which is now used by many people to
access the Internet. WANs cover a country or continent. LANs and MANs are
unswitched (i.e., do not have routers); WANs are switched. Wireless networks
are becoming extremely popular, especially wireless LANs. Networks can be
interconnected to form internetworks.
Network software consists of protocols, which are rules by which processes
communicate. Protocols are either connectionless or connection-oriented. Most
networks support protocol hierarchies, with each layer providing services to the
layers above it and insulating them from the details of the protocols used in the
lower layers. Protocol stacks are typically based either on the OSI model or on
the TCP/IP model. Both have network, transport, and application layers, but they
differ on the other layers. Design issues include multiplexing, flow control, error
control, and others. Much of this book deals with protocols and their design.
Networks provide services to their users. These services can be connectionoriented or connectionless. In some networks, connectionless service is provided
in one layer and connection-oriented service is provided in the layer above it.
Well-known networks include the Internet, ATM networks, Ethernet, and the
IEEE 802.11 wireless LAN. The Internet evolved from the ARPANET, to which
other networks were added to form an internetwork. The present Internet is actually a collection of many thousands of networks, rather than a single network.
What characterizes it is the use of the TCP/IP protocol stack throughout. ATM is
widely used inside the telephone system for long-haul data traffic. Ethernet is the
most popular LAN and is present in most large companies and universities.
Finally, wireless LANs at surprisingly high speeds (up to 54 Mbps) are beginning
to be widely deployed.
To have multiple computers talk to each other requires a large amount of standardization, both in the hardware and software. Organizations such as the ITU-T,
ISO, IEEE, and IAB manage different parts of the standardization process.

SEC. 1.9

SUMMARY

81

PROBLEMS
1. Imagine that you have trained your St. Bernard, Bernie, to carry a box of three 8mm
tapes instead of a flask of brandy. (When your disk fills up, you consider that an
emergency.) These tapes each contain 7 gigabytes. The dog can travel to your side,
wherever you may be, at 18 km/hour. For what range of distances does Bernie have a
higher data rate than a transmission line whose data rate (excluding overhead) is 150
Mbps?
2. An alternative to a LAN is simply a big timesharing system with terminals for all
users. Give two advantages of a client-server system using a LAN.
3. The performance of a client-server system is influenced by two network factors: the
bandwidth of the network (how many bits/sec it can transport) and the latency (how
many seconds it takes for the first bit to get from the client to the server). Give an
example of a network that exhibits high bandwidth and high latency. Then give an
example of one with low bandwidth and low latency.
4. Besides bandwidth and latency, what other parameter is needed to give a good characterization of the quality of service offered by a network used for digitized voice
traffic?
5. A factor in the delay of a store-and-forward packet-switching system is how long it
takes to store and forward a packet through a switch. If switching time is 10 µsec, is
this likely to be a major factor in the response of a client-server system where the
client is in New York and the server is in California? Assume the propagation speed
in copper and fiber to be 2/3 the speed of light in vacuum.
6. A client-server system uses a satellite network, with the satellite at a height of 40,000
km. What is the best-case delay in response to a request?
7. In the future, when everyone has a home terminal connected to a computer network,
instant public referendums on important pending legislation will become possible.
Ultimately, existing legislatures could be eliminated, to let the will of the people be
expressed directly. The positive aspects of such a direct democracy are fairly obvious;
discuss some of the negative aspects.
8. A collection of five routers is to be connected in a point-to-point subnet. Between
each pair of routers, the designers may put a high-speed line, a medium-speed line, a
low-speed line, or no line. If it takes 100 ms of computer time to generate and inspect
each topology, how long will it take to inspect all of them?
9. A group of 2n − 1 routers are interconnected in a centralized binary tree, with a router
at each tree node. Router i communicates with router j by sending a message to the
root of the tree. The root then sends the message back down to j. Derive an approximate expression for the mean number of hops per message for large n, assuming that
all router pairs are equally likely.
10. A disadvantage of a broadcast subnet is the capacity wasted when multiple hosts
attempt to access the channel at the same time. As a simplistic example, suppose that

82

INTRODUCTION

CHAP. 1

time is divided into discrete slots, with each of the n hosts attempting to use the channel with probability p during each slot. What fraction of the slots are wasted due to
collisions?
11. What are two reasons for using layered protocols?
12. The president of the Specialty Paint Corp. gets the idea to work with a local beer
brewer to produce an invisible beer can (as an anti-litter measure). The president tells
her legal department to look into it, and they in turn ask engineering for help. As a
result, the chief engineer calls his counterpart at the other company to discuss the
technical aspects of the project. The engineers then report back to their respective
legal departments, which then confer by telephone to arrange the legal aspects.
Finally, the two corporate presidents discuss the financial side of the deal. Is this an
example of a multilayer protocol in the sense of the OSI model?
13. What is the principal difference between connectionless communication and
connection-oriented communication?
14. Two networks each provide reliable connection-oriented service. One of them offers
a reliable byte stream and the other offers a reliable message stream. Are these identical? If so, why is the distinction made? If not, give an example of how they differ.
15. What does ‘‘negotiation’’ mean when discussing network protocols? Give an example.
16. In Fig. 1-19, a service is shown. Are any other services implicit in this figure? If so,
where? If not, why not?
17. In some networks, the data link layer handles transmission errors by requesting damaged frames to be retransmitted. If the probability of a frame’s being damaged is p,
what is the mean number of transmissions required to send a frame? Assume that
acknowledgements are never lost.
18. Which of the OSI layers handles each of the following:
(a) Dividing the transmitted bit stream into frames.
(b) Determining which route through the subnet to use.
19. If the unit exchanged at the data link level is called a frame and the unit exchanged at
the network level is called a packet, do frames encapsulate packets or do packets
encapsulate frames? Explain your answer.
20. A system has an n-layer protocol hierarchy. Applications generate messages of length
M bytes. At each of the layers, an h-byte header is added. What fraction of the network bandwidth is filled with headers?
21. List two ways in which the OSI reference model and the TCP/IP reference model are
the same. Now list two ways in which they differ.
22. What is the main difference between TCP and UDP?
23. The subnet of Fig. 1-25(b) was designed to withstand a nuclear war. How many
bombs would it take to partition the nodes into two disconnected sets? Assume that
any bomb wipes out a node and all of the links connected to it.
24. The Internet is roughly doubling in size every 18 months. Although no one really
knows for sure, one estimate put the number of hosts on it at 100 million in 2001. Use

CHAP. 1

PROBLEMS

83

these data to compute the expected number of Internet hosts in the year 2010. Do you
believe this? Explain why or why not.
25. When a file is transferred between two computers, two acknowledgement strategies
are possible. In the first one, the file is chopped up into packets, which are individually acknowledged by the receiver, but the file transfer as a whole is not acknowledged. In the second one, the packets are not acknowledged individually, but the
entire file is acknowledged when it arrives. Discuss these two approaches.
26. Why does ATM use small, fixed-length cells?
27. How long was a bit on the original 802.3 standard in meters? Use a transmission
speed of 10 Mbps and assume the propagation speed in coax is 2/3 the speed of light
in vacuum.
28. An image is 1024 × 768 pixels with 3 bytes/pixel. Assume the image is
uncompressed. How long does it take to transmit it over a 56-kbps modem channel?
Over a 1-Mbps cable modem? Over a 10-Mbps Ethernet? Over 100-Mbps Ethernet?
29. Ethernet and wireless networks have some similarities and some differences. One
property of Ethernet is that only one frame at a time can be transmitted on an Ethernet.
Does 802.11 share this property with Ethernet? Discuss your answer.
30. Wireless networks are easy to install, which makes them inexpensive since installation
costs usually far overshadow equipment costs. Nevertheless, they also have some
disadvantages. Name two of them.
31. List two advantages and two disadvantages of having international standards for network protocols.
32. When a system has a permanent part and a removable part (such as a CD-ROM drive
and the CD-ROM), it is important that the system be standardized, so that different
companies can make both the permanent and removable parts and everything still
works together. Give three examples outside the computer industry where such international standards exist. Now give three areas outside the computer industry where
they do not exist.
33. Make a list of activities that you do every day in which computer networks are used.
How would your life be altered if these networks were suddenly switched off?
34. Find out what networks are used at your school or place of work. Describe the network types, topologies, and switching methods used there.
35. The ping program allows you to send a test packet to a given location and see how
long it takes to get there and back. Try using ping to see how long it takes to get from
your location to several known locations. From thes data, plot the one-way transit
time over the Internet as a function of distance. It is best to use universities since the
location of their servers is known very accurately. For example, berkeley.edu is in
Berkeley, California, mit.edu is in Cambridge, Massachusetts, vu.nl is in Amsterdam,
The Netherlands, www.usyd.edu.au is in Sydney, Australia, and www.uct.ac.za is in
Cape Town, South Africa.
36. Go to IETF’s Web site, www.ietf.org, to see what they are doing. Pick a project you
like and write a half-page report on the problem and the proposed solution.

84

INTRODUCTION

CHAP. 1

37. Standardization is very important in the network world. ITU and ISO are the main
official standardization organizations. Go to their Web sites, www.itu.org and
www.iso.org, respectively, and learn about their standardization work. Write a short
report about the kinds of things they have standardized.
38. The Internet is made up of a large number of networks. Their arrangement determines
the topology of the Internet. A considerable amount of information about the Internet
topology is available on line. Use a search engine to find out more about the Internet
topology and write a short report summarizing your findings.

