6

INTRODUCTION

CHAP. 1

1.2 HISTORY OF OPERATING SYSTEMS
Operating systems have been evolving through the years. In the following
sections we will briefly look at a few of the highlights. Since operating systems
have historically been closely tied to the architecture of the computers on which
they run, we will look at successive generations of computers to see what their
operating systems were like. This mapping of operating system generations to
computer generations is crude, but it does provide some structure where there
would otherwise be none.
The first true digital computer was designed by the English mathematician
Charles Babbage (1792–1871). Although Babbage spent most of his life and fortune trying to build his ‘‘analytical engine,’’ he never got it working properly
because it was purely mechanical, and the technology of his day could not produce the required wheels, gears, and cogs to the high precision that he needed.
Needless to say, the analytical engine did not have an operating system.
As an interesting historical aside, Babbage realized that he would need
software for his analytical engine, so he hired a young woman named Ada
Lovelace, who was the daughter of the famed British poet Lord Byron, as the
world’s first programmer. The programming language Ada® is named after her.

1.2.1 The First Generation (1945–55) Vacuum Tubes and Plugboards
After Babbage’s unsuccessful efforts, little progress was made in constructing
digital computers until World War II. Around the mid-1940s, Howard Aiken at
Harvard, John von Neumann at the Institute for Advanced Study in Princeton, J.
Presper Eckert and William Mauchley at the University of Pennsylvania, and
Konrad Zuse in Germany, among others, all succeeded in building calculating
engines. The first ones used mechanical relays but were very slow, with cycle
times measured in seconds. Relays were later replaced by vacuum tubes. These
machines were enormous, filling up entire rooms with tens of thousands of
vacuum tubes, but they were still millions of times slower than even the cheapest
personal computers available today.
In these early days, a single group of people designed, built, programmed,
operated, and maintained each machine. All programming was done in absolute
machine language, often by wiring up plugboards to control the machine’s basic
functions. Programming languages were unknown (even assembly language was
unknown). Operating systems were unheard of. The usual mode of operation was
for the programmer to sign up for a block of time on the signup sheet on the wall,
then come down to the machine room, insert his or her plugboard into the computer, and spend the next few hours hoping that none of the 20,000 or so vacuum
tubes would burn out during the run. Virtually all the problems were straightforward numerical calculations, such as grinding out tables of sines, cosines, and logarithms.

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

7

By the early 1950s, the routine had improved somewhat with the introduction
of punched cards. It was now possible to write programs on cards and read them
in instead of using plugboards; otherwise, the procedure was the same.

1.2.2 The Second Generation (1955–65) Transistors and Batch Systems
The introduction of the transistor in the mid-1950s changed the picture radically. Computers became reliable enough that they could be manufactured and
sold to paying customers with the expectation that they would continue to function long enough to get some useful work done. For the first time, there was a
clear separation between designers, builders, operators, programmers, and maintenance personnel.
These machines, now called mainframes, were locked away in specially air
conditioned computer rooms, with staffs of professional operators to run them.
Only big corporations or major government agencies or universities could afford
the multimillion dollar price tag. To run a job (i.e., a program or set of programs), a programmer would first write the program on paper (in FORTRAN or
assembler), then punch it on cards. He would then bring the card deck down to
the input room and hand it to one of the operators and go drink coffee until the
output was ready.
When the computer finished whatever job it was currently running, an operator would go over to the printer and tear off the output and carry it over to the output room, so that the programmer could collect it later. Then he would take one
of the card decks that had been brought from the input room and read it in. If the
FORTRAN compiler was needed, the operator would have to get it from a file
cabinet and read it in. Much computer time was wasted while operators were
walking around the machine room.
Given the high cost of the equipment, it is not surprising that people quickly
looked for ways to reduce the wasted time. The solution generally adopted was
the batch system. The idea behind it was to collect a tray full of jobs in the input
room and then read them onto a magnetic tape using a small (relatively) inexpensive computer, such as the IBM 1401, which was very good at reading cards,
copying tapes, and printing output, but not at all good at numerical calculations.
Other, much more expensive machines, such as the IBM 7094, were used for the
real computing. This situation is shown in Fig. 1-1.
After about an hour of collecting a batch of jobs, the tape was rewound and
brought into the machine room, where it was mounted on a tape drive. The operator then loaded a special program (the ancestor of today’s operating system),
which read the first job from tape and ran it. The output was written onto a second tape, instead of being printed. After each job finished, the operating system
automatically read the next job from the tape and began running it. When the
whole batch was done, the operator removed the input and output tapes, replaced
the input tape with the next batch, and brought the output tape to a 1401 for

8

INTRODUCTION

Card
reader

Tape
drive

Input
tape

Output
tape
Printer

1401

(a)

System
tape

CHAP. 1

(b)

7094

(c)

(d)

1401

(e)

(f)

Figure 1-1. An early batch system. (a) Programmers bring cards to 1401. (b)
1401 reads batch of jobs onto tape. (c) Operator carries input tape to 7094. (d)
7094 does computing. (e) Operator carries output tape to 1401. (f) 1401 prints
output.

printing off line (i.e., not connected to the main computer).
The structure of a typical input job is shown in Fig. 1-2. It started out with a
$JOB card, specifying the maximum run time in minutes, the account number to
be charged, and the programmer’s name. Then came a $FORTRAN card, telling
the operating system to load the FORTRAN compiler from the system tape. It
was followed by the program to be compiled, and then a $LOAD card, directing
the operating system to load the object program just compiled. (Compiled programs were often written on scratch tapes and had to be loaded explicitly.) Next
came the $RUN card, telling the operating system to run the program with the
data following it. Finally, the $END card marked the end of the job. These primitive control cards were the forerunners of modern job control languages and command interpreters.
Large second-generation computers were used mostly for scientific and
engineering calculations, such as solving the partial differential equations that
often occur in physics and engineering. They were largely programmed in FORTRAN and assembly language. Typical operating systems were FMS (the Fortran
Monitor System) and IBSYS, IBM’s operating system for the 7094.

1.2.3 The Third Generation (1965–1980) ICs and Multiprogramming
By the early 1960s, most computer manufacturers had two distinct, and totally
incompatible, product lines. On the one hand there were the word-oriented,
large-scale scientific computers, such as the 7094, which were used for numerical
calculations in science and engineering. On the other hand, there were the
character-oriented, commercial computers, such as the 1401, which were widely
used for tape sorting and printing by banks and insurance companies.
Developing and maintaining two completely different product lines was an
expensive proposition for the manufacturers. In addition, many new computer

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

9

$END
Data for program

$RUN
$LOAD
Fortran program

$FORTRAN
$JOB, 10,6610802, MARVIN TANENBAUM

Figure 1-2. Structure of a typical FMS job.

customers initially needed a small machine but later outgrew it and wanted a
bigger machine that would run all their old programs, but faster.
IBM attempted to solve both of these problems at a single stroke by introducing the System/360. The 360 was a series of software-compatible machines ranging from 1401-sized to much more powerful than the 7094. The machines differed only in price and performance (maximum memory, processor speed, number
of I/O devices permitted, and so forth). Since all the machines had the same
architecture and instruction set, programs written for one machine could run on all
the others, at least in theory. Furthermore, the 360 was designed to handle both
scientific (i.e., numerical) and commercial computing. Thus a single family of
machines could satisfy the needs of all customers. In subsequent years, IBM has
come out with compatible successors to the 360 line, using more modern technology, known as the 370, 4300, 3080, and 3090 series.
The 360 was the first major computer line to use (small-scale) Integrated Circuits (ICs), thus providing a major price/performance advantage over the secondgeneration machines, which were built up from individual transistors. It was an
immediate success, and the idea of a family of compatible computers was soon
adopted by all the other major manufacturers. The descendants of these machines
are still in use at computer centers today. Nowadays they are often used for
managing huge databases (e.g., for airline reservation systems) or as servers for
World Wide Web sites that must process thousands of requests per second.
The greatest strength of the ‘‘one family’’ idea was simultaneously its greatest
weakness. The intention was that all software, including the operating system,

10

INTRODUCTION

CHAP. 1

OS/360 had to work on all models. It had to run on small systems, which often

just replaced 1401s for copying cards to tape, and on very large systems, which
often replaced 7094s for doing weather forecasting and other heavy computing. It
had to be good on systems with few peripherals and on systems with many peripherals. It had to work in commercial environments and in scientific environments. Above all, it had to be efficient for all of these different uses.
There was no way that IBM (or anybody else) could write a piece of software
to meet all those conflicting requirements. The result was an enormous and
extraordinarily complex operating system, probably two to three orders of magnitude larger than FMS. It consisted of millions of lines of assembly language written by thousands of programmers, and contained thousands upon thousands of
bugs, which necessitated a continuous stream of new releases in an attempt to
correct them. Each new release fixed some bugs and introduced new ones, so the
number of bugs probably remained constant in time.
One of the designers of OS/360, Fred Brooks, subsequently wrote a witty and
incisive book (Brooks, 1996) describing his experiences with OS/360. While it
would be impossible to summarize the book here, suffice it to say that the cover
shows a herd of prehistoric beasts stuck in a tar pit. The cover of Silberschatz et
al. (2000) makes a similar point about operating systems being dinosaurs.
Despite its enormous size and problems, OS/360 and the similar thirdgeneration operating systems produced by other computer manufacturers actually
satisfied most of their customers reasonably well. They also popularized several
key techniques absent in second-generation operating systems. Probably the most
important of these was multiprogramming. On the 7094, when the current job
paused to wait for a tape or other I/O operation to complete, the CPU simply sat
idle until the I/O finished. With heavily CPU-bound scientific calculations, I/O is
infrequent, so this wasted time is not significant. With commercial data processing, the I/O wait time can often be 80 or 90 percent of the total time, so something
had to be done to avoid having the (expensive) CPU be idle so much.
The solution that evolved was to partition memory into several pieces, with a
different job in each partition, as shown in Fig. 1-3. While one job was waiting
for I/O to complete, another job could be using the CPU. If enough jobs could be
held in main memory at once, the CPU could be kept busy nearly 100 percent of
the time. Having multiple jobs safely in memory at once requires special
hardware to protect each job against snooping and mischief by the other ones, but
the 360 and other third-generation systems were equipped with this hardware.
Another major feature present in third-generation operating systems was the
ability to read jobs from cards onto the disk as soon as they were brought to the
computer room. Then, whenever a running job finished, the operating system
could load a new job from the disk into the now-empty partition and run it. This
technique is called spooling (from Simultaneous Peripheral Operation On Line)
and was also used for output. With spooling, the 1401s were no longer needed,
and much carrying of tapes disappeared.

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

11

Job 3
Job 2
Job 1

Memory
partitions

Operating
system

Figure 1-3. A multiprogramming system with three jobs in memory.

Although third-generation operating systems were well suited for big scientific calculations and massive commercial data processing runs, they were still
basically batch systems. Many programmers pined for the first-generation days
when they had the machine all to themselves for a few hours, so they could debug
their programs quickly. With third-generation systems, the time between submitting a job and getting back the output was often several hours, so a single misplaced comma could cause a compilation to fail, and the programmer to waste
half a day.
This desire for quick response time paved the way for timesharing, a variant
of multiprogramming, in which each user has an online terminal. In a timesharing
system, if 20 users are logged in and 17 of them are thinking or talking or drinking
coffee, the CPU can be allocated in turn to the three jobs that want service. Since
people debugging programs usually issue short commands (e.g., compile a fivepage procedure†) rather than long ones (e.g., sort a million-record file), the computer can provide fast, interactive service to a number of users and perhaps also
work on big batch jobs in the background when the CPU is otherwise idle. The
first serious timesharing system, CTSS (Compatible Time Sharing System), was
developed at M.I.T. on a specially modified 7094 (Corbató et al., 1962). However, timesharing did not really become popular until the necessary protection
hardware became widespread during the third generation.
After the success of the CTSS system, MIT, Bell Labs, and General Electric
(then a major computer manufacturer) decided to embark on the development of a
‘‘computer utility,’’ a machine that would support hundreds of simultaneous
timesharing users. Their model was the electricity distribution system—when you
need electric power, you just stick a plug in the wall, and within reason, as much
power as you need will be there. The designers of this system, known as MULTICS (MULTiplexed Information and Computing Service), envisioned one
huge machine providing computing power for everyone in the Boston area. The
idea that machines far more powerful than their GE-645 mainframe would be sold
for a thousand dollars by the millions only 30 years later was pure science fiction.
Sort of like the idea of supersonic trans-Atlantic undersea trains now.
333333333333333333333333333333333333333333333333

†We will use the terms ‘‘procedure,’’ ‘‘subroutine,’’ and ‘‘function’’ interchangeably in this book.

12

INTRODUCTION

CHAP. 1

MULTICS was a mixed success. It was designed to support hundreds of users
on a machine only slightly more powerful than an Intel 386-based PC, although it
had much more I/O capacity. This is not quite as crazy as it sounds, since people
knew how to write small, efficient programs in those days, a skill that has subsequently been lost. There were many reasons that MULTICS did not take over the
world, not the least of which is that it was written in PL/I, and the PL/I compiler
was years late and barely worked at all when it finally arrived. In addition, MULTICS was enormously ambitious for its time, much like Charles Babbage’s analytical engine in the nineteenth century.
To make a long story short, MULTICS introduced many seminal ideas into the
computer literature, but turning it into a serious product and a major commercial
success was a lot harder than anyone had expected. Bell Labs dropped out of the
project, and General Electric quit the computer business altogether. However,
M.I.T. persisted and eventually got MULTICS working. It was ultimately sold as a
commercial product by the company that bought GE’s computer business
(Honeywell) and installed by about 80 major companies and universities worldwide. While their numbers were small, MULTICS users were fiercely loyal. General Motors, Ford, and the U.S. National Security Agency, for example, only shut
down their MULTICS systems in the late 1990s, 30 years after MULTICS was
released.
For the moment, the concept of a computer utility has fizzled out but it may
well come back in the form of massive centralized Internet servers to which relatively dumb user machines are attached, with most of the work happening on the
big servers. The motivation here is likely to be that most people do not want to
administrate an increasingly complex and finicky computer system and would
prefer to have that work done by a team of professionals working for the company
running the server. E-commerce is already evolving in this direction, with various
companies running e-malls on multiprocessor servers to which simple client
machines connect, very much in the spirit of the MULTICS design.
Despite its lack of commercial success, MULTICS had a huge influence on
subsequent operating systems. It is described in (Corbató et al., 1972; Corbató and
Vyssotsky, 1965; Daley and Dennis, 1968; Organick, 1972; and Saltzer, 1974). It
also has a still-active Web site, www.multicians.org, with a great deal of information about the system, its designers, and its users.
Another major development during the third generation was the phenomenal
growth of minicomputers, starting with the DEC PDP-1 in 1961. The PDP-1 had
only 4K of 18-bit words, but at $120,000 per machine (less than 5 percent of the
price of a 7094), it sold like hotcakes. For certain kinds of nonnumerical work, it
was almost as fast as the 7094 and gave birth to a whole new industry. It was
quickly followed by a series of other PDPs (unlike IBM’s family, all incompatible) culminating in the PDP-11.
One of the computer scientists at Bell Labs who had worked on the MULTICS
project, Ken Thompson, subsequently found a small PDP-7 minicomputer that no

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

13

one was using and set out to write a stripped-down, one-user version of MULTICS .
This work later developed into the UNIX® operating system, which became popular in the academic world, with government agencies, and with many companies.
The history of UNIX has been told elsewhere (e.g., Salus, 1994). Part of that
story will be given in Chap. 10. For now, suffice it to say, that because the source
code was widely available, various organizations developed their own (incompatible) versions, which led to chaos. Two major versions developed, System V,
from AT&T, and BSD, (Berkeley Software Distribution) from the University of
California at Berkeley. These had minor variants as well. To make it possible to
write programs that could run on any UNIX system, IEEE developed a standard
for UNIX, called POSIX, that most versions of UNIX now support. POSIX defines
a minimal system call interface that conformant UNIX systems must support. In
fact, some other operating systems now also support the POSIX interface.
As an aside, it is worth mentioning that in 1987, the author released a small
clone of UNIX , called MINIX, for educational purposes. Functionally, MINIX is
very similar to UNIX, including POSIX support. A book describing its internal
operation and listing the source code in an appendix is also available (Tanenbaum
and Woodhull, 1997). MINIX is available for free (including all the source code)
over the Internet at URL www.cs.vu.nl/~ast/minix.html.
The desire for a free production (as opposed to educational) version of MINIX
led a Finnish student, Linus Torvalds, to write Linux. This system was developed
on MINIX and originally supported various MINIX features (e.g., the MINIX file
system). It has since been extended in many ways but still retains a large amount
of underlying structure common to MINIX, and to UNIX (upon which the former
was based). Most of what will be said about UNIX in this book thus applies to
System V, BSD, MINIX, Linux, and other versions and clones of UNIX as well.

1.2.4 The Fourth Generation (1980–Present) Personal Computers
With the development of LSI (Large Scale Integration) circuits, chips containing thousands of transistors on a square centimeter of silicon, the age of the personal computer dawned. In terms of architecture, personal computers (initially
called microcomputers) were not all that different from minicomputers of the
PDP-11 class, but in terms of price they certainly were different. Where the minicomputer made it possible for a department in a company or university to have its
own computer, the microprocessor chip made it possible for a single individual to
have his or her own personal computer.
In 1974, when Intel came out with the 8080, the first general-purpose 8-bit
CPU, it wanted an operating system for the 8080, in part to be able to test it. Intel
asked one of its consultants, Gary Kildall, to write one. Kildall and a friend first
built a controller for the newly-released Shugart Associates 8-inch floppy disk and
hooked the floppy disk up to the 8080, thus producing the first microcomputer
with a disk. Kildall then wrote a disk-based operating system called CP/M

14

INTRODUCTION

CHAP. 1

(Control Program for Microcomputers) for it. Since Intel did not think that
disk-based microcomputers had much of a future, when Kildall asked for the
rights to CP/M, Intel granted his request. Kildall then formed a company, Digital
Research, to further develop and sell CP/M.
In 1977, Digital Research rewrote CP/M to make it suitable for running on the
many microcomputers using the 8080, Zilog Z80, and other CPU chips. Many
application programs were written to run on CP/M, allowing it to completely
dominate the world of microcomputing for about 5 years.
In the early 1980s, IBM designed the IBM PC and looked around for software
to run on it. People from IBM contacted Bill Gates to license his BASIC interpreter. They also asked him if he knew of an operating system to run on the PC.
Gates suggested that IBM contact Digital Research, then the world’s dominant
operating systems company. Making what was surely the worst business decision
in recorded history, Kildall refused to meet with IBM, sending a subordinate
instead. To make matters worse, his lawyer even refused to sign IBM’s nondisclosure agreement covering the not-yet-announced PC. Consequently, IBM went
back to Gates asking if he could provide them with an operating system.
When IBM came back, Gates realized that a local computer manufacturer,
Seattle Computer Products, had a suitable operating system, DOS (Disk Operating System). He approached them and asked to buy it (allegedly for $50,000),
which they readily accepted. Gates then offered IBM a DOS/BASIC package,
which IBM accepted. IBM wanted certain modifications, so Gates hired the person who wrote DOS, Tim Paterson, as an employee of Gates’ fledgling company,
Microsoft, to make them. The revised system was renamed MS-DOS (MicroSoft
Disk Operating System) and quickly came to dominate the IBM PC market. A
key factor here was Gates’ (in retrospect, extremely wise) decision to sell MSDOS to computer companies for bundling with their hardware, compared to
Kildall’s attempt to sell CP/M to end users one at a time (at least initially).
By the time the IBM PC/AT came out in 1983 with the Intel 80286 CPU,
MS-DOS was firmly entrenched and CP/M was on its last legs. MS-DOS was later
widely used on the 80386 and 80486. Although the initial version of MS-DOS was
fairly primitive, subsequent versions included more advanced features, including
many taken from UNIX. (Microsoft was well aware of UNIX, even selling a
microcomputer version of it called XENIX during the company’s early years.)
CP/M, MS-DOS, and other operating systems for early microcomputers were
all based on users typing in commands from the keyboard. That eventually
changed due to research done by Doug Engelbart at Stanford Research Institute in
the 1960s. Engelbart invented the GUI (Graphical User Interface), pronounced
‘‘gooey,’’ complete with windows, icons, menus, and mouse. These ideas were
adopted by researchers at Xerox PARC and incorporated into machines they built.
One day, Steve Jobs, who co-invented the Apple computer in his garage,
visited PARC, saw a GUI, and instantly realized its potential value, something
Xerox management famously did not (Smith and Alexander, 1988). Jobs then

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

15

embarked on building an Apple with a GUI. This project led to the Lisa, which
was too expensive and failed commercially. Jobs’ second attempt, the Apple
Macintosh, was a huge success, not only because it was much cheaper than the
Lisa, but also because it was user friendly, meaning that it was intended for users
who not only knew nothing about computers but furthermore had absolutely no
intention whatsoever of learning.
When Microsoft decided to build a successor to MS-DOS, it was strongly
influenced by the success of the Macintosh. It produced a GUI-based system
called Windows, which originally ran on top of MS-DOS (i.e., it was more like a
shell than a true operating system). For about 10 years, from 1985 to 1995, Windows was just a graphical environment on top of MS-DOS. However, starting in
1995 a freestanding version of Windows, Windows 95, was released that incorporated many operating system features into it, using the underlying MS-DOS system only for booting and running old MS-DOS programs. In 1998, a slightly
modified version of this system, called Windows 98 was released. Nevertheless,
both Windows 95 and Windows 98 still contain a large amount of 16-bit Intel
assembly language.
Another Microsoft operating system is Windows NT (NT stands for New
Technology), which is compatible with Windows 95 at a certain level, but a complete rewrite from scratch internally. It is a full 32-bit system. The lead designer
for Windows NT was David Cutler, who was also one of the designers of the
VAX VMS operating system, so some ideas from VMS are present in NT. Microsoft expected that the first version of NT would kill off MS-DOS and all other versions of Windows since it was a vastly superior system, but it fizzled. Only with
Windows NT 4.0 did it finally catch on in a big way, especially on corporate networks. Version 5 of Windows NT was renamed Windows 2000 in early 1999. It
was intended to be the successor to both Windows 98 and Windows NT 4.0. That
did not quite work out either, so Microsoft came out with yet another version of
Windows 98 called Windows Me (Millennium edition).
The other major contender in the personal computer world is UNIX (and its
various derivatives). UNIX is strongest on workstations and other high-end computers, such as network servers. It is especially popular on machines powered by
high-performance RISC chips. On Pentium-based computers, Linux is becoming
a popular alternative to Windows for students and increasingly many corporate
users. (As an aside, throughout this book we will use the term ‘‘Pentium’’ to
mean the Pentium I, II, III, and 4.)
Although many UNIX users, especially experienced programmers, prefer a
command-based interface to a GUI, nearly all UNIX systems support a windowing
system called the X Windows system produced at M.I.T. This system handles the
basic window management, allowing users to create, delete, move, and resize
windows using a mouse. Often a complete GUI, such as Motif, is available to run
on top of the X Windows system giving UNIX a look and feel something like the
Macintosh or Microsoft Windows, for those UNIX users who want such a thing.

16

INTRODUCTION

CHAP. 1

An interesting development that began taking place during the mid-1980s is
the growth of networks of personal computers running network operating systems and distributed operating systems (Tanenbaum and Van Steen, 2002). In
a network operating system, the users are aware of the existence of multiple computers and can log in to remote machines and copy files from one machine to
another. Each machine runs its own local operating system and has its own local
user (or users).
Network operating systems are not fundamentally different from singleprocessor operating systems. They obviously need a network interface controller
and some low-level software to drive it, as well as programs to achieve remote
login and remote file access, but these additions do not change the essential structure of the operating system.
A distributed operating system, in contrast, is one that appears to its users as a
traditional uniprocessor system, even though it is actually composed of multiple
processors. The users should not be aware of where their programs are being run
or where their files are located; that should all be handled automatically and efficiently by the operating system.
True distributed operating systems require more than just adding a little code
to a uniprocessor operating system, because distributed and centralized systems
differ in critical ways. Distributed systems, for example, often allow applications
to run on several processors at the same time, thus requiring more complex processor scheduling algorithms in order to optimize the amount of parallelism.
Communication delays within the network often mean that these (and other)
algorithms must run with incomplete, outdated, or even incorrect information.
This situation is radically different from a single-processor system in which the
operating system has complete information about the system state.

1.2.5 Ontogeny Recapitulates Phylogeny
After Charles Darwin’s book The Origin of the Species was published, the
German zoologist Ernst Haeckel stated that ‘‘Ontogeny Recapitulates Phylogeny.’’ By this he meant that the development of an embryo (ontogeny) repeats
(i.e., recapitulates) the evolution of the species (phylogeny). In other words, after
fertilization, a human egg goes through stages of being a fish, a pig, and so on
before turning into a human baby. Modern biologists regard this as a gross simplification, but it still has a kernel of truth in it.
Something analogous has happened in the computer industry. Each new
species (mainframe, minicomputer, personal computer, embedded computer,
smart card, etc.) seems to go through the development that its ancestors did. The
first mainframes were programmed entirely in assembly language. Even complex
programs, like compilers and operating systems, were written in assembler. By
the time minicomputers appeared on the scene, FORTRAN, COBOL, and other
high-level languages were common on mainframes, but the new minicomputers

SEC. 1.2

HISTORY OF OPERATING SYSTEMS

17

were nevertheless programmed in assembler (for lack of memory). When microcomputers (early personal computers) were invented, they, too, were programmed
in assembler, even though by then minicomputers were also programmed in highlevel languages. Palmtop computers also started with assembly code but quickly
moved on to high-level languages (mostly because the development work was
done on bigger machines). The same is true for smart cards.
Now let us look at operating systems. The first mainframes initially had no
protection hardware and no support for multiprogramming, so they ran simple
operating systems that handled one manually-loaded program at a time. Later
they acquired the hardware and operating system support to handle multiple programs at once, and then full timesharing capabilities.
When minicomputers first appeared, they also had no protection hardware and
ran one manually-loaded program at a time, even though multiprogramming was
well established in the mainframe world by then. Gradually, they acquired protection hardware and the ability to run two or more programs at once. The first
microcomputers were also capable of running only one program at a time, but
later acquired the ability to multiprogram. Palmtops and smart cards went the
same route.
Disks first appeared on large mainframes, then on minicomputers, microcomputers, and so on down the line. Even now, smart cards do not have hard disks,
but with the advent of flash ROM, they will soon have the equivalent of it. When
disks first appeared, primitive file systems sprung up. On the CDC 6600, easily
the most powerful mainframe in the world during much of the 1960s, the file system consisted of users having the ability to create a file and then declare it to be
permanent, meaning it stayed on the disk even after the creating program exited.
To access such a file later, a program had to attach it with a special command and
give its password (supplied when the file was made permanent). In effect, there
was a single directory shared by all users. It was up to the users to avoid file
name conflicts. Early minicomputer file systems had a single directory shared by
all users and so did early microcomputer file systems.
Virtual memory (the ability to run programs larger than the physical memory)
had a similar development. It first appeared in mainframes, minicomputers,
microcomputers and gradually worked its way down to smaller and smaller systems. Networking had a similar history.
In all cases, the software development was dictated by the technology. The
first microcomputers, for example, had something like 4 KB of memory and no
protection hardware. High-level languages and multiprogramming were simply
too much for such a tiny system to handle. As the microcomputers evolved into
modern personal computers, they acquired the necessary hardware and then the
necessary software to handle more advanced features. It is likely that this
development will continue for years to come. Other fields may also have this
wheel of reincarnation, but in the computer industry it seems to spin faster.

