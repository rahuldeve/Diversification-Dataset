1

Geometric Representations of
Graphs
László Lovász
Institute of Mathematics
Eötvös Loránd University, Budapest
e-mail: lovasz@cs.elte.hu

December 11, 2009

2

Contents
0 Introduction

9

1 Planar graphs and polytopes

11

1.1

Planar graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

11

1.2

Planar separation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

15

1.3

Straight line representation and 3-polytopes . . . . . . . . . . . . . . . . . . .

15

1.4

Crossing number . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

17

2 Graphs from point sets
2.1

19

Unit distance graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

2.1.1

The number of edges . . . . . . . . . . . . . . . . . . . . . . . . . . . .

19

2.1.2

Chromatic number and independence number . . . . . . . . . . . . . .

20

2.1.3

Unit distance representation . . . . . . . . . . . . . . . . . . . . . . . .

21

Bisector graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

2.2.1

The number of edges . . . . . . . . . . . . . . . . . . . . . . . . . . . .

24

2.2.2

k-sets and j-edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

27

2.3

Rectilinear crossing number . . . . . . . . . . . . . . . . . . . . . . . . . . . .

28

2.4

Orthogonality graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

31

2.2

3 Harmonic functions on graphs

33

3.1

Definition and uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

33

3.2

Constructing harmonic functions . . . . . . . . . . . . . . . . . . . . . . . . .

35

3.2.1

Linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

35

3.2.2

Random walks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

3.2.3

Electrical networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

36

3.2.4

Rubber bands . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

3.2.5

Connections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

37

4 Rubber bands
4.1

41

Rubber band representation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3

41

4

CONTENTS
4.2

4.3

4.4

Rubber bands, planarity and polytopes . . . . . . . . . . . . . . . . . . . . . .

43

4.2.1

How to draw a graph? . . . . . . . . . . . . . . . . . . . . . . . . . . .

43

4.2.2

How to lift a graph? . . . . . . . . . . . . . . . . . . . . . . . . . . . .

46

4.2.3

How to find the starting face? . . . . . . . . . . . . . . . . . . . . . . .

51

Rubber bands and connectivity . . . . . . . . . . . . . . . . . . . . . . . . . .

52

4.3.1

Convex embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . . .

52

4.3.2

Degeneracy: essential and non-essential . . . . . . . . . . . . . . . . .

53

4.3.3

Connectivity and degeneracy . . . . . . . . . . . . . . . . . . . . . . .

54

4.3.4

Turning the method to an algorithm . . . . . . . . . . . . . . . . . . .

57

Repulsive springs and approximating maximum cut . . . . . . . . . . . . . . .

60

5 Rigidity

63

5.1

Stresses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

63

5.2

Rigidity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

64

5.2.1

Infinitesimal motions . . . . . . . . . . . . . . . . . . . . . . . . . . . .

64

5.2.2

Rigidity of convex 3-polytopes . . . . . . . . . . . . . . . . . . . . . .

67

5.2.3

Generic rigidity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

67

6 Representing graphs by touching domains
6.1

6.2

6.3

73

Coin representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

6.1.1

Koebe’s theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

73

6.1.2

Formulation in the space . . . . . . . . . . . . . . . . . . . . . . . . . .

74

6.1.3

Preparation for the proof . . . . . . . . . . . . . . . . . . . . . . . . .

74

6.1.4

The range of defects . . . . . . . . . . . . . . . . . . . . . . . . . . . .

77

6.1.5

An algorithmic proof . . . . . . . . . . . . . . . . . . . . . . . . . . . .

81

6.1.6

*Another algorithmic proof . . . . . . . . . . . . . . . . . . . . . . . .

82

6.1.7

*From rubber bands to touching circles . . . . . . . . . . . . . . . . .

84

6.1.8

Conformal transformations . . . . . . . . . . . . . . . . . . . . . . . .

85

6.1.9

Applications of circle packing . . . . . . . . . . . . . . . . . . . . . . .

86

6.1.10 Circle packing and the Riemann Mapping Theorem . . . . . . . . . . .

89

*Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

90

6.2.1

Orthogonal circles . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

90

6.2.2

Tangency graphs of general convex domains . . . . . . . . . . . . . . .

92

Square tilings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

93

6.3.1

Current flow through a rectangle . . . . . . . . . . . . . . . . . . . . .

93

6.3.2

Tangency graphs of square tilings . . . . . . . . . . . . . . . . . . . . .

95

CONTENTS

5

7 Analytic functions on graphs

101

7.1

Circulations and homology

7.2

Discrete holomorphic forms from harmonic functions . . . . . . . . . . . . . . 102

7.3

Operations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

7.4

7.3.1

Integration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104

7.3.2

Critical analytic functions . . . . . . . . . . . . . . . . . . . . . . . . . 105

7.3.3

Polynomials, exponentials and approximation . . . . . . . . . . . . . . 106

Nondegeneracy properties of rotation-free circulations . . . . . . . . . . . . . 107
7.4.1

7.5

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 101

Rubber bands and analytic functions . . . . . . . . . . . . . . . . . . . 111

Geometric representations and discrete analytic functions . . . . . . . . . . . 111
7.5.1

Square tilings and analytic functions . . . . . . . . . . . . . . . . . . . 111

7.5.2

Circle packings and analytic functions . . . . . . . . . . . . . . . . . . 113

7.5.3

Touching polygon representations . . . . . . . . . . . . . . . . . . . . . 113

7.6

Novikov’s discrete analytic functions . . . . . . . . . . . . . . . . . . . . . . . 114

7.7

Discrete analytic functions from circle packings . . . . . . . . . . . . . . . . . 114

8 The Colin de Verdière Number
8.1

115

The definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115
8.1.1

Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115

8.1.2

Formal definition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116

8.1.3

The Strong Arnold Property . . . . . . . . . . . . . . . . . . . . . . . 116

8.2

Basic properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

8.3

Small values . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 118

8.4

Nullspace representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.4.1

A nodal theorem for Colin de Verdière matrices . . . . . . . . . . . . . 119

8.4.2

Steinitz representations and Colin de Verdière matrices . . . . . . . . 121

8.5

Gram representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

8.6

Related graph parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121

9 Orthogonal representations

123

9.1

Orthogonal representations: definition . . . . . . . . . . . . . . . . . . . . . . 123

9.2

Smallest cone and the theta function . . . . . . . . . . . . . . . . . . . . . . . 124
9.2.1

Shannon capacity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125

9.2.2

Definition and basic properties of the theta function . . . . . . . . . . 126

9.2.3

More expressions for ϑ . . . . . . . . . . . . . . . . . . . . . . . . . . . 129

9.2.4

More properties . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132

9.2.5

How good is ϑ as an approximation? . . . . . . . . . . . . . . . . . . . 135

9.2.6

Perfect graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137

9.2.7

The TSTAB body and weighted θ-function . . . . . . . . . . . . . . . 138

6

CONTENTS
9.3

Minimum dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139
9.3.1

Minimum dimension with no restrictions . . . . . . . . . . . . . . . . . 139

9.3.2

General position orthogonal representations . . . . . . . . . . . . . . . 140

9.3.3

Faithful orthogonal representations . . . . . . . . . . . . . . . . . . . . 143

9.3.4

Orthogonal representations with the Strong Arnold Property . . . . . 144

9.4

The variety of orthogonal representations . . . . . . . . . . . . . . . . . . . . 147

9.5

Related representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152

10 Graph independence to linear independence

153

10.1 Cover-critical and independence-critical graphs . . . . . . . . . . . . . . . . . 153
11 Metric embeddings

157

11.1 Embeddings in low dimension . . . . . . . . . . . . . . . . . . . . . . . . . . . 157
11.2 Embeddings with small distortion . . . . . . . . . . . . . . . . . . . . . . . . . 158
11.3 Application to multicommodity flows . . . . . . . . . . . . . . . . . . . . . . . 160
11.4 Volume-respecting embeddings . . . . . . . . . . . . . . . . . . . . . . . . . . 161
12 Adjacency matrix and regularity partitions

163

12.1 Similarity metric . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
12.2 Regularity partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
13 Some general issues

165

13.1 Non-degeneracy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165
13.2 Duality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
13.3 Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 166
13.4 Background from linear algebra . . . . . . . . . . . . . . . . . . . . . . . . . . 169
13.4.1 Basic facts about eigenvalues . . . . . . . . . . . . . . . . . . . . . . . 169
13.4.2 Semidefinite matrices

. . . . . . . . . . . . . . . . . . . . . . . . . . . 170

13.4.3 Cross product . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
13.5 Graph theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
13.5.1 Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
13.5.2 Szemerédi partitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173
13.6 Eigenvalues of graphs

. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173

13.6.1 Matrices associated with graphs

. . . . . . . . . . . . . . . . . . . . . 173

13.6.2 The largest eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . . 175
13.6.3 The smallest eigenvalue . . . . . . . . . . . . . . . . . . . . . . . . . . 176
13.6.4 The eigenvalue gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 178
13.6.5 The number of different eigenvalues . . . . . . . . . . . . . . . . . . . 186
13.6.6 Spectra of graphs and optimization . . . . . . . . . . . . . . . . . . . . 188
13.7 Convex polytopes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 189

CONTENTS

7

13.7.1 Polytopes and polyhedra . . . . . . . . . . . . . . . . . . . . . . . . . . 189
13.7.2 The skeleton of a polytope . . . . . .
13.7.3 Polar, blocker and antiblocker . . . . .
13.7.4 Optimization . . . . . . . . . . . . . .
13.8 Semidefinite optimization . . . . . . . . . . .
13.8.1 Semidefinite programs . . . . . . . . .
13.8.2 Fundamental properties of semidefinite
13.8.3 Algorithms for semidefinite programs .

Bibliography

. . . . . .
. . . . . .
. . . . . .
. . . . . .
. . . . . .
programs
. . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

190
191
192
192
193
194
196

199

8

CONTENTS

Chapter 0

Introduction

9

10

CHAPTER 0. INTRODUCTION

Chapter 1

Planar graphs and polytopes
1.1

Planar graphs

A graph G = (V, E) is planar, if it can be drawn in the plane so that its edges are Jordan
curves and they intersect only at their endnodes1 . A plane map is a planar graph with a
fixed embedding. We also use this phrase to denote the image of this embedding, i.e., the
subset of the plane which is the union of the set of points representing the nodes and the
Jordan curves representing the edges.
The complement of a plane map decomposes into a finite number of arcwise connected
pieces, which we call the faces (or countries) of the planar map. We usually denote the
number of nodes, edges and faces of a planar graph by n, m and f .
Every planar map G = (V, E) has a dual map G∗ = (V ∗ , E ∗ ) (Figure 1.1) As an abstract
graph, this can be defined as the graph whose nodes are the faces of G. If the two faces share
k edges, then we connect them in G∗ by k edges, so that each edge e ∈ E will correspond to
an edge e∗ of G∗ . So |E ∗ | = |E|.

Figure 1.1: A planar map and its dual.

This dual has a natural drawing in the plane: in the interior of each face F of G we select
1 We use the word node for the node of a graph, the word vertex for the vertex of a polytope, and the word
point for points in the plane or in other spaces.

11

12

CHAPTER 1. PLANAR GRAPHS AND POLYTOPES

a point vF (which can be called its capital if we use the country terminology), and on each
edge e ∈ E we select a point ue (this will not be a node of G∗ , just an auxiliary point). We
connect vF to the points ue for each edge on the boundary of F by nonintersecting Jordan
curves inside F . If the boundary of F goes through e twice (i.e., both sides of e belong to F ),
then we connect vF to ue by two curves, entering e from two sides. The two curves entering
ue form a single Jordan curve representing the edge e∗ . It is not hard to see that each face
of G∗ will contain a unique node of G, and so (G∗ )∗ = G.
A planar map is called a triangulation if every face has 3 edges. Note that a triangulation
may have parallel edges, but no two parallel edges can bound a face.
In every simple planar map we can introduce new edges to turn all faces into triangles
while keeping the graph simple. In fact, let us draw new edges as long as we can preserving the
planar embedding and creating no parallel edges. The resulting graph G has no cutpoints;
for then it would have a face meeting two blocks of G, and two nodes of the face in different
blocks could be connected by an edge inside this face. So G is 2-connected and, therefore,
each face is a circuit. Suppose that C is the boundary of a face with at least 4 nodes. One of
the two diagonals of C must be missing, since they all ought to run outside C and therefore,
they would cross. This diagonal could be added inside C.
We often need the following basic fact about planar graphs, which we quote without proof
(see Exercise 1.1.
Theorem 1.1.1 (Euler’s Formula) For every connected planar graph, n−m+f = 2 holds.
¤
Some important consequences of Euler’s Formula are the following.
Corollary 1.1.2 (a) A simple planar graph with n nodes has at most 3n − 6 edges.
(b) A simple bipartite planar graph with n nodes has at most 2n − 4 edges.
(c) Every simple planar graph has a node with degree at most 5.
(d) Every simple bipartite planar graph has a node with degree at most 3.
Corollary 1.1.3 The graphs K5 and K3,3 (see Figure 1.2) are not planar.
The following theorem gives a characterization of planar graphs.
Theorem 1.1.4 (Kuratowski’s Theorem) A graph G is embedable in the plane if and
only if it does not contain a subgraph homeomorphic to the complete graph K5 or the complete
bipartite graph K3,3 .
¤
Among planar graphs, 3-connected planar graphs are especially important. A cycle C in
a graph G is called non-separating, if it has no chords, and the removal of its nodes does not
disconnect the graph.

1.1. PLANAR GRAPHS

K

5

13

K

3,3

Figure 1.2: The two Kuratowski graphs. The two drawings on the right hand side
show that both graphs can be drawn in the plane with a single crossing.

Proposition 1.1.5 Let G be a 3-connected planar graph, and C a cycle in G. Then C is
non-separating if and only if it bounds a face.
¤
Corollary 1.1.6 Every simple 3-connected planar graph has an essentially unique embedding
in the plane in the sense that the set of cycles that bound faces is uniquely determined. ¤
The following characterization of 3-connected planar graphs was proved by Tutte [197].
We will not use it and formulate it without proof.
Theorem 1.1.7 Let G be a 3-connected graph. Then every edge of G is contained in at least
two non-separating cycles. G is planar if and only if every edge is contained in exactly two
non-separating cycles.
¤
As a further application of Euler’s Formula, we prove the following lemma, which will be
useful in several arguments later on.
Lemma 1.1.8 In a simple planar map whose edges are 2-colored with red and blue there is
always a node where the red edges (and so also the blue edges) are consecutive.
Proof. We may assume that the graph is connected (else, we can apply the lemma to each
component). Define a happy corner as a pair of edges that are consecutive on the boundary
of a face and one of them is red, the other blue. We are going to estimate the number N of
happy corners in two different ways.
Suppose that the conclusion does not hold. Then for every node v, the color of the edge
changes at least four times if we consider the edges incident with v in their cyclic order
according to the embedding. (In particular, every node must have degree at least four.) So
every node is incident with at least 4 happy corners, which implies that
N ≥ 4n.

(1.1)

On the other hand, let fr be the number of faces with r edges on their boundary. (To
be precise: since we did not assume that the graph is 2-connected, it may happen that an

14

CHAPTER 1. PLANAR GRAPHS AND POLYTOPES

edge has the same face on both sides. In this case we count this edge twice.) The maximum
number of happy corners a face with r edges can have is
(
r − 1, if r is odd,
r,
if r is even.
Thus the number of happy corners is at most
N ≤ 2f3 + 4f4 + 4f5 + 6f6 + 6f7 + . . .

(1.2)

To relate the upper and lower bounds, we use the trivial identities
f = f3 + f4 + f5 + . . .

and

2m = 3f3 + 4f4 + 5f5 + . . .

By (1.1) and Euler’s Formula,
N ≥ 4n = 4(m + 2 − f ) = 8 + (6f3 + 8f4 + 10f5 + . . . ) − (4f3 + 4f4 + 4f5 + . . . )
= 8 + (2f3 + 4f4 + 6f5 + . . . )
This clearly contradicts the upper bound (1.2) on N .

¤

Exercise 1.1 (a) Prove Euler’s Formula. (b) Find a relationship between the numbers of
nodes, edges and faces of a map in the projective plane and on the torus.
Exercise 1.2 Let G be a simple planar graph. Prove that you can add edges to G so that
you make it 3-connected while keeping it simple and planar.
Exercise 1.3 Construct a 2-connected simple planar graph on n nodes which has an exponential (in n) number of different embeddings in the plane. (Recall: two embeddings are
considered different if there is a cycle that is the boundary cycle of a face in one embedding
but not in the other.)
Exercise 1.4 Show that the statement of Lemma 1.1.8 remains valid for maps on the projective plane, but not for maps on the torus.
Exercise 1.5 (a) Let L be a finite set of lines in the plane, not all going through the same
point. Prove that there is a point where exactly two lines in L go through.
(b) Let S be a finite set of points in the plane, not all on a line. Prove that there is a line
which goes through exactly two points in S.
Exercise 1.6 (a) Let L be a finite set of lines in the plane, not all going through the same
point. Color these lines red and blue. Prove that there is a point where at least two lines in
L intersect and all the lines through this point have the same color.
(b) Let S be a finite set of points in the plane, not all on a line. Color these points red
and blue. Prove that there is a line which goes through at least two points in S and all whose
points have the same color.

1.2. PLANAR SEPARATION

1.2

15

Planar separation

The following important theorem about planar graphs was proved by Lipton and Tarjan:
Theorem 1.2.1 Every planar graph G = (V, E) on n nodes contains a set S ⊆ V such that
√
|S| ≤ 4 n, and every connected component of G \ S has at most 2n/3 nodes.
We do not prove this theorem here; a proof (with a weaker bound of 3n/4 on the sizes of
the components) based on circle packing will be presented in Section 6.1.9.
Exercise 1.7 If G is a k × k grid, then every set S of nodes separating G into parts smaller
√
that (1 − c)k 2 has at least 2ck nodes.

1.3

Straight line representation and 3-polytopes

Theorem 1.3.1 (Fáry–Wagner Theorem) Every simple planar map can be drawn in the
plane with straight edges.
Proof. We use induction on the number of nodes. If this is at most 3, the assertion is
trivial. It suffices to prove the assertion for triangulations.
There is an edge xy which is contained in two triangles only. For let x be a node which is
contained inside some triangle T (any point not on the outermost triangle has this property)
and choose x, T so that the number of faces inside T is minimal. Let y be any neighbor of x.
Now if xy belongs to three triangles (x, y, z1 ), (x, y, z2 ) and (x, y, z3 ), then all these triangles
would be properly contained in T and, say, (x, y, z1 ) would contain z3 inside it, contrary to
the minimality of T .
Contract xy to a node p and remove one edge from both arising pairs of parallel edges.
This way we get a new simple triangulation G0 and by the induction hypothesis, there is an
isomorphic triangulation G00 with straight edges.
Now consider the edges pz1 and pz2 of G00 . They split the angle around p into two angles;
one of these contains the edges whose pre-images in G are adjacent to x, the other one those
whose pre-images are adjacent to y. Therefore, we can “pull x and y apart” and get an a
straight line drawing of G.
¤
Let P be a convex 3-polytope. (See Section 13.7 for basic facts about polytopes). The
vertices and edges of P form a graph GP , which we call the skeleton of P .
Proposition 1.3.2 The skeleton of every 3-polytope is a 3-connected planar graph.
We describe the simple proof, because this is our first example of how a geometric representation can be used to derive a purely graph-theoretic property, namely 3-connectivity.

16

CHAPTER 1. PLANAR GRAPHS AND POLYTOPES

Proof. Let F be any facet of P , and let x be a point that is outside P but very close to F ;
more precisely, assume that the plane Σ of F separates x from P , but for every other facet
F 0 , x is on the same side of the plane of F 0 as P . Let us project the skeleton of P from x to
the plane Σ. Then we get an embedding of GP in the plane.
To see that GP is 3-connected, it suffices to show that for any four nodes a, b, c, d there
is a path from a to b which avoids c and d.
If a, b, c, d are not coplanar, then let Π be a plane that separates {a, b} from {c, d}; then
we can connect a and b by a polygon consisting of edges of P that stays on the same side of
Π as a and b, and so avoids c and d (see Appendix 13.7, Corollary 13.7.6).
If a, b, c, d are coplanar, let Π be a plane that contains them. One of the open halfspaces
bounded by Π contains at least one vertex of P . We can then connect a and b by a polygon
consisting of edges of P that stays on this side of Π (except for its endpoints a and b), and
so avoids c and d.
¤
The embedding of GP in the plane constructed above is called the Schlegel diagram of
the polytope (with respect to the facet F ).
The converse of this last proposition is an important and much more difficult theorem,
proved by Steinitz [188]:
Theorem 1.3.3 (Steinitz’s Theorem) A simple graph is isomorphic to the skeleton of a
3-polytope if and only if it is 3-connected and planar.
A bijection between the nodes of a simple graph G and the vertices of a convex polytope
P in R3 that gives a bijection between the edges of G and the edges of P is called a Steinitz
representation of the graph G. We don’t prove Steinitz’s Theorem here, but later constructions of representations by polytopes, and in fact with special properties, will follow from the
material in chapters 4, 6.1 and 8.
There may be many representations of a graph by a 3-polytope; in 3-space the variety of
these representations has nice properties, but in higher dimension it can be very complicated.
We refer to [167].
The construction of the planar embedding of GP in the proof of Proposition 1.3.2 gives an
embedding with straight edges. Therefore Steinitz’s Theorem also proves the Fáry–Wagner
theorem, at least for 3-connected graphs. It is easy to see that the general case can be reduced
to this by adding new edges so as to make the graph 3-connected (see exercise 1.2).
Finally, we note that the Steinitz representation is also related to planar duality.
Proposition 1.3.4 Let P be a convex polytope with the origin in its interior, and let P ∗ be
its polar. Then the skeletons GP are GP ∗ are dual planar graphs.
Proof.
¤

1.4. CROSSING NUMBER

1.4

17

Crossing number

If G is a non-planar graph, we may want to draw it so as to minimize the number of intersections of the curves representing the edges. To be more precise, we assume that the drawing
satisfies the following conditions: none of the edges is allowed to pass through a node, no
three edges pass through the same point, any two edges intersect in a finite number of points,
and every common point of two edges other than a common endpoint is an intersection point
(so edges cannot touch). The minimum number of intersection points in such a drawing is
called the crossing number of the graph, and denoted by cr(G).
The question whether cr(G) = 0, i.e., whether G is planar, is decidable in polynomial
time; but to compute cr(G) in general is NP-hard.
It is ot even known what the crossing numbers of complete are complete bipartite graphs
are. Guy proved (by constructing an appropriate drawing of Kn in the plane) that
µ ¶
1 n
n−1
n−2
n−3
3 n
cr(Kn ) ≤ b c · b
c·b
c·b
c<
.
4 2
2
2
2
8 4

(1.3)

Zarankiewicz proved that
µ ¶µ ¶
n
n−1
m
m−1
1 n m
cr(Kn,m ) ≤ b c · b
c·b c·b
c<
.
2
2
2
2
4 2
2

(1.4)

In both cases, it is conjectured that equality holds in the first inequality.
We will need lower bounds on the crossing number. We start with an easy observation.
Lemma 1.4.1 Let G be a simple graph with n nodes and m edges, then
cr(G) ≥ m − 3n + 6.
Proof. Consider a drawing of G with a minimum number of crossings. If m ≤ 3n − 6,
then the assertion is trivial. If m > 3n − 6, then by Corollary 1.1.2(a), there is at least one
crossing. Deleting one of the edges participating in a crossing, the number of crossings drops
by at least one, and the lemma follows by induction.
¤
If m is substantially larger than n, then this bound becomes very week. The following
theorem of Ajtai, Chvátal, Newborn, and Szemerédi [3] and Leighton [123] gives a much
better bound when m is large, which is best possible except for the constant.
Theorem 1.4.2 Let G be a simple graph with n nodes and m edges, where m ≥ 4n. Then
cr(G) ≥

m3
.
64n2

18

CHAPTER 1. PLANAR GRAPHS AND POLYTOPES

Proof.

Consider a drawing of G with a minimum number of crossings. Let p = (4n)/m,

and delete each node of G independently with probability 1 − p. The remaining graph H
satisfies
cr(H) ≥ |E(H)| − 3|V (H)| + 6 > |E(H)| − 3|V (H)|.
Take the expectation of both sides. Then
E(|V (H)|) = pn,

E(|E(H)) = p2 m,

E(cr(H)) ≤ p4 cr(G).

(For the last inequality: a crossing of G survives if and only if all four endpoints of the two
participating edges survive, so the drawing of H we get has, in expectation, p4 cr(G) crossings.
However, this may not be an optimal drawing of H, hence we only get an inequality.) Thus
cr(G) ≥

m 3n
m3
−
=
.
p2
p3
64n2
¤

Chapter 2

Graphs from point sets
2.1

Unit distance graphs

Given a set S of points in Rd , we construct the unit distance graph on S by connecting two
points by an edge if and only if their distance is 1. Many basic properties of these graphs
have been studied, most of which turn out quite difficult.

2.1.1

The number of edges

Let S be a finite set of n points in the plane. A natural problem is the following [61]: how
many times can the same distance (say, distance 1) occur between the points of a set S of
size n. In this first paper Erdős gave the following upper bound.
Theorem 2.1.1 The unit distance graph of n points in the plane has at most n3/2 edges.
Proof. The graph of unit distances contains no K2,3 .

¤

It was not easy to improve these simple bounds. The only substantial progress was made
by Spencer, Szemerédi and Trotter [185], who proved the following bound, which is still the
best known.
Theorem 2.1.2 The unit distance graph of n points in the plane has at most n4/3 edges.
Proof. We describe a recent elegant proof of this bound by Székely [194].

¤

The proof just given also illuminates that this bound depends not only on graph theoretic
properties of G(S) but, rather, on properties of its actual embedding in the plane. The
difficulty of improving the bound n4/3 is in part explained by a construction of Brass [31],
which shows that in some Banach norm, n points in the plane can determine as many as
Θ(n3/2 ) unit distances.
19

20

CHAPTER 2. GRAPHS FROM POINT SETS
The situation in R3 is probably even less understood, but in dimensions 4 and higher, an

easy construction of Lenz shows that every finite complete k-partite graph can be realized in
R2k as the graph of unit distances: place the points on two orthogonal planes, at distance
√
1/ 2 from the origin. Hence the maximum number of unit distances between n points in Rd
(d ≥ 4) is Θ(n2 ).
Another intriguing unsolved question is the case when the points in S form the vertices
of a convex n-gon. Erdős and Moser conjectured the upper bound to be O(n). The best
construction [58] gives 2n − 7 and the best upper bound is O(n log n) [78].
We get different, and often more accessible questions if we look at the largest distances
among n points. In 1937 Hopf and Pannwitz [103] proved the following.
Theorem 2.1.3 The largest distance among n points in the plane occurs at most n times.
Proof. The proof of this is left to the reader as Exercise 2.1.

¤

Exercise 2.1 Let S be a set of n points in the plane.
(a) Prove that any two longest segments connecting points in S have a point in common.
(b) Prove that the largest distance between points in S occurs at most n times.
For 3-space, Heppes and Révész [93] proved that the largest distance occurs at most
2n − 2 times. In higher dimensions the number of maximal distances can be quadratic:
Lenz’s construction can be carried out so that all the n2 /4 unit distances are maximal. The
exact maximum is known in dimension 4 [30].

2.1.2

Chromatic number and independence number

The problem of determining the maximum chromatic number of a unit distance graph in the
plane was raised by Hadwiger [86]. He proved the following bounds; we don’t know more
about this even today.
Theorem 2.1.4 Every planar unit distance graph has chromatic number at most 7. There
is a planar unit distance graph with chromatic number 4.
Proof.

Figure 2.1(a) shows a coloring of all points of the plane with 7 colors such that

no two points at distance 1 have the same color. Figure 2.1(b) shows a unit distance graph
with chromatic number 4: Suppose it can be colored with three colors, and let the top point
v have color red (say). The other two points in the left hand side triangle containing v must
be green and blue, so the left bottom point must be red again. Similarly, the right bottom
point must be red, so we get two adjacent red points.
¤

2.1. UNIT DISTANCE GRAPHS

(a)

21

(b)

Figure 2.1: (a) The plane can be colored with 7 colors. (b) Three colors are not
enough.

For higher dimensions, we know in a sense more. Klee noted that χ(G(Rd )) is finite
for every d, and Larman and Rogers [121] proved the bound χ(G(Rd )) < (3 + o(1))d . Erdős
conjectured and Frankl and Wilson [76] proved that the growth of χ(Rd ) is indeed exponential
in d: χ(Rd ) > (2 − o(1))d . The exact base of this exponential function is not known.
One gets interesting problems by looking at graph formed by largest distances. The
chromatic number of the graph of the largest distances is equivalent to the well known Borsuk
problem: how many sets with smaller diameter can cover a convex body in Rd ? Borsuk
conjectured that the answer is d + 1, which is not difficult to prove this if the body is smooth.
The bound is also proved for d ≤ 3. (Interestingly the question in small dimensions was settled
by simply using upper bound on the number of edges in the graph of the largest distances,
discussed above). But the general case was settled in the negative by Kahn and Kalai [109],
who proved that the minimum number of covering sets (equivalently, the chromatic number
√
of the graph of largest distances) can be as large as 2Ω( d) .

2.1.3

Unit distance representation

We can turn the above question around and ask: Given a graph G, how can we represent it
by unit distances? To be precise, a unit distance representation of a graph G = (V, E) is a
mapping u : V → Rd for some d ≥ 1 such that |ui − uj | = 1 for every ij ∈ E (we allow that
|ui − uj | = 1 for some nonadjacent nodes i, j).
Every finite graph has a unit distance representation in a sufficiently high dimension. (For
example, we can map its nodes onto the vertices of a regular simplex with edges of length
1.) The first problem that comes to mind, first raised by Erdős, Harary and Tutte [66], is
to find the minimum dimension dim(G) in which a graph has a unit distance representation.
Figure 2.2 shows a 2-dimensional unit distance representation of the Petersen graph [66]. It

22

CHAPTER 2. GRAPHS FROM POINT SETS

turns out that this minimum dimension is linked to the chromatic number of the graph:
lg χ(G) ≤ d(G) ≤ 2χ(G).
The lower bound follows from the results of Larman and Rogers [121] mentioned above; the
upper bound follows from a modification of Lenz’s construction.

Figure 2.2: A unit distance representation of the Petersen graph.

There are many other senses in which we may want to find the most economical unit
distance representation. We only discuss one: what is the smallest radius of a ball containing
a unit distance representation of G (in any dimension)? This question can be answered using
semidefinite programming (see Appendix 13.16).
Considering the Gram matrix A = (uT
i uj ), it is easy to obtain the following reduction to
semidefinite programming:
Proposition 2.1.5 A graph G has a unit distance representation in a ball of radius R (in
some appropriately high dimension) if and only if there exists a positive semidefinite matrix
A such that
Aii ≤ R2
Aii − 2Aij + Ajj = 1

(i ∈ V )
(ij ∈ E).

In other words, the smallest radius R is the square root of the optimum value of the semidefinite program
minimize w
subject to A º 0
Aii ≤ w

(i ∈ V )

Aii − 2Aij + Ajj = 1

(ij ∈ E).

2.1. UNIT DISTANCE GRAPHS

23

The unit distance embedding of the Petersen graph in Figure 2.2 is not an optimal solution of this problem. Let us illustrate how semidefinite optimization can find the optimal
embedding by determining this for the Petersen graph. In the formulation above, we have
to find a 10 × 10 positive semidefinite matrix A satisfying the given linear constraints. For a
given w, the set of feasible solutions is convex, and it is invariant under the automorphisms of
the Petersen graph. Hence there is an optimum solution which is invariant under these automorphisms (in the sense that if we permute the rows and columns by the same automorphism
of the Petersen graph, we get back the same matrix).
Now we know that the Petersen graph has a very rich automorphism group: not only
can we transform every node into every other node, but also every edge into every other
edge, and every nonadjacent pair of nodes into every other non-adjacent pair of nodes. A
matrix invariant under these automorphisms has only 3 different entries: one number in
the diagonal, another number in positions corresponding to edges, and a third number in
positions corresponding to nonadjacent pairs of nodes. This means that this optimal matrix
A can be written as
A = xP + yJ + zI,
where P is the adjacency matrix of the Petersen graph, J is the all-1 matrix, and I is the
identity matrix. So we only have these 3 unknowns x, y and z to determine.
The linear conditions above are now easily translated into the variables x, y, z. But what
to do with the condition that A is positive semidefinite? Luckily, the eigenvalues of A can
also be expressed in terms of x, y, z. The eigenvalues of P are well known (and easy to
compute): they are 3, 1 (5 times) and -2 (4 times). Here 3 is the degree, and it corresponds
to the eigenvector 1 = (1, . . . , 1). This is also an eigenvector of J (with eigenvalue 10), and
so are the other eigenvectors of P , since they are orthogonal to 1, and so are in the nullspace
of J. Thus the eigenvalues of xP + yJ are 3x + 10y, x, and −2x. Adding zI just shifts the
spectrum by z, so the eigenvalues of A are 3x + 10y + z, x + z, and −2x + z. Thus the positive
semidefiniteness of A, together with the linear constraints above, gives the following linear
program for x, y, z, w:
minimize w
subject to 3x + 10y + z ≥ 0,
x + z ≥ 0,
−2x + z ≥ 0,
y + z ≤ w,
2z − 2x = 1.

24

CHAPTER 2. GRAPHS FROM POINT SETS

It is easy to solve this: x = −1/4, y = 1/20, z = 1/4, and w = 3/10. Thus the smallest
p
radius of a ball in which the Petersen graph has a unit distance representation is 3/10.
The corresponding matrix A has rank 4, so this representation is in 4 dimension.
It would be difficult to draw a picture of this representation, but we can offer the following
nice matrix, whose columns will realize this representation (the center of the smallest ball
containing it is not in the origin!):


1 1 1 1 0 0 0 0 0 0
1 0 0 0 1 1 1 0 0 0

1
0 1 0 0 1 0 0 1 1 0
(2.1)


2
0 0 1 0 0 1 0 1 0 1
0 0 0 1 0 0 1 0 1 1
(This matrix reflects the fact that the Petersen graph is the complement of the line-graph of
K5 .)

2.2
2.2.1

Bisector graphs
The number of edges

For a set S of n = 2m points in R2 , we construct the bisector graph GS on S by connecting
two points by an edge if and only if the line through them has m − 2 points on each side.
Lemma 2.2.1 Let v ∈ S, let e, f be two edges of GS incident with v that are consecutive in
the cyclic order of such edges, let W be the wedge between e and f , and let W 0 be the negative
of W . Then there is exactly one edge of GS incident with v in the wedge W 0 .
Proof. Draw a line `, with orientation, through v, and rotate it counterclockwise by π.
Consider the number of points of S on its left hand side. This number changes from less
than m − 2 to larger than m − 2 whenever the negative half of e passes through an edge of
GS incident with v, and it changes from larger than m − 2 to less than m − 2 whenever the
positive half of e passes through an edge of GS incident with v.
¤
Corollary 2.2.2 Let v ∈ S, and let ` be a line through v that does not pass through any
other point in S. Let H be the halfplane bounded by e that contains at most m − 1 points.
Then for some d ≥ 0, d edges incident with v are contained in H and d + 1 of them are on
the opposite side of e.
Corollary 2.2.3 Every node in GS has odd degree.
Corollary 2.2.4 Let ` be a line that does not pass through any point in S. Let k and n − k
of the points be contained on the two sides of `, where k ≤ m. Then ` intersects exactly k
edges of GS .

2.2. BISECTOR GRAPHS

25

From this corollary it is easy to deduce:
Theorem 2.2.5 The number of edges of GS is at most 2n3/2 .
Proof.
Consider a line that is not parallel to any segment connecting two points of S,
√
and call its direction “vertical”. Let k = d ne, and let us divide S by k − 1 vertical lines
`1 , . . . , `k−1 into k parts, each containing at most k points.
By Corollary 2.2.4, each line `i intersects at most m edges of GS , so the number of those
edges which intersect any of the lines `i is at most (k − 1)m ≤ (k − 1)k 2 /2. On the other
hand, the number of those edges of GS that do not intersect any `i is obviously at most
¡ ¢
k k2 = (k − 1)k 2 /2. Hence the number of edges of GS is at most (k − 1)k 2 ≤ 2n3/2 .
¤
While the proof above seems to have a lot of room for improvement, the easy arguments
only lead to an improvement in the constant. But using the “crossing number method”, a
substantially better bound can be derived [51]:
Theorem 2.2.6 The number of edges of GS is at most 4n4/3 .
Proof. We prove that
µ ¶
n
cr(GS ) ≤
.
2

(2.2)

Theorem 1.4.2 implies then that
|E(GS )| ≤ (64n2 cr(GS ))1/3 < 4n4/3 .
To prove (2.2), note that we already have a drawing of GS in the plane, and it suffices to
estimate the number of crossings between edges in this drawing. Fix a coordinate system so
that no segment connecting two points of S is vertical. Let P = v0 v1 . . . vk be a path in GS .
We call P a convex chain, if
(i) the x-coordinates of the vi are increasing,
(ii) the slopes of the edges vi vi+1 are increasing,
(iii) for 1 ≤ i ≤ k − 1, there is no edge incident with vi whose slope is between the slopes
of vi−1 vi and vi vi+1 ,
(iv) P is maximal with respect to this property.
Note that we could relax (iii) by excluding such an edge going to the right from vi ; indeed,
if an edge vi u has slope between the slopes of vi−1 vi and vi vi+1 , and the x-coordinate of u
is larger than that of vi , then there is also an edge wvi that has slope between the slopes of
vi−1 vi and vi vi+1 , and the x-coordinate of w is smaller than that of vi . This follows from
Lemma 2.2.1.

26

CHAPTER 2. GRAPHS FROM POINT SETS

Claim 2.2.7 Every edge of GS belongs to exactly one convex chain.
Indeed, the edge as a path of length 1 satisfies (i)-(iii), and so it will be contained in a
maximal such path. On the other hand, if two convex chains share and edge, then they must
“branch” somewhere, which is impossible as they both satisfy (iii).
We can use convex chains to charge every intersection of two edges of GS to a pair of
nodes of GS as follows. Suppose that the edges ab and cd intersect at a point p, where the
notation is such that a is to the left from b, c is to the left of d, and the slope of ab is larger
than the slope of cd. Let P = v0 v1 . . . ab . . . vk and Q = u0 u1 . . . cd . . . ul be the convex chains
containing ab and cd, respectively. Let vi be the first point on P for which the semiline vi vi+1
intersects Q, and let uj be the last point on Q for which the semiline ui−1 ui intersects P .
We charge the intersection of ab and cd to the pair (vi , uj ).
Claim 2.2.8 Both P and Q are contained in the same side of the line vi uj .
Assume (for convenience) that vi uj lies on the x-axis. We may also assume that P goes
at least as deep below the x-axis as Q.
If the slope of vi−1 vi is positive, then the semiline vi−1 vi intersects Q, contrary to the
definition of i. Similarly, the slope of uj uj+1 (if j < l) is positive.
If the slope of vi vi+1 is negative, then the semiline vi vi+1 stays below the x-axis, and P
is above this semiline. So if it intersects Q, then Q must reach farther below the x-axis than
P , a contradiction.
Thus we know that the slope of vi−1 vi is negative but the slope of vi vi+1 is positive, which
implies by convexity that P stays above the x-axis. By our assumption, this implies that Q
also stays above the x-axis, which proves the claim.
Claim 2.2.9 At most one convex chain starts at each node.
Assume that P = v0 v1 . . . ab . . . vk and Q = u0 u1 . . . cd . . . ul are two convex chains with
u0 = v0 . The edges v0 v1 and u0 u1 are different, so we may assume that the slope of u0 u1 is
larger. By Lemma 2.2.1, there is at least one edge tu0 entering u0 whose slope is between the
slopes of v0 v1 and u0 u1 . We may assume that tu0 has the largest slope among all such edges.
But then appending the edge tu0 to Q conditions (i)-(iii) are preserved, which contradicts
(iv).
To complete the proof of (2.2), it suffices to show:
Claim 2.2.10 At most one intersection point is charged to every pair (v, u) of points of S.
Suppose not. Then there are two pairs of convex chains P, Q and P 0 , Q0 such that p and
P 0 pass through v, Q and Q0 pass through u, some edge of P to the right of v intersects an
edge of Q to the left of P , and similarly for P 0 and Q0 . We may assume that P 6= P 0 . Let

2.2. BISECTOR GRAPHS

27

vw and vw0 be the edges of P and P 0 going from v to the right. Claim 2.2.7 implies that
these edges are different, and so we may assume that the slope of uw is larger than the slope
of uw0 ; by Claim 2.2.8, these slopes are positive. As in the proof of Claim 2.2.9, we see that
Q does not start at v, and in fact it continues to the left by an edge with positive slope. But
this contradicts Claim 2.2.8.
¤

2.2.2

k-sets and j-edges

A k-set of S is a subset T ⊆ S of cardinality k such that T can be separated from its
complement T \ S by a line. An i-set with 1 ≤ i ≤ k is called an (≤ k)-set.
A j-edge of S is an ordered pair uv, with u, v ∈ S and u 6= v, such that there are exactly
j points of S on the right hand side of the line uv. Let ej = ej (S) denote the number of
j-edges of S; it is well known and not hard to see that for 1 ≤ k ≤ n − 1, the number of
k-sets is ek−1 . An i-edge with i ≤ j will be called a (≤ j)-edge; we denote the number of
(≤ j)-edges by Ej = e0 + . . . + ej .
It is not hard to derive a sharp lower bound for each individual ej . We will use the
following theorem from [60], which can be proved extending the proof of Corollary 2.2.4.
Theorem 2.2.11 Let S be a set of n points in the plane in general position, and let T be
a k-set of S. Then for every 0 ≤ j ≤ (n − 2)/2, the number of j-edges uv with u ∈ T and
v ∈ S \ T is exactly min(j + 1, k, n − k).
As an application, we prove the following bounds on ej (the upper bound is due to Dey
[51], the lower bound, as we shall see, is an easy consequence of Theorem 2.2.11).
Theorem 2.2.12 For every set of n points in the plane in general position and for every
j < n−2
2 ,
2j + 3 ≤ ej ≤ 8nj 1/3 .
For every j ≥ 0 and n ≥ 2j + 3, this bound is attained.
Proof. The proof of the upper bound is a rather straightforward extension of the proof of
Theorem 2.2.6.
To prove the lower bound, consider any j-edge uv, and let e be the line obtained by
shifting the line uv by a small distance so that u and v are also on the right hand side, and
let T be the set of points of S on the smaller side of e. This will be the side containing u and
v unless n = 2j + 3; hence |T | ≥ j + 1. By Theorem 2.2.11, the number of j-edges xy with
x ∈ T , y ∈ S \ T is exactly j + 1. Similarly, the number of j-edges xy with y ∈ T , x ∈ S \ T
is exactly j + 1, and these are distinct from the others since n ≥ 2j + 3. Together with uv,
this gives a total of 2j + 3 such pairs.

¤

28

CHAPTER 2. GRAPHS FROM POINT SETS
The following construction shows that the lower bound in the previous theorem is sharp.

Despite significant progress in recent years, the problem of determining the (order of magnitude of) the best upper bound remains open. The current best construction, due to Tóth
√
[196], gives a set of points with ej ≈ neΘ( log k) .
Example 2.2.13 Let S0 be a regular (2j + 3)-gon, and let S1 be any set of n − 2j − 3 points
in general position very near the center of S0 .
Every line through any point in S1 has at least j + 1 points of S0 on both sides, so the
j-edges are the longest diagonals of S0 , which shows that their number is 2j + 3.
We now turn to estimating the number Ej of ≤ j-edges. Theorem 2.2.12 immediately
implies the following lower bound on the number of (≤ j)-edges:
j
X

ei ≥ (j + 2)2 − 1.

(2.3)

i=0

This is, however, not tight, since the constructions showing the tightness of the lower bound
ej ≥ 2j + 3 are diffierent for different values of j. The following bounds can be proved by
more complicated arguments.
Theorem 2.2.14 Let S be a set of n points in the plane in general position, and j < (n −
2)/2. Then
µ
¶
j+2
3
≤ Ej ≤ n(j + 1).
2
.
The upper bound, due to Alon and Győri [10], is tight, as shown by a convex polygon;
the lower bound [1, 142] is also tight for k ≤ n/3, as shown by the following example:
Example 2.2.15 Let r1 , r2 and r3 be three rays emanating from the origin with an angle of
120◦ between each pair. Suppose for simplicity that n is divisible by 3 and let Si be a set of
n/3 points in general position, all very close to ri but at distance at least 1 from each other
and from the origin. Let S = S1 ∪ S2 ∪ S3 .
Then for 1 ≤ k ≤ n/3, every k-set of S contains the i points farthest from 0 in one Sa ,
for some 1 ≤ i ≤ k, and the (k − i) points farthest from 0 in another Sb . Hence the number
¡ ¢
of k-sets is 3k and the number of (≤ k)-sets equals 3 k+1
2 .

2.3

Rectilinear crossing number

The rectilinear crossing number of a graph G is the minimum number of crossings in a drawing
of G in the plane with straight edges and nodes in general position. The Fáry–Wagner

2.3. RECTILINEAR CROSSING NUMBER

29

Theorem 1.3.1 says that if the crossing number of a graph is 0, then so is its rectilinear
crossing number. Is it true more generally that these two crossing numbers are equal? The
answer is negative: the rectilinear crossing number may be strictly larger than the crossing
number [183]. As a general example, the crossing number of the complete graph Kn is less
¡ ¢
¡ ¢
¡ ¢
than 38 n4 = 0.375 n4 (see (1.3)), while its rectilinear crossing number is at least 0.37501 n4
if n is large enough [142].
The rectilinear crossing number of the complete n-graph Kn can also be defined as follows.
Let S be a set of n points in general position in the plane, i.e., no three points are collinear.
Four points in S may or may not form the vertices of a convex quadrilateral; if they do,
we call this subset of 4 elements convex. We are interested in the number cS (S) of convex
¡ ¢
4-element subsets. This can of course be as large as n4 , if S is in convex position, but what
is its minimum?
The graph K5 is not planar, and hence every 5-element set has at least one convex 4element subset, from which it follows by straightforward averaging that at least 1/5 of the
4-element subsets are convex for every n ≥ 5.
¡ ¢
The best upper bound on the minimum number of convex quadrilaterals is 0.3807 n4 ,
obtained using computer search by Aichholzer, Aurenhammer and Krasser [2]. As an application of the results on k-sets in Section 2.2.2, we prove the following lower bound [1, 142]:
Theorem 2.3.1 Let S be a set of n points in the plane in general position. Then the number
¡ ¢
cS (S) of convex quadrilaterals determined by S is at least 0.375 n4 .
The constant in the bound can be improved to 0.37501 [142]. This tiny improvement is
significant, as we have seen, but we cannot give the proof here.
The first ingredient of our proof is an expression of the number of convex quadrilaterals
in terms of j-edges of S.
Let cS denote the number of 4-tuples of points in S that are in convex position, and let
bS denote the number of those in concave (i.e., not in convex) position. We are now ready
to state the crucial lemma [205, 142], which expresses cS as a positive linear combination of
the numbers ej (one might say, as the second moment of the distribution of j-edges).
Lemma 2.3.2 For every set of n points in the plane in general position,
cS =

X
j< n−2
2

µ
ej

n−2
−j
2

Proof. Clearly we have
µ ¶
n
cS + bS =
.
4

¶2

µ ¶
3 n
−
.
4 3

(2.4)

30

CHAPTER 2. GRAPHS FROM POINT SETS

To get another equation between these quantities, let us count, in two different ways, ordered
4-tuples (u, v, w, z) such that w is on the right of the line uv and z is on the left of this line.
First, if {u, v, w, z} is in convex position, then we can order it in 4 ways to get such an
ordered quadruple; if {u, v, w, z} is in concave position, then it has 6 such orderings. Hence
the number of such ordered quadruples is 4cS + 6bS . On the other hand, any j-edge uv it
can be completed to such a quadruple in j(n − j − 2) ways. So we have
4cS + 6bS =

n−2
X

ej j(n − j − 2).

(2.5)

j=0

From (2.4) and (2.5) we obtain


µ ¶ n−2
X
1 n
6
−
cS =
ej (n − j − 2)j  .
2
4
j=0
Using that
n−2
X

ej = n(n − 1),

(2.6)

j=0

we can write
µ ¶ n−2
X (n − 2)(n − 3)
n
6
=
ej
4
4
j=0
to get
cS =

µ
¶
n−2
1X
(n − 2)(n − 3)
ej
− j(n − j − 2) ,
2 j=0
4

from which the lemma follows by simple computation, using (2.6) and that ej = en−2−j . ¤
Having expressed cS (up to some error term) as a positive linear combination of the ej ’s,
we can substitute any lower estimates for the numbers ej to obtain a lower bound for cS .
Using (2.3), we get
µ ¶
1 n
cS ≥
+ O(n3 ).
4 4
This lower bound for cS is quite weak. To obtain the stronger lower bound stated in Theorem
2.3.1, we do “integration by parts”, i.e., we pass from j-facets to (≤ j)-facets. We substitute
ej = Ej − Ej−1 in Lemma 2.3.2 and rearrange to get the following:
Lemma 2.3.3 For every set of n points in the plane in general position,
µ ¶
X
3 n
cS =
+ εn ,
Ej (n − 2j − 3) −
4 3
n−2
j<

2

2.4. ORTHOGONALITY GRAPHS

31

where
(
εn =

1
n−3
4E 2 ,

if n is odd,

0,

if n is even.

Note that the last two terms in the formula of this lemma are O(n3 ). Using these bounds,
Theorem 2.3.1 follows.

2.4

Orthogonality graphs

For a given d, let Gd denote the graph whose nodes are all unit vectors in Rd (i.e., the points
of the unit sphere S d−1 ), and we connect two nodes if and only if they are orthogonal.
Example 2.4.1 The graph G2 consists of disjoint 4-cycles.
Theorem 2.4.2 (Erdős–De Bruijn Theorem) Let G be an infinite graph and k ≥ 1, and
integer. Then G is k-colorable if and only if every finite subgraph of G is k-colorable.
Proof. See [129], Exercise 9.14.
Theorem 2.4.3 There exists a constant c > 1 such that
cd ≤ χ(Gd ) ≤ 4d .

¤

32

CHAPTER 2. GRAPHS FROM POINT SETS

Chapter 3

Harmonic functions on graphs
In this section we consider general graphs, but some of the most important applications will
concern planar graphs.
The notion of a harmonic function is basic in analysis, usually defined as smooth functions
f : Rd → R (perhaps defined only in some domain) satisfying the differential equation
Pd
2
2
∆f = 0, where ∆ =
i=1 ∂ /∂xi is the Laplace operator. It is a basic fact that such
functions can be characterized by the “mean value property” that their value at any point
equals to their average on a ball around this point.
Taking this second characterization as our starting point, we can define an analogous
notion of harmonic functions defined on the nodes of a graph.
Harmonic functions play an important role in the study of random walks (after all, the
averaging in the definition can be interpreted as expectation after one move). They also
come up in the theory of electrical networks, and also in statics. This provides a connection
between these fields, which can be exploited. In particular, various methods and results from
the theory of electricity and statics, often motivated by physics, can be applied to provide
results about random walks. From our point of view, their main applications will be rubber
band representations and discrete analytic functions. In this chapter we develop this simple
but useful theory.

3.1

Definition and uniqueness

Let G = (V, E) be a connected simple graph and S ⊆ V . A function f : V → R is called a
harmonic at a node v ∈ V if
1
dv

X

f (u) = f (v)

∀v ∈ V \ S.

(3.1)

u∈N (v)

33

34

CHAPTER 3. HARMONIC FUNCTIONS ON GRAPHS

This equation can also be written as
X

(f (u) − f (v)) = 0

(3.2)

u∈N (v)

A node where a function is not harmonic is called a pole of the function.
We can extend this notion to multigraphs by replacing (3.1) by
1
dv

X

f (u) = f (v)

∀v ∈ V \ S.

(3.3)

e∈E
V (e)={u,v}3v

Another way of writing this is
X

auv (f (u) − f (v)) = 0

∀v ∈ V \ S,

(3.4)

u∈V

where auv is the multiplicity of the edge uv. We could further generalize this by assigning
arbitrary nonnegative weights βuv to the edges of G (uv ∈ E(G)), and require the condition
X

βuv (f (u) − f (v)) = 0

∀v ∈ V \ S,

(3.5)

u∈V

Every constant function is harmonic at each node. On the other hand,
Proposition 3.1.1 Every non-constant harmonic function has at least two poles.
Proof. Let S be the set where the function assumes its maximum, and let S 0 be the set of
those nodes in S that are connected to any node outside S. Then every node in S 0 must be
a pole, since in (3.1), every value f (u) on the left had side is at most f (v), and at least one
is less, so the average is less than f (v). Since the function is nonconstant, S is a nonempty
proper subset of V , and since the graph is connected, S 0 is nonempty. So there is a pole
where the function attains its maximum. Similarly, there is another pole where it attains its
minimum.
¤
For any two nodes there will be a nonconstant harmonic function that is harmonic everywhere else. More generally, we have the following theorem.
Theorem 3.1.2 For every nonempty set S ⊆ V and every function f0 : S → R there is a
unique function f : V → R extending f0 that is harmonic at each node of V \ S.
We call this function f the harmonic extension of f0 .
The uniqueness of the harmonic extension is easy by the argument in the proof of Proposition 3.1.1. Suppose that f and f 0 are two harmonic extensions of f0 . Then g = f − f 0 is
harmonic on V \ S, and satisfies g(v) = 0 at each v ∈ S. If g is the identically 0 function,

3.2. CONSTRUCTING HARMONIC FUNCTIONS

35

then f = f 0 as claimed. Else, either is minimum or its maximum is different from 0. But we
have seen that both these sets contain at least one pole, which is a contradiction.
Before proving the existence of harmonic extensions in the next section, we make a few
remarks.
If |S| = 1, then this unique extension must be the constant function. Thus Theorem 3.1.2
implies Proposition 3.1.1 (but it was instructive to prove that separately).
If S = {a, b}, then a function that is harmonic outside S is uniquely determined by two of
its values, f (a) and f (b). Scaling the function by a real number and translating by a constant
preserves harmonicity at each node, so if we know the function f with (say) f (a) = 0 and
f (b) = 1, harmonic at v 6= a, b, then g(v) = A+(B −A)f (v) describes the harmonic extension
with g(a) = A, g(b) = B.
Note that equation (3.2) is equivalent to saying that Fij = f (j) − f (i) defines a flow from
a to b.

3.2

Constructing harmonic functions

There are several ways to construct a harmonic function with given poles, and we describe
four of them. This abundance of proof makes sense, because each of these constructions
illustrates an application area of harmonic functions.

3.2.1

Linear algebra

First we construct a harmonic function with two given poles a and b. Let χa : V → R denote
the function which is 1 on a and 0 everywhere else. Consider the equation
Lf = χb − χa ,
(where L is the Laplacian of the graph). Every function satisfying this is harmonic outside
{a, b}.
The matrix L is not quite invertible, but it has a one-dimensional nullspace spanned by
the vector 1 = (1, . . . , 1)T , and so it determines f up to adding the same scalar to every
entry. So we may assume that 1T f = 0. If J ∈ RV ×V denotes the all-1 matrix, then L + J
is invertible, and
(L + J)f = Lf = χb − χa ,
and so we can express f as
f = (L + J)−1 (χb − χa ).

(3.6)

All other functions harmonic outside {a, b} can be obtained from f by multiplying by a
constant and adding another constant.

36

CHAPTER 3. HARMONIC FUNCTIONS ON GRAPHS
Second, suppose that |S| ≥ 3, let a, b ∈ S, and let ga,b denote the function with is

harmonic outside a and b, and satisfies ga,b (a) = 0, ga,b (b) = 1. Note that every linear
combination of the functions ga,b is harmonic outside S.
0
0
Let ga,b
denote the restriction of ga,b to S. Fix a. We claim that the functions ga,b
, b ∈ S\
P
P
0
{a}, are linearly independent. Indeed, a relation b αa,b ga,b
= 0 implies that b αa,b ga,b = 0
P
(by the uniqueness of the harmonic extension), which in turn implies b αa,b Lga,b = 0. But
the value of this last function at b is αa,b Lga,b (b), so every αa,b = 0.
0
So the functions ga,b
generate the (|S| − 1)-dimensional space of all functions f ∈ RS
with f (a) = 0. If we throw in the constant functions, they generate the whole space RV .
0
This means that f0 is a linear combination of functions ga,b
and a constant function. The
corresponding linear combination of the ga,b is the harmonic extension of f .

3.2.2

Random walks

Let G = (V, E) be a connected graph and a, b ∈ V . Let f (v) denote the probability that
a random walk starting at node v hits b before it hits a. Then f is harmonic at all nodes
except a and b. Clearly we have f (a) = 0 and f (b) = 1.
More generally, given a set S ⊆ V and a function f0 : S → R, we define f (v) for v ∈ V \ S
as the expectation of f0 (s), where s is the (random) node where a random walk starting at
v first hits S. (One needs some facts from the theory of random walks, for example, that a
random walk hits the set S with probability 1.)
Then f (v) is harmonic at V \ S. Indeed, let v 0 = v, v 1 , . . . be the random walk. If v ∈
/ S,
then, conditioning on the first step,
E(f (s)) =

X

X

P(v 1 = j)E(f (s) | v 1 = j) =

j∈N (v)

j∈N (v)

1
f (j).
dv

Moreover, it is clear that f (v) = f0 (v) for all v ∈ S.

3.2.3

Electrical networks

Consider the graph G as an electrical network, where each edge represents a unit resistance.
Assume that an electric current is flowing through G, entering at a and leaving at b. Let
f (v) be the potential of node v. Then f is a harmonic except at a and b.
More generally, given a set S ⊆ V and a function f0 : S → R, let a current flow through
the network while keeping each v ∈ S at potential f0 (v). Then the potential f (v) defined for
all nodes v is an extension of f0 . Furthermore, the current through an edge uv is f (u) − f (v)
by Ohm’s Law, and hence Kirchhoff’s Current Law, (3.2) holds for every v ∈ V \ S.

3.2. CONSTRUCTING HARMONIC FUNCTIONS

3.2.4

37

Rubber bands

Consider the edges of the graph G as ideal rubber bands (or springs) with unit Hooke constant
(i.e., it takes h units of force to stretch them to length h). Grab the nodes a and b and stretch
the graph so that their distance is 1. Then the distance f(v) of node v from a is a function
that is harmonic for v 6= a, b, and satisfies f (a) = 0 and f (v) = 1.
More generally, if we have a set S ⊆ V and we fix the positions of each node v ∈ S at a
given point f0 (v) of the real line, and let the remaining nodes find their equilibrium, then the
position f (v) of a node will be a function that extends f0 (trivially), and that is harmonic
at each v ∈
/ S. To see the last fact, note that the edge uv pulls v with force f (u) − f (v) (the
force is positive if it pulls in the positive direction), and so (3.2) is just the condition of the
equilibrium.
We’ll come back to this construction repeatedly, extending it to higher dimensions. We’ll
prove that the equilibrium exists and is unique without any reference to physics.

3.2.5

Connections

A consequence of the uniqueness property in Theorem 3.1.2 is that the harmonic functions
constructed in sections 3.2.1, 3.2.2, 3.2.3 and 3.2.4 are the same. As an application of this
idea, we show the following interesting identities (see Nash-Williams [156], Chandra at al.
[38]).
Considering the graph G as an electrical network, let Rst denote the effective resistance
between nodes s and t. Considering the graph G as a spring structure in equilibrium, with
two nodes s and t nailed down at 1 and 0, let Fab denote the force pulling the nails. Doing a
random walk on the graph, let κ(a, b) denote the commute time between nodes a and b (i.e.,
the expected time it takes to start at a, walk until you first hit b, and then walk until you
first hit a again).
Theorem 3.2.1
Rab =
Proof.

1
κab
=
.
Fab
2m

Consider the function f satisfying Lf (a) = 1,

P
v

f (v) = 0 and harmonic for

v 6= a, b. By the discussion in section 3.2.3, f (v) is equal to the potential of v if we push a
unit current through G from a to b. So the effective resistance is Rab = f (b) − f (a).
The second equality is similarly easily derived from Hooke’s Law.
Finally, we know that for an appropriate A and B, Af (u) + B is the probability that a
random walk starting at u visits a before b. Checking this for u = s and u = t, we get that
P
A = 1/(f (b) − f (a)) and B = −f (u)/(f (b) − f (a)). Hence p0 = d1a u∈N (a) (A(f (u) − f (a))
is the probability that a random walk starting at a hits b before returning to t.

38

CHAPTER 3. HARMONIC FUNCTIONS ON GRAPHS
Let T be the first time when a random walk starting at a returns to a and S, the first

time when it returns to a after visiting b. We know from the theory of random walks that
E(T ) = 2m/da and by definition, E(S) = κ(a, b). Clearly T ≤ S and the probability of
T = S is exactly p0 . This implies that E(S − T ) = (1 − p0 )E(S), since if T < S, then after
the first T steps, we have to walk from a until we reach b and then return to b. Hence
p0 =

E(T )
2m
=
.
E(S)
da κ(a, d)
¤

For the rubber band model, imagine that we slowly stretch the graph until nodes a and
b will be at distance 1. When they are at distance t, the force pulling our hands is tFab , and
hence the energy we have to spend is
Z 1
1
tFab dt = Fab .
2
0
By conservation of energy, we get the identity
X
(f (i) − f (j))2 = Fab .

(3.7)

ij∈E

Using the “topological formulas” from the theory of electrical networks for the resistance,
we get a further characterization of these quantities:
Corollary 3.2.2 Let G0 denote the graph obtained from G by identifying a and b, and let
T (G) denote the number of spanning trees of G. Then
Rab =

T (G)
.
T (G0 )

Another application of this equivalence between our three models can be used to prove
the following.
Theorem 3.2.3 Let G = (V, E) be a graph and a, b ∈ V . Let G0 be the graph obtained from
G by connecting two nodes by a new edge.
(a) The commute time between a and b does not increase.
(b) (Raleigh’s Theorem) The effective resistance between nodes a and b does not increase.
(c) If nodes a and b nailed down at 0 and 1, the force pulling the nails in the equilibrium
does not decrease.
Proof.

The three statements are equivalent by Theorem 3.2.1; we prove (c). By (3.7),

it suffices to prove that the equilibrium energy does not decrease. Consider the equilibrium

3.2. CONSTRUCTING HARMONIC FUNCTIONS

39

position of G0 , and delete the new edge. The contribution of this edge to the energy was
nonnegative, so the total energy decreases. The current position of the nodes may not be in
equilibrium; but the equilibrium position minimizes the energy, so when they move to the
equilibrium of G0 , the energy further decreases.
¤
Exercise 3.1 Let a, b be two nodes of a connected graph G. We define the hitting time
H(a, b) as the expected number of steps before a random walk, started at a, will reach b.
Consider the graph as a rubber band structure as above, but also attach a weight of d(v)
to each node v. Hold the graph at b and let it find its equilibrium. Prove that a will be at
distance H(a, b) below b.

40

CHAPTER 3. HARMONIC FUNCTIONS ON GRAPHS

Chapter 4

Rubber bands
4.1

Rubber band representation

Let G = (V, E) be a connected graph and ∅ 6= S ⊆ V . Fix an integer d ≥ 1 and a map
x0 : S → Rd . We extend this to a map x : V → Rd (a geometric representation of G) as
follows.
First, let’s give an informal description. Replace the edges by ideal rubber bands (satisfying Hooke’s Law). Think of the nodes in S as nailed to their given position (node i ∈ S to
x0i ∈ Rd ), but let the other nodes settle in equilibrium. We’ll see that this equilibrium position is uniquely determined. We call it the rubber band representation of G in Rd extending
x0 .
To be precise, let xi ∈ Rd be the position of node i ∈ V . By definition, xi = x0i for i ∈ S.
The energy of this representation is defined as
E(x) =

X

|xi − xj |2

ij∈E

We want to find the representation with minimum energy, subject to the boundary conditions:
minimize E(x)

(4.1)
x0i

subsect to xi =

for all i ∈ S.

(4.2)

Lemma 4.1.1 If S 6= ∅, then the function E(x) is strictly convex.

Proof. We have
E(x) =

d
XX

(xik − xjk )2 .

ij∈E k=1

41

42

CHAPTER 4. RUBBER BANDS

Figure 4.1: Rubber band representation of the Petersen graph, with one pentagon
nailed (dark nodes).
1
Every function (xik − xjk )2 is convex, so E is convex. Suppose that E( x+y
2 ) = 2 (E(x) + E(y))
for some x, y : V → Rd . Then for every edge ij and every 1 ≤ k ≤ d we have

µ

xjk + yjk
xik + yik
−
2
2

¶2
=

(xik − xjk )2 + (xik − xjk )2
,
2

which implies that xik − yik = xjk − yjk . Since xik = yik for every i ∈ S, it follows that
xik = yik for every i ∈ V . So x = y, which means that E is strictly convex.
¤
It is trivial that if any of the xi tends to infinity, then E(x) tends to infinity (still assuming
the boundary conditions 4.2 hold, where S is nonempty). With Lemma 4.1.1 this implies
that the representation with minimum energy is uniquely determined. If i ∈ V \ S, then at
the minimum point the partial derivative of E(x) with respect to any coordinate of x must
be 0. This means that for every i ∈ V \ S,
X

(xi − xj ) = 0.

(4.3)

j∈N (i)

This we can rewrite as
xi =

1 X
xj .
di

(4.4)

j∈N (i)

This equation means that every free node is in the center of gravity of its neighbors. Equation
(4.3) also has a nice physical meaning: the rubber band connecting i and j pulls i with force
xj −xi , so (4.3) states that the forces acting on i sum to 0 (as they should at the equilibrium).

4.2. RUBBER BANDS, PLANARITY AND POLYTOPES

43

We saw in Chapter 3 that every 1-dimensional rubber band representation gives rise
to a harmonic function. Equation (4.4) extends this to higher dimensional rubber band
representations:
Proposition 4.1.2 In a rubber band representation, each coordinate function is harmonic
at every free node.
It will be useful to extend the rubber band construction to the case when the edges of G
have arbitrary positive weights. Let wij denote the weight of the edge ij. We then define
the energy function of a representation i 7→ xi by
Ew (x) =

X

wij |xi − xj |2 .

ij∈E

The simple arguments above remain valid: Ew is strictly convex if at least one node is nailed,
there is a unique optimum, and for the optimal representation every i ∈ V \ S satisfies
X

wij (xi − xj ) = 0.

(4.5)

j∈N (i)

This we can rewrite as
xi = P

X

1

j∈N (i)

wij

wij xj .

(4.6)

j∈N (i)

Thus xi is no longer in the center of gravity of its neighbors, but it is still a convex combination
of them with positive coefficients. In other words, it is in the relative interior of the convex
hull of its neighbors.
Exercise 4.1 Prove that Emin (w) := minx Ew (x) (where the minimum is taken over all
representations x with some nodes nailed) is a concave function of w.
Exercise 4.2 Let G = (V, E) be a connected graph, ∅ 6= S ⊆ V , and x0 : S → Rd . Extend
x0 to x : V \ S → Rd as follows: starting a random walk at j, let i be the (random) node
where S is first hit, and let xj denote the expectation of the vector x0i . Prove that x is the
same as the rubber band extension of x0 .

4.2
4.2.1

Rubber bands, planarity and polytopes
How to draw a graph?

The rubber band method was first analyzed by Tutte [197]. In this classical paper he describes
how to use “rubber bands” to draw a 3-connected planar graph with straight edges and convex
faces.

44

CHAPTER 4. RUBBER BANDS

Figure 4.2: Rubber band representations of the skeletons of platonic bodies

Let G = (V, E) be a 3-connected planar graph, and let F0 be any face of it. Let C0 be
the cycle bounding F0 . Let us map the nodes of C0 on the vertices of a convex polygon P0
in the plane, in the same cyclic order. Let i 7→ vi be the rubber band representation of G in
the plane with extending this map. We also draw the edges of G as straight line segments
connecting the appropriate endpoints. We call this mapping the rubber band representation
of G with C0 nailed. Figure 4.2 shows the rubber band representation of the skeletons of the
five platonic bodies.
By the above, we know that each node not on C0 is positioned at the center of gravity of
its neighbors. Tutte’s main result about this embedding is the following:
Theorem 4.2.1 If G is a simple 3-connected planar graph, then its rubber band representation gives an embedding of G in the plane.
Proof. The key to the proof is the following claim.
Claim 1. Let ` be a line intersecting the polygon P0 , and let U be the set of nodes of G that
are mapped onto a given (open) side of `. Then U induces a connected subgraph of G.
Clearly the nodes of C0 in U form a path P . We may assume that ` is not parallel to any
edge. Let a ∈ S \ V (C0 ), then va is in the center of gravity of its neighbors, and so either it
is mapped onto the same point of the plane as all its neighbors (a degenerate case), or it has
a neighbor a1 such that va1 is on the same side of ` as a, but farther from `. In the second
case, we find a neighbor a2 of a1 such that va2 is on the same side of ` as va1 , but farther
from `, etc. This way we get a path Q in G that connects a to P , such that V (Q) ⊆ S.
In the degenerate case, consider all nodes mapped onto va , and a connected component
H of the subgraph of G induced by them. If H contains a nailed node then it contains a
path from a to P , all in U . Else, there must be an edge connecting a node a1 in H to a node

4.2. RUBBER BANDS, PLANARITY AND POLYTOPES

45

a2 with va2 6= va , and then, since the system is in equilibrium, this node can be chosen so
that va2 is at least as far from ` than va (here we use that no edge is parallel to `). From
here the proof goes just as in the nondegenerate case. (See Figure 4.3).

Figure 4.3: Every line cuts a rubber band embedding into connected parts.

Next, we exclude a possible degeneracy. Call a node degenerate if there is a line such that
the node and all its neighbors get positioned on this line.
Claim 2. No degenerate nodes exist.
Suppose that there are degenerate nodes; then there is a line ` which contains a node and
all its neighbors. Fix this line, and consider the subgraph induced by all degenerate nodes
on ` with all neighbors also on `, and let H be a connected component of this subgraph (H
may be a single node). Let S be the set of neighbors of H (outside H). Then |S| ≥ 3 by
3-connectivity.
Let U1 and U2 be the sets of nodes of G on the two sides of `. We claim that each node a
in S is connected to both U1 and U2 . By the definition of S, a is positioned on `, but it has
a neighbor that is not on `, and so it has a neighbor in U1 ∪ U2 . If a ∈
/ V (C0 ), then a is the
center of gravity of its neighbors, and so it cannot happen that it has a neighbor on one side
of ` but not the other. If a ∈ V (C0 ), then its two neighbors along C0 are on different sides
of `.
Now V (H) induces a connected graph by definition, and U1 and U2 induce connected
subgraphs by Claim 1. So we can contract these sets to single nodes. These three nodes will
be adjacent to all nodes in S. So G can be contracted to K3,3 , which is a contradiction since
it is planar. This proves Claim 2.
Claim 3. Let ab be an edge that is not an edge of C0 , and let F1 and F2 be the two faces
incident with ab. Then all other nodes of F1 are mapped on one side of the line ` through va
and vb , and all other nodes of F2 are mapped on the other side.

46

CHAPTER 4. RUBBER BANDS
Suppose not, then F1 has a node c and F2 has a node d such that vc and vd are both

on (say) the positive side of `, or on ` itself. In the latter case, they have a neighbor on
the positive side of `, by Claim 2. So by Claim 1, there is a path P connecting c and d
whose internal nodes are positioned on the positive side of `. Similarly, there is a path P 0
connecting a and b whose internal nodes are positioned on the negative side of `. Thus P
and P 0 are disjoint. But look at the planar embedding: the edge ab, together with P 0 , forms
a Jordan curve that separates b and d, so P cannot exist.
Claim 4. The boundary of every face F is mapped onto a convex polygon PF .
This is immediate from Claim 3, since the line of an edge never intersects the interior of
the face.
Claim 5. The interiors of the polygons PF (where F is a bounded face) are disjoint.
Let x be a point inside P0 , we want to show that it is covered by one PF only. Clearly
we may assume that x is not on the image of any edge. Draw a line through x that does not
go through the image any node, and see how many times its points are covered by interiors
of such polygons. As we enter P0 , this number is clearly 1. Claim 2 says that as the line
crosses an edge, this number does not change. So x is covered exactly once.
Now the proof is essentially finished. Suppose that the images of two edges have a common
point. Then two of the faces incident with them would have a common interior point, which
is a contradiction except if these faces are the same, and the two edges are consecutive edges
of this face.
¤
Before going on, let’s analyze this proof a little. The key step, namely Claim 1, is
very similar to a basic fact concerning convex polytopes, namely Corollary 13.7.7. Let us
call a geometric representation of a graph section-connected, if for every open halfspace, the
subgraph induced by those nodes that are mapped into this halfspace is connected (or empty).
The skeleton of a polytope, as a representation of itself, is section-connected; and so is the
rubber-band representation of a planar graph. Note that the proof of Claim 1 did not make
use of the planarity of G; in fact, the same proof gives:
Lemma 4.2.2 Let G be a connected graph, and let w be a geometric representation of an
induced subgraph H of G (in any dimension). If w is section-connected, then its rubber-band
extension to G is also section-connected.

4.2.2

How to lift a graph?

An old construction of Cremona and Maxwell can be used to “lift” Tutte’s rubber band
representation to a Steinitz representation.

4.2. RUBBER BANDS, PLANARITY AND POLYTOPES

47

Theorem 4.2.3 Let G = (V, E) be a 3-connected planar graph, and let T be a triangular
face of G. Let
µ ¶
vi1
i 7→ vi =
∈ R2
vi2
be a rubber band representation of G obtained by nailing T to any triangle in the plane. Then
we can assign a number ηi ∈ R to each i ∈ V such that ηi = 0 for i ∈ V (T ), ηi > 0 for
i ∈ V \ V (T ), and the mapping
 
vi1
i 7→ ui = vi2  ∈ R3
ηi
is a Steinitz representation of G.
Before starting with the proof, we need a little preparation to deal with edges on the
boundary triangle. Recall that we can think of Fij = vi − vj as the force with which the edge
ij pulls its endpoint j. Equilibrium means that for every internal node j,
X

Fij = 0.

(4.7)

i∈N (j)

This does not hold for the nailed nodes, but we can modify the definition of Fij along the
three boundary edges so that (4.7) will hold for all nodes (this is the only point where we
use that the outer face is a triangle). This is natural by a physical argument: let us replace
the outer edges by rigid bars, and remove the nails. The whole structure will remain in
equilibrium, so appropriate forces must act in the edges ab, bc and ac to keep balance.
To translate this to mathematics, one has to work a little; this is leaft to the reader as
an exercise.
Exercise 4.3 Let G = (V, E) be a simple 3-connected planar graph with a triangular face
T . Let v be a rubber band representation of G in the plane with {a, b, c, } nailed. Define
Fij = vi − vj for all edges in E \ E(T ). Then we can define Fij for i, j ∈ V (T ), i 6= j so that
(a) Fij = −Fji for all ij ∈ E,
(b) (4.7) holds for all nodes i ∈ V ,
(c) Fij is parallel to vj − vi .
Now we are ready to prove theorem 4.2.3.
Proof.

Imagine that we have the proper lifting. Let’s call the third coordinate direction
“vertical”. For each face F , let gF be a normal vector. Since no face is parallel to a vertical
line, we can normalize gF so that its third coordinate is 1. Clearly for each face F , gF will
be an outer normal, except for F = T , when gF is an inner normal.

48

CHAPTER 4. RUBBER BANDS
Write gF =

¡hF ¢
1

. Let ij be any edge of G, and let F1 and F2 be the two faces incident

with ij. Then both gF1 and gF2 are orthogonal to the edge ui uj of the polytope, and therefore
so is their difference. Since
¶¶
µµ
¶¶T µµ
hF1 − hF2
vi − vj
T
(gF1 − gF2 ) (ui − uj ) =
= (hF1 − hF2 )T (vi − vj ),
0
ηi − ηj
we get
(hF1 − hF2 )T (vi − vj ) = 0.

(4.8)

We also have
hT = 0,
since the facet T is not lifted.
Using that not only gF1 − gF2 , but also gF1 is orthogonal to the edge vi vj , we get from
gFT1 (ui − uj ) = hT
F1 (vi − vj ) + (ηi − ηj )
that
ηi − ηj = −hT
F1 (vi − vj ).

(4.9)

This discussion allows us to explain the plan of the proof: given the Tutte representation,
we first reconstruct the vectors hi so that all equations (4.8) are satisfied, then using these,
we reconstruct the numbers ηi so that equations (4.9) are satisfied. It will not be hard to
verify then that we get a Steinitz representation.
Let R denote the counterclockwise rotation in the plane by 90◦ . We claim that we can
replace (4.8) by the stronger equation
hF1 − hF2 = RFij

(4.10)

and still have a solution. Starting with hT = 0, and moving from face to adjacent face, this
equation will determine the value of hF for every face. What we have to show is that we
don’t run into contradiction, i.e., if we get to the same face F in two different ways, then we
get the same vector hF . This is equivalent to saying that if we walk around a closed cycle
of faces, then the total change in the vector hF is zero. We can think of this closed cycle
as a Jordan curve in the plane, that does not go through any nodes, and crosses every edge
at most once. We want to show that the sum of RFij over all edges that it crosses is zero
(where the order of the endpoints is determined so that the Jordan curve crosses the edge
from right to left).
From (4.7) we have that
X
RFij = 0.
i∈N (j)

4.2. RUBBER BANDS, PLANARITY AND POLYTOPES

49

Summing this over all nodes j for which xj lies in the interior of the Jordan curve, the terms
corresponding to edges with both endpoints inside cancel (since Fij + Fji = 0), and we get
that the sum is 0 for the edges crossing the Jordan curve. This proves that we can define the
vectors hF .
Second, we construct numbers ηi satisfying (4.9) by a similar argument. We set ηi = 0
if i is an external node. Equation (4.9) tells us what the value at one endpoint of an edge
must be, if we have it for the other endpoint.
Again, the main step is to prove that we don’t get a contradiction when coming back to
a value we already defined.
The first concern is that (4.9) gives two conditions for each, depending on which face
incident with it we choose. But if F1 and F2 are two faces incident with the edge ij, then
T
T
T
hT
F1 (vi − vj ) − hF2 (vi − vj ) = (hF1 − hF2 ) (vi − vj ) = (RFij ) (vi − vj ) = 0,

since Fij is parallel to vi − vj and so RFij is orthogonal to it. Thus the two conditions on
the difference ηi − ηj are the same; in other words,
χij = −hT
F (vi − vj )
depends only on the edge ij.
Now consider a cycle C in G, and (for reference) orient it counterclockwise. We want to
show that the total change of ηi we prescribed along this cycle is 0. For every face F with
boundary cycle ∂F , we have
X
X
hT
χij =
F (vi − vj ) = 0.
ij∈E(∂F )

ij∈E(∂F )

Summing this over all faces inside C the contribution of every edge cancels except for the
edges on C. This proves that
X
χij = 0
ij∈E(C)

as claimed.

¡ ¢
¡ ¢
Now define ui = ηvii for every node i and gF = h1F for every face F . It remains to prove
that i 7→ ui maps the nodes of G onto the vertices of a convex polytope, so that edges go to
edges and faces go to faces. We start with observing that if F is a face and ij is an edge of
F , then
gFT ui − gFT uj = hT
F (vi − vj ) + (ηi − ηj ) = 0,
and hence there is a scalar αF so that all nodes of F are mapped onto the hyperplane
gFT x = αF . We know that the image of F under i 7→ vi is a convex polygon, and so the same
follows for the map i 7→ ui .

50

CHAPTER 4. RUBBER BANDS
To conclude, it suffices to prove that if ij is any edge, then the two convex polygons

obtained as images of faces incident with ij “bend” in the right way; more exactly, let F1
and F2 be the two faces incident with ij, and let QF1 and QF2 be two corresponding convex
polygons. We claim that QF2 lies on the same side of the plane gFT1 x = αF1 as the bottom
face. Let x be any point of the polygon QF2 not on the edge ui uj . We want to show that
gFT1 x < αF1 . Indeed,
gFT1 x − αF1 = gFT1 x − gFT1 ui = gFT1 (x − ui ) = (gF1 − gF2 )T (x − ui )
(since both x and ui lie on the plane gFT2 x = αF2 ),
=

µ
¶T
h F1 − h F2
(x − ui ) = (hF1 − hF2 )T (x0 − vi )
0

(where x0 is the projection of x onto the first two coordinates)
= (RFij )T (x0 − vi ) < 0
(since x0 lies on the right hand side of the edge vi vj ). This completes the proof.

¤

Theorem 4.2.3 proves Steinitz’s theorem in the case when the graph has a triangular face.
We are also home if the dual graph has a triangular face; then we can represent the dual
graph as the skeleton of a 3-polytope, choose the origin in the interior of this polytope, and
consider its polar; this will represent the original graph.
So the proof of Steinitz’s theorem is complete, if we prove the following simple fact:
Lemma 4.2.4 Let G be a 3-connected simple planar graph. Then either G or its dual has a
triangular face.
Proof. If G∗ has no triangular face, then every node in G has degree at least 4, and so
|E(G)| ≥ 2|V (G)|.
If G has no triangular face, then similarly
|E(G∗ )| ≥ 2|V (G∗ )|.
Adding up these two inequalities and using that |E(G)| = |E(G∗ )| and |V (G)| + |V (G∗ )| =
|E(G)| + 2 by Euler’s theorem, we get
2|E(G)| ≥ 2|V (G)| + 2|V (G∗ )| = 2|E(G)| + 4,
a contradiction.

¤

4.2. RUBBER BANDS, PLANARITY AND POLYTOPES

51

Figure 4.4: Rubber band representation of a cube with one node deleted, and of
an octahedron with the edges of a triangle deleted. Corresponding edges are parallel
and have the same length.

Exercise 4.4 Prove that every Schlegel diagram with respect to a face F can be obtained as
a rubber band representation of the skeleton with the nodes of the face nailed (the strengths
of the rubber bands must be chosen appropriately).
Exercise 4.5 Let G be a 3-connected planar graph with a triangular face abc. Let p denote
the infinite face, and let q, r, s be the faces neighboring p. Let G∗ be the dual graph. Consider
a rubber band representation x : V (G) → R2 of G with a, b, c nailed down, and also a rubber
band representation y : V (G∗ ) \ {p} → R2 of G \ p with q, r, s nailed down (both with unit
rubber band strengths). Prove that the positions of q, r, s can be chosen so that for every edge
ij ∈ E(G)\{ab, bc, ca}, and the corresponding edge uv ∈ E(G∗ ) (where the labeling is chosen
so that moving from i to j, the node u is on the left hand side), we have xi − xj = yu − yv
(Figure 4.4).

4.2.3

How to find the starting face?

Can we use Tutte’s method to test a 3-connected graph for planarity? The main difficulty
is that it assumes that we already know a face. So it may be a good method for designing
actual drawings of these graphs, it seems to assume that we already know the combinatorial
structure of the embedding.
But we only need to know one face to be able to apply Tutte’s method, and it turns out
that to find one face is easier than to find the whole embedding.
Recall that a depth-first search spanning tree in a connected graph G with root r ∈ V is
a spanning tree T with the property that for every edge ij of G, both endnodes of G lie on
a path starting at r. It is very easy to find a depth-first search spanning tree in a graph by
the depth-first search algorithm.
Lemma 4.2.5 Let G be any 3-connected graph with any node r specified as its root, and let
T be a depth-first search spanning tree in G. Let ij ∈ E \ E(T ), and let P be a path in T

52

CHAPTER 4. RUBBER BANDS

starting at R containing both endnodes of ij. Let i be he first endnode of ij P encounters.
Let P 0 be the subpath of P between r and i and P 00 , the subpath between i and j. Choose ij
so that P 0 is maximal (with respect to inclusion). Among these edges, choose one so that P 00
is minimal. Then the cycle C formed by P 00 and ij is non-separating.
Proof. Observe that i 6= r; indeed i = r could only happen if all edges in E \ E(T ) would
be incident with r, but any endnode of T different from r is incident with at least two edges
in E \ E(T ), one of which does not go to r.
Suppose that C has a chord. This is an edge in E \ T , and either it connects two points
of P 00 different from i (which contradicts the maximality of P 0 ), or it connects i to a node of
P 00 different from j (which contradicts the minimality of P 00 ).
Second, suppose that G \ V (C) is disconnected. Then V (C) separates some node x from
r. Walk from x along the tree to r, and let k be the first node of C we encounter. let T 0 be
the subtree of T separated (in T ) from r by x.
There is a path Q from x to r avoiding the pair {i, k} (by 3-connectivity). Traversing this
path from x, let v be its first node not in T 0 , and uv its edge before v. By the depth-first
search property of T , this node must be on P between k and r, and we know it is different
from i and k. This also implies that uv ∈ E \ E(T ).
Now if v is between i and r, then walking from v to r along P we get a path from x to
r avoiding C, which is impossible. If v is between i and k, then the edge uv contradicts the
maximality of P 0 .
¤
Now if G is planar, then the cycle C in Lemma 4.2.5 is a face by Lemma 1.1.5, and so we
can use it as the starting face in Tutte’s method.

4.3

Rubber bands and connectivity

The idea of rubber bands can be related to graph connectivity, and can be used to give a
test for k-connectivity of a graph (Linial, Lovász and Wigderson [126]).

4.3.1

Convex embeddings

Let G = (V, E) be a graph and S ⊂ V , a fixed set of its nodes. A convex representation of G
(in dimension d, with boundary S) is an mapping of V to Rd such that every node in V \ S
is in the convex hull of its neighbors. The representation is in general position if any d + 1
representing points are affine independent.
The following trivial fact is nevertheless useful to state:
Proposition 4.3.1 If x is a convex representation of a graph G = (V, E) with boundary S,
then x(V ) ⊆ conv(x(S)).

4.3. RUBBER BANDS AND CONNECTIVITY

53

Our discussion when introducing rubber bands implies the following:
Proposition 4.3.2 The rubber band representation extending any map from S ⊆ V to Rd
is convex with boundary S.
(Not every convex representation is constructible this way.)
Convex representations in R1 with a two-element boundary set {s, t} can be defined
combinatorially, and they constitute an important tool for graph algorithms.
Let G = (V, E) be a graph on n nodes and s, t ∈ V , s 6= t. A labeling ` : V → {1, . . . , n}
is called an s − t-numbering if `(s) = 1, `(t) = n, and every node i ∈ V \ {s, t} has a neighbor
j with `(j) < `(i) and another neighbor j with `(j 0 ) > `(i).
We quote the following well known graph-theoretic fact without proof.
Proposition 4.3.3 If G = (V, E) is a 2-connected graph, then it has an s − t-numbering for
every s, t ∈ V , s 6= t.
Exercise 4.6 (a) Prove proposition 4.3.3.
(b) Show that instead of the 2-connectivity of G, it suffices to assume that deleting any node,
either the rest is connected or it has two components, one containing s and one containing t.
(c) Consider the harmonic extension f of any function f0 : {s, t} → R. Prove that if the values
of f are distinct, then the ordering of the values gives an s − t-numbering.

4.3.2

Degeneracy: essential and non-essential

We start with a discussion of what causes degeneracy in rubber band embeddings. Consider
the two graphs in Figure 4.5. It is clear that if we nail the nodes on the convex hull, and
then let the rest find its equilibrium, then there will be a degeneracy: the grey nodes will
all movetothe same position. However, the reasons for this degeneracy are different: In the
first case, it is due to symmetry; in the second, it is due to the node that separates the grey
nodes from the rest, and thereby pulls them onto itself.
One can distinguish the two kinds of degeneracy as follows: In the first graph, the
strengths of the rubber bands must be strictly equal; varying these strengths it is easy
to break the symmetry and thereby get rid of the degeneracy. However, in the second graph,
no matter how we change the strengths of the rubber bands (as long as they remain positive),
the grey nodes will always be pulled together into one point.
Figure 4.6 illustrates a bit more delicate degeneracy. In all three pictures, the grey points
end up collinear in the rubber band embedding. In the first graph, the reason is symmetry
again. In the second, there is a lot of symmetry, but it does not explain why the three grey
nodes are collinear in the equilibrium. (It is not hard to argue though that they are collinear:
a good exercise!) In the third graph (which is not drawn in its equilibrium position, but
before it) there are two nodes separating the grey nodes from the nailed nodes, and the grey

54

CHAPTER 4. RUBBER BANDS

nodes will end up on the segment connecting these two nodes. In the first two cases, changing
the strength of the rubber bands will pull the grey nodes off the line; in the third, this does
not happen.

Figure 4.5: Two reasons why two nodes and up on top of each other: symmetry, or
a separating node

Figure 4.6: Three reasons why three nodes can end up collinear: symmetry, a
separating pair of nodes, or just accident

4.3.3

Connectivity and degeneracy

For X, Y ⊆ V , we denote by κ(X, Y ) the maximum number of vertex disjoint paths from X
to Y (if X and Y are not disjoint, some of these paths may be single nodes). By Menger’s

4.3. RUBBER BANDS AND CONNECTIVITY

55

Theorem, κ(X, Y ) is the minimum number of nodes that cover all X − Y paths in G. We
say that X and Y are fully connected if |X| = |Y | = κ(X, Y ). The graph G is k-connected if
and only if |V | > k and any two k-subsets are fully connected. The largest k for which this
holds is the vertex-connectivity of G, denoted κ(G). By convention, the complete graph Kn
is (n − 1)-connected but not n-connected.
Lemma 4.3.4 Let S, T ⊆ V . Then for every convex representation x of G with boundary
S, rk(x(T )) ≤ κ(S, T ).
Proof. There is a subset U ⊆ V with |U | = κ(S, T ) such that V \ U contains no (S, T )paths. Let W be the union of connected components of G \ U containing a vertex from T .
Then x, restricted to W , gives a convex representation of G[W ] with boundary U . Hence by
Proposition 4.3.1, x(W ) ⊆ conv(x(U )), and so
rk(x(T )) ≤ rk(x(W )) = rk(x(U )) ≤ |U | = κ(S, T ).
¤
The Lemma gives a lower bound on the connectivity between two sets S and T . The
following theorem asserts that if we take the best convex representation, this lower bound is
tight:
Theorem 4.3.5 Let G = (V, E) be a graph and S, T ⊆ V with κ(S, T ) = d + 1. Then G has
a convex representation in Rd with boundary S such that rk(xT ) = d + 1.
Corollary 4.3.6 Let G = (V, E) be a graph, d ≥ 1 and S ⊆ V . Then G has a general
position convex representation in Rd with boundary S if and only if no node of G can be
separated from S by fewer than d + 1 nodes.
Corollary 4.3.7 A graph G is k-connected if and only if for every S ⊆ V with |S| = k, G
has a general position convex representation with boundary S.
To prove Theorem 4.3.5, we have to construct a convex representation, and we use 4.3.2.
However, a rubber band representation will not automatically satisfy this: a subset of nodes
can be affine dependent (say, three nodes can fall on one line) because of symmetry, or just
by coincidence. We’ll have to choose “generic” edge weights. We are going to prove:
Theorem 4.3.8 Let G = (V, E) be a graph and S, T ⊆ V with κ(S, T ) ≥ d + 1. Choose
a random edge weight cij ∈ (0, 1] for every edge ij, independently and uniformly. Map the
nodes of S into Rd so that the images of any d + 1 nodes are affine independent. Then with
probability 1, the rubber band extension of this map is a convex representation of G in Rd
with boundary S such that rk(xT ) = d + 1.

56

CHAPTER 4. RUBBER BANDS

Figure 4.7: Three nodes accidentally on a line. Making the edges along the three
paths shown very strong pulls them apart.

Proof. The proof will consist of two steps: first, we show that there is some choice of the
edgeweights for which the conclusion holds; then we use this to prove that the conclusion
holds for almost all choices of edge-weights.
For the first step, we use that by Menger’s Theorem, there are d + 1 disjoint paths
P0 , . . . , Pd such that each Pi connects a node si ∈ S with a node ti ∈ T . The idea is to make
the rubber bands on these paths very strong (while keeping the strength of the other edges
fixed). Then these paths will pull each node ti very close si . Since the positions of s0 , . . . sd
are affine independent, so are the positions of the nodes s0 , . . . sd (Figure 4.7).
To make this precise, let D be the diameter of the set {xi : i ∈ S}, let E 0 = ∪i E(Pi ), fix
any R > 0, and define a weighting w = wR of the edges by
(
we =

R,
1,

if e ∈ E 0 ,
otherwise.

Let x be the rubber band extension of the given mapping of the nodes of S with these
strengths.
Recall that f minimizes the potential Ew over all representations of G with the given
nodes nailed. Let y be the representation with yj = xj if j ∈ S, yj = si if j ∈ Pi and (say)
f 0 (v) = s0 for any node not in S or any Pi . In y the edges with strength are have 0 length,
and so
Ew (x) ≤ Ew (y) ≤ D2 |E|.
On the other hand,
Ew (x) ≥

X
uv∈E 0

Rkxu − xv k2 .

4.3. RUBBER BANDS AND CONNECTIVITY
By the Cauchy-Schwartz inequality we get that

X
kxu − xv k ≤ |E(Pi )|
kxsi − xti k ≤
uv∈E(Pi )

≤

³n
R

Ew (x)

´1/2

p
≤

57

X

1/2
kxu − xv k2 

uv∈E(Pi )

n|E|D
√
.
R

Thus xti → xsi if R → ∞.
. . , xsd are affine independent, the columns of the matrix M =
µ Since xs0 , . ¶
1 ...
1
d
are linearly independent, and so the matrix M T M = (1 + xT
si xsj )i,j=0 is
xs0 . . . xsd
T
d
T
nonsingular. So det(M T M ) 6= 0. But 1 + xT
ti xtj → 1 + xsi xsj , so the matrix (1 + xti xtj )i,j=0
is nonsingular if R is large enough. This proves that xt0 , . . . , xtd are affine independent.
This completes the first step. Now we argue that this holds for almost all edge-weightings.
To prove this, we only need some general considerations. The embedding minimizing the
energy is unique, and so it can be computed from the equations (4.5) (say, by Cramer’s
Rule). What is important from this is that the xi can be expressed as rational functions
d
of the edgeweights. Furthermore, the value det((1 + xT
ti xtj )i,j=0 ) is a polynomial in the
coordinates of the xi , and so it is also a rational function of the edgeweights. We know
that this rational function is not identically 0; hence it follows that it is 0 only on a set of
weight-vectors of measure 0.
¤

4.3.4

Turning the method to an algorithm

Computing the rubber band representation
Let G = (V, E) be a connected graph on n nodes and S ⊆ V , |S| = k. Given a map
x : S → Rd , we can compute its rubber band extension by solving the system of linear
equations
X
cij (xi − xj ) = 0
(i ∈ V \ S).
(4.11)
j∈N (i)

This system has (n − k)d unknowns and the same number of equations, and we know that
it has a unique solution, since this is where the gradient of a strictly convex function (which
tends to ∞ at ∞) vanishes.
At the first sight, solving (4.11) takes inverting an (n − k)d × (n − k)d matrix. However,
we can immediately see that the coordinates can be computed independently, and since they
satisfy the same equations except for the right hand side, it suffices to invert the matrix of
the system once.
Below, we shall have to compute the rubber band representations of the same graph,
changing only the nailed set S. Can we make use of some of the computation done for one
of these representations when computing the others?

58

CHAPTER 4. RUBBER BANDS
The answer is yes. First, we do something which seems to make things worse: We create

new “equilibrium” equations for the nailed nodes, introducing new variables for the force
that acts on the nail. So we can write (4.11) as
µ ¶
y
,
(4.12)
Lc x =
0
where y ∈ RS , and Lc is the matrix


if ij ∈ E,
ij
−c
P
(Lc )ij =
k∈N (i) cik , if i = j,


0,
otherwise.
Now we use the same trick as we did for harmonic functrions: Lc is “almost” invertible in the
sense that the only vector in its nullspace in 1. Hence Lc +J is invertible; let M = (Lc +J)−1 .
Since (Lc + J)J = J 2 = nJ, it follows that M J = n1 J. Furthermore, from M (Lc + J) = I it
follows that M Lc = I − M J = I − n1 J. Hence multiplying (4.12) by M from the left, we get
µ ¶
³
1 ´
y
I− J x=M
,
n
0
or
x = z1 + M

µ ¶
y
,
0

(4.13)

P
where z = n1 i∈v xi .
First, let us look at the equations in (4.13) that correspond to nodes in S. Here the left
hand side is given, so we get k equations in the variables z and y.***
Nondegeneracy and connectivity testing
Rubber band representations yield a (randomized) graph connectivity algorithm with good
running time.
We start with describing a test checking whether or not two given k-tuples X and Y
of nodes are fully linked. For this, it suffices to map the nodes in X into Rk−1 in general
position, say one node is mapped onto the origin and the others to the basis vectors ei .
Then we compute the rubber band representation extending this map, and check whether
the points of Y are in general position.
If we want to apply a linkedness test for connectivity testing, it seems that we have to
apply it for all pairs of k-sets, which is too much. The following lemma shows how to get
around this.
Lemma 4.3.9 For every vertex v ∈ V we select an arbitrary k-subset S(v) of N (v). Then
G is k-connected iff S(u) and S(v) are linked for every u and v.

4.3. RUBBER BANDS AND CONNECTIVITY

59

Proof. The ”only if” part follows from the well-known property of k-connected graphs that
any two k-subsets are linked. The ”if” part follows from the observation that if S(u) and
S(v) are linked then u and v are connected by k openly disjoint paths.
¤
Thus the linkedness subroutine needs be called at most O(n2 ) times. But we do not even
have to check this for every pair (u, v), if we use the following simple lemma.
Lemma 4.3.10 Let G be any graph and H, a k-connected graph with V (H) = V (G). Then
G is k-connected iff u and v are connected by k openly disjoint paths in G for every edge
uv ∈ E(H).
Proof. The “only if” part is trivial. To prove the “if” part, suppose that G has a cutset
S with |S| < k. Let G1 be a connected component of G. Since S cannot be a cutset in H,
there is an edge uv ∈ E(H) connecting V (G1 ) to V (G) \ S \ V (G1 ). But the S separates u
¤
from v in G, a contradiction.
This lemma implies that it suffices to check that S(u) and S(v) are linked for every edge
uv ∈ E(H). This means O(nk) calls on a linkedness subroutine rather than O(n2 ). (If we
allow randomization then we can do even better; we refer to [126] for details.)
Numerical issues
(See Figure 4.8.)

Figure 4.8: The rubber band embedding gives nodes that are exponentially close.

The computation of the rubber band representation requires solving a system of linear
equations. We have seen that for a graph with n nodes, the positions of the nodes can
get exponentially close in a rubber band representation, which means that we might have to
compute with exponentially small numbers (in n), which means that we have to compute with
linearly many digits, which gives an extra factor of n in the running time. For computational
purposes it makes sense to solve the system in a finite field rather than in R. Of course, this
”modular” embedding has no physical or geometrical meaning any more, but the algebraic
structure remains!

60

CHAPTER 4. RUBBER BANDS
Let G(V, E), |V | = n, X, Y ⊆ V , |X| = |Y | = d + 1, p a prime and c ∈ FE
p . Let

X = {0, 1, . . . , d}. A modular rubber band representation of G (with respect to X, p and
c) is defined as any assignment V → Fdp satisfying (4.6) such that x0 = 0 and xi = ei for
i ∈ X \ {0}.
Lemma 4.3.11 Let G, X and Y be as above, and let N > 0 be an integer. Choose uniformly
a random a prime p < N and a random vector c ∈ FE
p . Then
(a) With probability at least 1−n2 /N , there is a unique modular rubber band representation
of G with respect to X, p and c.
(a) With probability at least 1 − n2 /N , a modular rubber band representation xc satisfies
rk(f (Y )) = κ(X, Y ).
Proof. The determinant of the system 4.6 is a polynomial of degree ≤ n2 in the strengths
cij . A standard application of Schwartz’s Lemma gives a bound on the probability that this
determinant vanishes, which gives (a). The proof of (b) is similar.
¤

4.4

Repulsive springs and approximating maximum cut

A cut in a graph G = (V, E) is the set of edges connecting a set S ⊆ V to V \ S, where
∅ ⊂ S ⊂ V . The Max Cut Problem is to find a cut with maximum cardinality. We denote by
Cmax this maximum.
(More generally, we can be given a weighting w : V → R+ , and we could be looking for a
cut with maximum total weight. To keep things simple, however, we restrict our introductory
discussions to the unweighted case.)
The Max Cut Problem is NP-hard; one natural approach is to find an “approximately”
maximum cut. Formulated in different terms, Erdős in 1967 described the following simple
heuristic algorithm for the Max Cut Problem: for an arbitrary ordering (v1 , . . . , vn ) of the
nodes, we color v1 , v2 , . . . , vn successively red or blue. For each i, vi is colored blue iff the
number of edges connecting vi to blue nodes among v1 , . . . , vi−1 is less than the number of
edges connecting vi to red nodes in this set. Then the cut formed by the edges between red
and blue nodes contains at least half of all edges. In particular, we get a cut that is at least
half as large as the maximum cut.
There is an even easier randomized algorithm to achieve this approximation, at least in
expected value. Let us 2-color the nodes of G randomly, so that each node is colored red or
blue independently, with probability 1/2. Then the probability that an edge belongs to the
cut between red and blue is 1/2, and expected number of edges in this cut is |E|/2.
Both of these algorithms show that the maximum cut can be approximated from below in
polynomial time with a multiplicative error of at most 1/2. Can we do better? The following

4.4. REPULSIVE SPRINGS AND APPROXIMATING MAXIMUM CUT

61

strong negative result of Hastad [88] (improving results of Arora et al. [17] and Bellare et
al. [23]) shows that we cannot get arbitrarily close to the optimum:
Proposition 4.4.1 It is NP-hard to find a cut with more than (16/17)Cmax ≈ .94Cmax edges.
Building on results of Delorme, Poljak and Rendl [49, 161], Goemans and Williamson
[79] give a polynomial time algorithm that approximates the maximum cut in a graph with
a relative error of about 13%:
Theorem 4.4.2 One can find in polynomial time a cut with at least .878Cmax edges.
The algorithm of Goemans and Williamson makes use of the following geometric construction. We want to find an embedding i 7→ ui (i ∈ V ) of the nodes of the graph in the
unit sphere in Rd so that the following “energy” is maximized:
E(u) =

X 1
X 1 − uT uj
i
(ui − uj )2 =
.
4
2

ij∈E

ij∈E

We can think of replacing the rubber bands by (strange) repulsive strings, which push their
endpoints apart with a force that increases proportionally with the length.
If we work in R1 , then the problem is equivalent to the Maximum Cut problem: each node
is represented by either 1 or −1, and the edges between differently labeled nodes contribute
1 to the energy, the other edges contribute 0. Hence the maximum energy Emax is an upper
bound on the maximum size Cmax of a cut.
Unfortunately, the argument above also implies that for d = 1, the optimal embedding is
NP-hard to find. While I am not aware of a proof of this, it is probably NP-hard for d = 2
and more generally, for any fixed d. The surprising fact is that for d = n, such an embedding
can be found in polynomial time using semidefinite optimization (cf. Chapter 13.3).
Let X denote the V × V matrix defined by Xij = uT
i uj . The X satisfies the constraints:
X º 0,

(4.14)

Xii = 1,

(4.15)

and the energy E(u) can be expressed as
¶
Xµ
1
1 − Xij .
2

(4.16)

ij∈E

Conversely, if X is a V × V matrix satisfying (4.14) and (4.15), then we can write it as a
Gram matrix of vectors in Rn , these vectors will have unit length, and (4.16) gives the energy.
The semidefinite optimization problem of maximizing (4.16), subject to (4.14) and (4.15)
can be solved in polynomial time (with an arbitrarily small relative error). So Emax is a
polynomial time computable upper bound on the size of the maximum cut.

62

CHAPTER 4. RUBBER BANDS
How good is this bound? And how to construct an approximately optimum cut from this

representation? Here is the simple but powerful trick: take a random hyperplane H through
the origin in Rn (Figure 4.9). The partition of Rd given by H yields a cut in our graph.
Since the construction pushes adjacent points apart, one expects that the random cut will
intersect many edges.

Figure 4.9: A cut in the graph given by a random hyperplane

To be more precise, let ij ∈ E and let ui , uj ∈ S n−1 be the corresponding vectors in the
embedding constructed above. It is easy to see that the probability that a random hyperplane
H through 0 separates ui and uj is αij /π, where αij = arccos uT
i uj is the angle between ui
and uj . It is not difficult to verify that if −1 ≤ t ≤ 1, then arccos t ≥ 1.38005(1 − t). Thus
the expected number of edges intersected by H is
X arccos uT uj
X
1.38005
1 − uT
i
i uj
≥
=
2Emax ≥ .878Cmax .
1.38005
π
π
π

ij∈E

ij∈E

(One objection to the above algorithm could be that it uses random numbers. In fact,
the algorithm can be derandomized by well established but non-trivial techniques. We do not
consider this issue in these notes; see e.g. [15], Chapter 15 for a survey of derandomization
methods.)

Chapter 5

Rigidity
We can replace the edges of a graph by rigid bars, instead of rubber bands. This way we
obtain a physical model, which is related to rubber bands, but is richer, and more important
from the point of view of applications. We have to restrict ourselves to a few basic results of
this rich theory; see e.g. Recski [164] or Whiteley [206] for more.

5.1

Stresses

Let G = (V, E) be a graph and x : V → Rd , a geometric representation of G. A function
σ : E → R is called a stress if for every node i,
X

σij (xj − xi ) = 0.

(5.1)

j∈N (i)

We can rewrite this as follows. Let Mx denote the m × dn matrix, in which the rows are
indexed by edges e ∈ E, the columns are indexed by pairs it (i ∈ V , 1 ≤ t ≤ d), and
(
xit − xjt if i is an endpoint of e = ij,
(Mx )e,it =
(5.2)
0
otherwise.
Then σ is a stress means that MxT σ = 0, i.e., σ is in the left nullspace of Mx . We denote this
space of stresses by Sx .
In a rubber band representation, the strengths of the edges “almost” form a stress: (5.1)
holds for all nodes that are not nailed. Lemma 4.3 said in this language the following:
Corollary 5.1.1 A rubber band representation of a simple connected planar graph in the
plane with a triangular face nailed has a stress that is 1 on the internal edges and negative
on the boundary edges.
An important result about stresses is a classic indeed: it was proved by Cauchy in 1822.
63

64

CHAPTER 5. RIGIDITY

Theorem 5.1.2 (Cauchy’s theorem) The skeleton of a 3-polytope has no nonzero stress.
Proof. Suppose that a convex polytope P carries a nonzero stress σ. Let G0 be the subgraph
of its skeleton formed by those edges which have a nonzero stress, together with the vertices
they are incident with. Color an edge ij of G0 red if σij > 0, and blue otherwise. The graph
G0 is planar, and in fact it comes embedded in the surface of P , which is homeomorphic to
the sphere. By lemma 1.1.8, G0 has a node i that such that the red edges (and then also the
blue edges) incident with v are consecutive in the given embedding of G0 . This implies that
we can find a plane through xi which separates (in their embedding on the surface of P ) the
red and blue edges incident with i. Let e be the normal vector of this plane pointing in the
halfspace which contains the red edges. Then for every red edge ij we have eT (xj − xi ) > 0,
and for every blue edge ij we have eT (xj − xi ) < 0. By the definition of the coloring, this
means that we have σij eT (xj − xi ) > 0 for every edge ij of G0 . Also by definition, we have
σij eT (xj − xi ) = 0 for those edges ij of GP that are not edges of G. Thus
X
σij eT (xj − xi ) > 0.
j∈N (i)

But
X

σij eT (xj − xi ) = eT

j∈N (i)

³ X

´
σij (xj − xi ) = 0

j∈N (i)

by the definition of a stress, a contradiction.

¤

The most interesting consequence of Cauchy’s Theorem is that is we make a convex
polyhedron out of cardboard, then it will be rigid (see Section 5.2). This is not true if
instead of convexity we only assume that the surface of the polytope is homeomorphic to the
sphere, as shown by a tricky counterexample of Connelly [47].
Example 5.1.3 (Connelly’s flexible polyhedron)

5.2

Rigidity

Let G = (V, E) be a graph and x : V → Rd , a geometric representation of G. We think of
the edges of G as rigid bars, and of the nodes, as flexible joints. We are interested in the
rigidity of such a structure, or more generally, in its possible motions.

5.2.1

Infinitesimal motions

Suppose that the nodes of the graph move smoothly in d-space; let xi (t) denote the position
of node i at time t. The fact that the bars are rigid says that for every edge ij ∈ E(G),
¡
¢ ¡
¢
xi (t) − xj (t) · xi (t) − xj (t) = const.

5.2. RIGIDITY

65

Differentiating, we get
¡

¢ ¡
¢
xi (t) − xj (t) · ẋi (t) − ẋj (t) = 0.

This motivates the following definition. Given a geometric representation x : V → Rd of G,
a map v : V → Rd is called an infinitesimal motion if the equation
(xi − xj ) · (vi − vj ) = 0

(5.3)

holds for every edge ij ∈ E). The vector vi can be thought of as the velocity of node i.
There is another way of describing this. Suppose that we move each node i from xi to
xi + εvi for some vectors vi Rd . Then the squared length of an edge ij changes by
∆ij (ε) = |(xi + εvi ) − (xj + εvj )|2 − |xi − xj |2 = 2ε(xi − xj ) · (vi − vj ) + ε2 |vi − vj |2 .
Hence we can read off that if (vi : i ∈ V ) is an infinitesimal motion, then ∆ij (ε) = O(ε2 )
(ε → 0); else, ∆ij (ε) = Θ(ε2 ).
There are some trivial infinitesimal motions.
(a) If vi is the same vector v for every i ∈ V , then (5.3) is trivially satisfied. This solution
corresponds to the parallel translation of the structure with velocity v.
(b) In the plane, let R denote the rotation of the plane by 90◦ in the positive direction.
Then vi = Rxi (i ∈ V ) defines an infinitesimal motion; this motion corresponds to the
rotation of the structure about the origin.
(c) In 3-space, vi = w × xi gives a solution for every fixed vector w, corresponding to
rotation with axis w.
In general, every rigid motion of the whole space gives an infinitesimal motion. These
trivial infinitesimal motions are characterized by the property that (5.3) holds for every pair
of nodes i 6= j (not just adjacent pairs).
The system (5.3) can be viewed as a system of linear equations for the coordinates of the
velocity vectors vi . In fact, it involves the same matrix M as defined by (5.2). If v ∈ Rdn
denotes the vector obtained by concatenating the vectors vi (i ∈ V ) (so ots coordinates are
indexed by pairs i, j where i ∈ V and 1 ≤ j ≤ d), then we can write (5.3) as
M v = 0.

(5.4)

The solutions of this equation form a linear space Vx , and the trivial solutions mentioned
¡ ¢
above form a subspace Ux of this of dimansion d+1
(if the vectors xi are not contained in
2
an affine hyperplane).
We say that the structure is infinitesimally rigid, if every infinitesimal motion is trivial.
Whether or not a structure is infinitesimally rigid can be decided by simple linear algebra.
There are (at least) two other versions of the notion of rigidity. We can ask whether there
is a finite motion in the sense one can specify a continuous orbit xi (t) for each node i so that

66

CHAPTER 5. RIGIDITY

(a)

(b)

(c)

Figure 5.1: Structure (a) is infinitesimally rigid and has no finite motion in the
plane, but has a different realization (b). Structure (c) has no finite motion and no
other realization, but it is not infinitesimally rigid.

xi (0) = xi , |xi −xj | is constant for every pair of adjacent nodes, but not for all pairs of nodes.
Or we can ask whether there is a different realization of the same structure in the form of
another representation y : V → Rd such that |xi − xj | = |yi − yj | for every pair of adjacent
nodes, but not for all pairs of nodes. (In this case, it is possible that the representation y
cannot be obtained from the representation x by a continuous motion preserving the lengths
of the edges.) Figure 5.1 shows two examples of structures rigid in one sense but not in the
other. It is in general difficult to decide whether the structure is rigid in these senses, and
we’ll only be concerned with infinitesimal rigidity.
Let us conclude this section with a little counting. The system of equations 5.3 has dn
unknowns (the coordinates of the velocities) and m equations. We also know that it has a
¡ ¢
solution space of dimension at least d+1
2 .
How many of these equations are linearly independent? Let λij (ij ∈ E) be multipliers
such that combining the equations with them we get 0. This means that
X
λij (xj − xi ) = 0
j∈N (i)

for every node i; in other words, λ is a stress!
This discussion implies that
dim(Vx ) − dim(Sx ) = dn − m.

(5.5)

¡ ¢
An interesting special case is obtained when m = dn − d+1
2 . The structure is infinitesimally
¡d+1¢
rigid if and only dim(Vx ) = dim(Ux ) = 2 , which in this case is equivalent to dim(Sx ) = 0,
which means that there is no nonzero stress.
Example 5.2.1 A representation of the 3-prism in the plane has an infinitesimal motion
(equivalently a stress) if and only if it is drawn so that the lines of the three thick edges pass
through one point (Figure 5.2). This is also the case when it has a nonzero stress.

5.2. RIGIDITY

67

(a)

(b)

(c)

Figure 5.2: Rigid and nonrigid representations of the 3-prism in the plane. In the
first, fixing the black nodes, the white triangle can “circle” the black triangle. The
second is infinitesimally rigid and stress-free; the third has an infinitesimal motion
(equivalently, it has a stress) but no finite motion.

5.2.2

Rigidity of convex 3-polytopes

For example, let G be the skeleton of a convex 3-polytope P , with the representation given
by the positions of the vertices. Cauchy’s Theorem 5.1.2 tells us that the space of stresses
¡¢
has dimension 0, so we get that dimot = 3n − m. We know that dimot ≥ 42 = 6, and hence
m ≤ 3n − 6. Of course, this inequality we know already (Corollary 1.1.2). It also follows
that if equality holds, i.e., when all faces of P are triangles, then the space of infinitesimal
motions is 6-dimensional; in other words,
Corollary 5.2.2 The structure consisting of the vertices and edges of a convex polytope with
triangular faces is rigid.
If not all faces of the polytope P are triangles, then it follows by the same computation
that the skeleton is not infinitesimally rigid. However, if the faces are “made out of cardboard”, which means that they are forced to preserve their shape, then the polytope will be
rigid. To prove this, one can put a pyramid over each face (flat enough to preserve convexity),
and apply Corollary 5.2.2 to this new polytope P 0 with triangular faces. Every nontrivial
infinitesimal motion of P 0 would extend to a nontrivial infinitesimal motion of P , but we
know already that P has no such motion.

5.2.3

Generic rigidity

We say that a representation x : V → Rd of a graph is generic, if all the coordinates of
the vectors xi are algebraically independent transcendentals. This assumption is generally an
overkill, and we could always replace it by “the coordinates of the representing vectors do not
satisfy any algebraic relation that they don’t have to, and which is important to avoid in the
proof”. Another way of handling the genericity assumption is to choose independent random
values for the coordinates from any distribution that is absolutely continuous with respect

68

CHAPTER 5. RIGIDITY

to the Lebesgue measure (e.g. Gaussian, or uniform from a bounded domain). We call this
a random representation of G. We can then state the results as probabilistic statements
holding with probability 1.
We say that a graph is generically stress-free in Rd , if every generic representation of
it in Rd is stress-free; similarly, we say that G is generically rigid in Rd , if every generic
representation of it in Rd is infinitesimally rigid.
Proposition 5.2.3 Let G be a graph and d ≥ 1. If G has a representation in Rd that is infinitesimally rigid [stress-free], then every generic representation of G in Rd is infinitesimally
rigid [stress-free].
Proof.
Both infinitesimal rigidity and stress-freeness can be expressed in terms of the
non-vanishing of certain determinants composed of the differences of the coordinates of the
representation. If this holds for some choice of the coordinates, then it holds for the alge¤
braically independent choice.
Another way of stating this observation:
Proposition 5.2.4 If G is generically rigid [stress-free] in Rd , then a random representation
of G is infinitesimally rigid [stress-free] in Rd with probability 1.
Generic rigidity and finite motion
In a generic position, there is no difference between infinitesimal non-rigidity and finite motion, as proved by Asimov and Roth [18]:
Theorem 5.2.5 Suppose that a generic representation of a graph in Rd admits a nonzero
infinitesimal motion. Then it admits a finite motion.
Proof. Let x be a generic representation of G in Rd , and let B be a basis for the columns of
the matrix Mx . If c is any column not in B, then c can be expressed as a linear combination
of the columns in B. From Cramer’s Rule it follows that the coefficients in this linear
combination are rational functions of the entries of Mx with integral coefficients. Multiplying
by the common denominator, we get a vector v = v(c, x) such that Mx v = 0, the entries of v
are polynomials of the coordinates of the xi with integer coefficients, and the nonzero entries
are all contained in B ∪ {c}.
It follows by elementary linear algebra that the vectors v(c, x), where c ranges through
all columns of Mx outside B, generate the space Vx , and since Vx 6= Ux by hypothesis, there
is a column c such that v(c, x) ∈
/ Ux . Let us fix such a c.
Note that Mx v(c, x) = 0 is an algebraic equation in the entries xi,t , and so by the
assumption that these entries are algebraically independent, it follows that My v(c, y) = 0

5.2. RIGIDITY

69

holds identically for every representation y : V → Rd . In other words, v(c, y) ∈ Vy for every
y. If we choose y in a small neighborhood of x, then v(c, y) ∈
/ Uy must also hold.
Now consider the differential equations
ẏ(t) = v(c, y(t)),

y(0) = x.

Since the left hand side is a continuous function of y, this system has a solution in a small
interval 0 ≤ t ≤ T . This defines a non-rigid motion of the graph. Indeed, for every edge ij,
d
|yi (t)−yj (t)|2 = (yi (t)−yj (t))·(ẏi (t)− ẏj (t)) = (yi (t)−yj (t))·(v(c, yi (t))−v(c, yj (t))) = 0,
dt
since v(c, y(t)) ∈ Vy .

¤

Generic rigidity in the plane
Laman [119] gave a characterization of generically stress-free graphs in the plane. To formulate the theorem, we need a couple of definitions. A graph G = (V, E) is called a Laman
graph, if if every subset S ⊆ V with |S| ≥ 2 spans at most 2|S| − 3 edges. Note that the
Laman condition implies that the graph has no multiple edges.
To motivate this notion, we note that every planar representation of a graph with n nodes
and more than 2n − 3 edges will admit a nonzero stress, by (5.5). If a graph has exactly
2n − 3 edges, then a representation of it will be rigid if and only if it is stress-free.
The Henneberg construction increases the number of nodes of a graph G in one of two
ways: (H1) create a new node and connect it to at most two old nodes, (H2) subdivide an
edge and connect the new node to a third node. If a graph can be obtained by iterated
Henneberg construction, starting with a single edge, then we call it a Henneberg graph.
Theorem 5.2.6 (Laman’s Theorem) For a graph G = (V, E) the following are equivalent:
(a) G is generically stress-free in the plane;
(b) G is a Laman graph;
(c) G is a Henneberg graph.
Proof. (a)⇒(b): Suppose that V has a subset S with |S| ≥ 2 spanning more than 2|S| − 3
edges. Then the equations corresponding to these edges are dependent, so there is a stress
on the graph.
(b)⇒(c): We prove by induction on the number of nodes that a Laman graph G can be
built up by the Henneberg construction. Since the average degree G is 2|E|/|V | ≤ (4|V | −
6)/|V | < 4, there is a node i of degree at most 3. If i has degree 2, then we can delete i
and proceed by induction and (H1). So we may assume that i has degree 3. Let a, b, c be its
neighbors.

70

CHAPTER 5. RIGIDITY
Call a set S ⊆ V with |S| ≥ 2 spanning exactly 2|S| − 3 edges of G a tight set. The key

observation is:
Claim 5.2.7 If two tight sets have more than one node in common, then their union is also
tight.
Indeed, suppose that S1 and S2 are tight, and let E1 and E2 be the sets of edges they
span. Then E1 ∩ E2 is the set of edges spanned by S1 ∩ S2 , and so |E1 ∩ E2 | ≥ 2|S1 ∩ S2 | − 3
(using the assumption that |S1 ∩ S2 | ≥ 2). The set E1 ∪ E2 is a subset of the set of edges
spanned by S1 ∪ S2 , and so the number of edges spanned by S1 ∪ S2 is at least
|E1 ∪E2 | = |E1 |+|E2 |−|E1 ∩E2 | ≥ (2|S1 |−3)+(2|S2 |−3)−(2|S1 ∩S2 |−3) = 2|S1 ∪S2 |−3.
Since G is a Laman graph, we must have equality here, implying that S1 ∪ S2 is tight.
Claim 5.2.8 There is no tight set containing {a, b, c} but not i.
Indeed, adding i to such a set the Laman property would be violated.
We want to prove that we can delete i and create a new edge between two of its neighbors
to get a Laman graph G0 ; then G arises from G0 by Henneberg construction (H2). Let us try
to add the edge ab to G − i; if the resulting graph G0 is not a Laman graph, then there is a
set S ⊆ V \ {i} with |S| ≥ 2 spanning more than 2|S| − 3 edges of G0 . Since S spans at most
2|S| − 3 edges of G, this implies that S is tight, and a, b ∈ S. (If a and b are adjacent in
G, then {a, b} is such a tight set.) If there are several sets S with this property, then Claim
5.2.7 implies that their union is also tight. We denote by Sab this union. Similarly we get
the tight sets Sbc and Sca .
Claim 5.2.8 implies that the sets Sab , Sbc and Sca are distinct, and hence Claim 5.2.7
implies that any two of them have one node in common. Then the set S = Sab ∪Sbc ∪Sca ∪{i}
spans at least
(2|Sab | − 3) + (2|Sbc | − 3) + (2|Sca | − 3) + 3 = 2|S| − 2
edges, a contradiction.
(c)⇒(a): Let G arise from G0 by the Henneberg construction, and let σ be a nonzero
stress on G. We want to construct a nonzero stress on G0 (which would complete the proof
by induction).
First suppose that we get G by step (H1), and let i be the new node. It suffices to argue
that σ is 0 on the new edges, so that σ gives a stress on G0 . If i has degree 0 or 1, then
this is trivial. Suppose that i has two neighbors a and b. Then by the assumption that the
representation is generic, we know that the points xi , xa and xb are not collinear, so from the
stress condition
σia (xi − xa ) + σib (xi − xb ) = 0

5.2. RIGIDITY

71

it follows that σia = σib = 0.
Suppose that G arises by Henneberg construction (H2), by subdividing the edge ab ∈
E(G0 ) and connecting the new node i to c ∈ V (G0 ) \ {a, b}. Let us modify the representation
x as follows:
(
1
(xa + xb ) if j = i,
0
xj = 2
xj
otherwise.
This representation for G is not generic any more, but it is generic if we restrict it to G0 .
If the generic representation x of G admits a nonzero stress, then by Proposition 5.2.3
so does every other representation, in particular x0 admits a nonzero stress σ 0 . Consider the
stress condition for i:
0
0
0
σia
(x0i − x0a ) + σib
(x0i − x0b ) + σic
(x0i − x0c ) = 0.

Here x0i − x0a = 12 (xb − xa ) and x0i − x0b = 21 (xa − xb ) are parallel but x0i − x0c is not parallel to
0
0
0
0
0
them. So it follows that σic
= 0 and σia
= σib
. Defining σab
= σia
, we get a nonzero stress
¤
on G0 , a contradiction.
Corollary 5.2.9 Let G be a graph with n nodes and 2n − 3 edges. Then G is generically
rigid in the plane if and only if it is a Laman graph.
Laman’s Theorem can be used to prove the following characterization of generically rigid
graphs in the plane.
Theorem 5.2.10 A graph G = (V, E) is generically rigid in the plane if and only if for
P
every decomposition G = G1 ∪ · · · ∪ Gk we have i (2|V (Gi )| − 3) ≥ 2|V | − 3.
Proof.
¤
It is worth while to state the following consequence of Theorem 5.2.10:
Corollary 5.2.11 Every 6-connected graph is generically rigid in the plane.
Proof. Suppose that G = (V, E) is 6-connected but not generically rigid, and consider such
a graph with a minimum number of nodes. Then by Theorem 5.2.10 there exist subgraphs
Gi = (Vi , Ei ) (i = 1, . . . , k) such that G = G1 ∪ · · · ∪ Gk and
k
X

(2|Vi | − 3) < 2|V | − 3.

(5.6)

i=1

We may assume that every Gi is complete, since adding edges inside a set Vi does not change
(5.6). Furthermore, we may assume that every node belong to at least two of the Vi : indeed,

72

CHAPTER 5. RIGIDITY

if v ∈ Vi is not contained in any other Vj , then deleting it preserves (5.6) and also preserves
6-connectivity (deleting a node from a graph whose neighbors form a complete graph does
not decrease the connectivity of this graph).
We claim that
X³
3 ´
2−
≥2
(5.7)
|Vi |
Vi 3v

for each node v. Let (say) V1 , . . . , Vr be those Vi containing v, |V1 | ≤ |V2 | ≤ . . . Each term
on the left hand side is at least 1/2, so if r ≥ 4, we are done. Furthermore, the graph is
6-connected, hence v has degree at least 6, and so
r
X
(|Vi | − 1) ≥ 6.

(5.8)

i=1

If r = 3, then |V3 | ≥ 3, and so the left hand side of (5.7) is at least 1/2 + 1/2 + 1 = 2. If
r = 2 and |V1 | ≥ 3, then the left hand side of (5.7) is at least 1 + 1 = 2. Finally, if r = 2 and
|V1 | = 2, then |V2 | ≥ 6 by (5.8), and so the left hand side of (5.7) is at least 1/2 + 3/2 = 2.
Now summing (5.7) over all nodes v, we get
k
k
X X³
X
3 ´ X³
3 ´X
2−
=
2−
1=
(2|Vi | − 3)
|Vi |
|Vi |
i=1
i=1

v∈V Vi 3v

v∈Vi

on the left hand side and 2|V | on the right hand side, which contradicts (5.6).

¤

Exercise 5.1 Construct a 5-connected graph that is not generically rigid in the plane.
A 3-dimensional analogue of Laman’s Theorem, or Theorem 5.2.10 is not known. It is
not even known whether there is a positive integer k such that every k-connected graph is
generically rigid in 3-space.

Chapter 6

Representing graphs by
touching domains
6.1
6.1.1

Coin representation
Koebe’s theorem

We prove Koebe’s important theorem on representing a planar graph by touching circles
[115], and its extension to a Steinitz representation, the Cage Theorem.
Theorem 6.1.1 (Koebe’s Theorem) Let G be a 3-connected planar graph. Then one can
assign to each node i a circle Ci in the plane so that their interiors are disjoint, and two
nodes are adjacent if and only if the corresponding circles are tangent.

Figure 6.1: The coin representation of a planar graph

If we represent each of these circles by their center, and connect two of these centers by a
segment if the corresponding circles touch, we get a planar map, which we call the tangency
graph of the family of circles. Koebe’s Theorem says that every planar graph is the tangency
graph of a family of openly disjoint circular discs.
73

74

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS
The following theorem, due to Andre’ev [16], gives a strengthening of Koebe’s theorem

in terms of a simultaneous representation of a 3-connected planar graph and of its dual by
touching circles. To be precise, we define a double circle representation in the plane of a
planar map G as two families of circles, (Ci : i ∈ V ) and Dj : j ∈ V ∗ ) in the plane, so
that for every edge ij, bordering faces a and b, the following holds: the circles Ci and Cj
are tangent at a point x; the circles Da and Db are tangent at the same point x; and the
circles Da and Db intersect the circles Ci and Cb at this point. Furthermore, the circles Ci
are disjoint and so are the circles Dj , except that the circle Dj0 representing the outer face
contains all the other circles in its interior.
Theorem 6.1.2 Every 3-connected planar map G has a double circle representation in the
plane.

6.1.2

Formulation in the space

Theorem 6.1.3 (The Cage Theorem) Every 3-connected planar graph is isomorphic to
the 1-skeleton of a convex 3-polytope such that every edge of the polytope touches a given
sphere.
It is worth while to describe this in another way. Let G = (V, E) be a 3-connected planar
graph and G∗ = V ∗ , E ∗ ) its dual. There is a bijection η : E → E ∗ from duality. It will be
convenient to define η so that if pq is oriented, then ij = η(pq) is also oriented, and so that
pq crosses ij from left to right.
We define the diamond graph G♦ = (V ♦ , E ♦ ) of G follows. We let V ♦ = V ∪ V ∗ , and
we connect p ∈ V to i ∈ V ∗ if p is a node of face i. We do not connect two nodes in
V nor in V ∗ , so G♦ is a bipartite graph. It is also clear that G♦ is planar. The double
cycle representation of G gives a representation of G♦ by circles such that adjacent nodes
correspond to orthogonal circles.
Figure 6.3(a) shows this double circle representation of the simplest 3-connected planar
graph, namely K4 . For one of the circles, the exterior domain should be considered as the
disk it bounds. The other picture shows part of the double cycle representation of a larger
planar graph.

6.1.3

Preparation for the proof

We fix a triangular face i0 in G or G∗ (say, G) as the outer face; let a, b, c be the nodes of i0 .
For every node p ∈ V , let F (p) denote the set of bounded faces containing p, and for every
face i, let V (i) denote the set of nodes contained in i. Let U = V ∪ V ∗ \ {i0 }, and let J
denote the set of pairs ip with i ∈ V ∗ \ {i0 } and p ∈ V (i).
Let us start with assigning a positive real number ru to every node u ∈ U . Think of
this as a guess for the radii of the circles we seek (we don’t guess the radius for the circle

6.1. COIN REPRESENTATION

75

Figure 6.2: A convex polytope in which every edge is tangent to a sphere creates
two families of circles on the sphere.

Figure 6.3: Two sets of circles, representing (a) K4 and its dual (which is another
K4 ); (b) a planar graph and its dual.

representing i0 ; this will be easy to add at the end). For every ip ∈ J, we define
αip = arctan

rp
.
ri

αpi = arctan

ri
π
= − αip .
rp
2

and

Suppose that p is an internal node. If the radii correspond to a correct double circle
representation, then 2αpi is the angle between the two edges of the face i at p (Figure 6.4).
Thus in this case we have
X

2αpi = 2π.

i∈F (p)

Of course, we cannot expect this to hold for an arbitrary choice of the ru ; let as call the value
δp =

X
i∈F (p)

αpi − π

76

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

αip

i

q

p

αpi

j
Figure 6.4: Notation.

the defect of the node p (for the given choice of radii; we divided by 2 for convenience).
For a boundary node p ∈ V (i0 ), we have to modify this definition: In the ideal situation,
we would have
X

2αpi =

i∈F (p)

π
,
3

and so we define the defect by
X

δp =

αpi −

i∈F (p)

π
.
6

If i is a bounded face, then for the right choice of radii we should have
X

2αip = 2π.

p∈V (i)

Accordingly, we define the defect of bounded faces:
δi =

X

αip − π.

i∈V (p)

(Note that all these defects may be positive or negative.) This way we define a mapping
Φ : RU → RU by
Φ : (ru : u ∈ U ) 7→ (δu : u ∈ U ).

(6.1)

The crucial observation is the following.
Lemma 6.1.4 If radii ru > 0 (u ∈ U ) are chosen so that we have δu = 0 for all u ∈ U , then
there is a double circuit representation with these radii.

6.1. COIN REPRESENTATION
Proof.

77

Let us construct two right triangles with sides ri and rp for every ip ∈ J, one

with each orientation, and glue these two triangles together along their hypotenuse to get
a kite Kip . Now starting from a node p1 of i0 , we put down all kites Kp1 i in the order of
the corresponding faces in a planar embedding of G. By δp1 = 0, these will fill an angle of
π/3 at p1 . No proceed to a bounded face i1 containing p1 , and put down all the remaining
kites Ki1 p in the order in which the nodes of i1 follow each other on the boundary of i1 . By
δi1 = 0, these triangles will cover a neighborhood of p1 . We proceed similarly to the other
bounded faces containing p1 , then to the other nodes of i1 , etc. The condition that δ = 0
will guarantee that we tile a regular triangle.
Let Cp be the circle with radius rp about the position of node p constructed above, and
define Di similarly. We still need to define Di0 . It is clear that i0 is drawn as a regular
triangle, and hence we necessarily have ra = rb = rc . We define Dp0 as the inscribed circle
of the regular triangle abc.
¤

6.1.4

The range of defects

The defect vector δ(r) depends on the ratio of the ru only; hence we may restrict our attention
to positive radii satisfying
X
ru = 1.
u∈U

Then the domain of the map δ is the interior of the (n + f − 2)-dimensional simplex Σ defined
by
X
xu ≥ 0,
xu = 1.
u∈U

Our next goal is to determine the range. By Lemma 6.1.4, all we need to show is that this
contains the origin.
While of course an arbitrary choice of the radii ru will not guarantee that δu = 0 for all
u ∈ U , the following lemma shows that this is true at least “one the average”:
Lemma 6.1.5 For every assignment of radii, we have
X
δu = 0.
u∈U

Proof. From the definition,


X
X
X

δu =
αip − π  +
u∈U

i∈V \{a,b,c}



+

X
p∈V

∗ \{p }
0



p∈F (i)

X
i∈V (p)



αpi − π  .


X
i∈{a,b,c}



X

p∈F (i)


π
αip −
6

78

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Every pair ip ∈ J contributes αip + αpi = π/2. Since |J| = 2m − 3, we get
π
π
(2m − 3) − (n − 3)π − 3 − (f − 1)π = (m − n − f + 2)π
2
6
By Euler’s formula, this proves the lemma.

¤

Let us examine the range of the mapping δ in more detail. We already know that it lies
P
in the hyperplane defined by u δu = 0. We can derive some inequalities too.
For S ⊆ U , define J[S] = {ip ∈ J : i, p ∈ S} and
f (S) =

5
1
|J[S]| − |S| + |S ∩ {a, b, c}|.
2
6

Lemma 6.1.6 (a) f (∅) = f (U ) = 0;
(b) f (S) < 0 for every set ∅ ⊂ S ⊂ U .
Proof. The proof of (a) is left to the reader as Exercise 6.1.
We could prove (b) by combinatorial arguments (e.g., induction), but let us give a geometric proof whose elements we can use later again.
Consider any straight line embedding of the graph (say, the Tutte rubber band embedding), with i0 forming a regular triangle and bounding the unbounded face. For ip ∈ J let
βpi denote the angle of the polygon i at the vertex p, and let βip = π − βpi . These numbers
obviously have the following properties:
0 < βip < π,

0 < βpi < π.

(6.2)

Furthermore,
X
βpi = 2π

(6.3)

i∈F (p)

for every node p 6= a, b, c,
X
π
βpi =
3

(6.4)

i∈F (p)

for i ∈ {a, b, c}, and
X
βip = 2π

(6.5)

p∈V (i)

for every bounded face p. We can do the following computation using (6.3), (6.4) and (6.5):
X
π|J(S)| =
(βpi + βip )
ip∈J
i,p∈S

<

X

X

i∈S∩V ∗ p∈V (i)

βip +

X

X

p∈S∩V i∈F (p)

5
= 2π|S| − π|S ∩ {a, b, c}|.
3

βpi

6.1. COIN REPRESENTATION

79

(The strict inequality comes from the fact that there is at least one pair ip where exactly
one element belongs to S, and here we omitted a positive term βip or βpi .) This proves the
Lemma.
¤

Lemma 6.1.7 For every set ∅ ⊂ S ⊂ U ,
X

δu > f (S)π.

u∈S

Proof. We have
X
u∈S

δu =

X

X

p∈S∩V i∈F (i)

X

αpi +

X

αip

i∈S∩V ∗ p∈V (i)

π
− |I(S)|π − |S ∩ (V \ {a, b, c}|π − |S ∩ {a, b, c}|
6
X
X
X
=
αip +
αpi +
(αip + αpi )
ip∈J
i∈S,p∈S
/

ip∈J
i∈S,p∈S
/

(6.6)

ip∈J[S]

π
− |I(S)|π − |S ∩ (V \ {a, b, c}|π − |S ∩ {a, b, c}|
6
X
X
=
αip +
αpi + f (S)π.
ip∈J
i∈S,p∈S
/

ip∈J
i∈S,p∈S
/

Since the first two terms are nonnegative, and (as in the previous proof) at least one is
¤
positive, the lemma follows.
Now we are prepared to describe the range of defects. Let P denote the polyhedron in
R defined by
U

X

xu = 0,

(6.7)

u∈U

X

xu ≥ d(S)π

(∅ ⊂ S ⊂ U ).

(6.8)

u∈S

Clearly P is bounded. Our lemmas imply the following properties of P and the map Φ
defined in (6.1):
Corollary 6.1.8 The origin is in the relative interior of P . The range of Φ is contained in
P.
The first statement follows by Lemma 6.1.7. For a vector in the range of Φ, equation
(6.7) follows by Lemma 6.1.5, while inequality (6.8) follows by Lemma 6.1.6.
Lemma 6.1.9 The map Φ is injective.

80

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Proof.

Consider two different choices r and r0 of radii. Let S be the set of elements of

U for which ru0 /ru is maximum, then S is a nonempty proper subset of U . There is a pair
ip ∈ J (i ∈ V ∗ , p ∈ V ) such that exactly one of i and p is in S; say, for example, that i ∈ S
and p ∈
/ S. Then for every pair iq, q ∈ V (i), we have rq0 /ri0 ≤ rq /ri , and strict inequality
holds if q = p. Thus
X
X
rq0
rq
arctan 0 <
arctan ,
ri
ri
q∈V (i)

q∈V (i)

0

showing that δi (r ) < δi (r), which proves the lemma.

¤

The next (technical) lemma is needed whenever we construct some assignment of radii as
a limit of other such assignments.
Lemma 6.1.10 Let r(1) , r(2) , · · · ∈ RU be a sequence of assignments of radii, and let δ (k) =
(k)
Φ(r(k) ). Suppose that for each u ∈ U , ru → ρu as k → ∞. Let S = {u : ρu > 0}. Then
X
(k → ∞).
δu(k) → d(S)π
u∈S

(We note that the right hand side is negative by Lemma 6.1.6.)
Proof. Recall the computation (6.6). The terms that result in strict inequality are αpi
with p ∈ S, i ∈
/ S, and αip with i ∈ S, p ∈
/ S. These terms tend to 0, so the slack in (6.7)
tends to 0.
¤
Now we are able to prove the main result in this section.
Theorem 6.1.11 The range of Φ is exactly the interior of P .
Proof. Suppose not, and let y1 be an interior point in P not in the range of ∆. Let y2 be
any point in the range. The segment connecting y1 and y2 contains a point y which is an
interior point of P and on the boundary of the range of ∆. Consider a sequence (r1 , r2 , . . . )
with ∆(rk ) → y. We may assume (by compactness) that (rk ) is convergent. If (rk ) tends to
an interior point r of Σ, then the fact that ∆ is continuous, injective, and dim(P ) = dim(Σ)
imply that the image of a neighborhood of r covers a neighborhood of y, a contradiction. If
(rk ) tends to a boundary point of Σ, then by Lemma 6.1.10, δ(rk ) tends to the boundary of
Σ, a contradiction.
¤
Since the origin is in the interior of P by Corollary 6.1.8, this also completes the proof of
Theorem 6.1.2.
The proof we saw is just an existence proof, however. In the next section we describe
an algorithmic proof. We’ll need most of the lemmas proved above in the analysis of the
algorithm.
Exercise 6.1 Prove part (a) of Lemma 6.1.6

6.1. COIN REPRESENTATION

6.1.5

81

An algorithmic proof

We measure the “badness” of the assignment of the radii by error
E=

X

δu2 .

u∈U

We want to modify the radii so that we reduce the error. The key observation is the following.
Let i be a face with δi > 0. Suppose that we increase the radius ri , while keep the other
radii fixed. Then αip decreases for every p ∈ V (i), and correspondingly αpi increases, but
nothing else changes. Hence δi decreases, δp increases for every p ∈ V (i), and all the other
defects remain unchanged. Since the total defect remains the same by Lemma 6.1.5, we can
describe this as follows: if we increase a radius, some of the defect of that node is distributed
to its neighbors (in the G♦ graph). Note, however, that it is more difficult to describe in
what proportion this defect is distributed (and we’ll try to avoid to have to describe this).
How much of the defect of i can be distributed this way? If ri → ∞, then αip → 0 for
every p ∈ V (i), and so δi → −π. This means that we “overshoot”. So we can distribute at
least the positive part of the defect this way.
The same argument applies to the defects of nodes, and we can distribute negative defects
similarly by decreasing the appropriate radius. Let us immediately renormalize the radii, so
P
that we maintain that u ru = 1. Recall that this does not change the defects.
There are many schemes that can be based on this observation: we can try to distribute
the largest (positive) defect, or a positive defect adjacent to a node with negative defect etc.
Brightwell and Scheinerman [26] prove that if we repeatedly pick any element u ∈ U with
positive defect and distribute all its defect, then the process will converge to an assignment
of radii with no defect. There is a technical hurdle to overcome: since the process is infinite,
one must argue that no radius tends to 0 or ∞. We give a somewhat different argument.
Consider a subset ∅ ⊂ S ⊂ U , and multiply each radius ru , u ∈
/ S, by the same factor
0 < λ < 1. Then αip is unchanged if both i and p are in S or outside S; if i ∈ S and p ∈
/S
then αip decreases while αpi increases; and similarly the other way around. Hence δu does
not increase if u ∈ S and strictly decreases if u ∈ S has a neighbor outside S. Similarly, δu
does not decrease if u ∈ U \ S and strictly increases if u ∈ U \ S has a neighbor in S.
Let ∅ ⊂ S ⊂ U be a set such that minu∈S δu > maxU ∈U \S δu . We claim that we can
/ S. If not,
decrease the radii in U \ S until one of the δu , u ∈ S, becomes equal to a δv , v ∈
then (after renormalization) the radii in U \ S would tend to 0 while still any defect in S
would be larger than any defect in U \ S. But it follows by Lemmas 6.1.6 and 6.1.10) that
in this case the total defect in S tends to a negative value, and so the total defect in U \ S
tends to a positive value. So there is an element of S with negative defect and an element of
U \ S with positive defect, which is a contradiction.
Let t be this common value, and let δu0 be the new defects. Then the change in the error

82

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

is
E − E0 =

X

δu2 −

u∈U

X

δu0

2

u∈U

Using Lemma 6.1.5, we can write this in the form
E − E0 =

X

(δu − δu0 )2 + 2

u∈U

X

(t − δu0 )(δu0 − δu ).

u∈U

By the choice of t, we have t ≤ δu0 and δu ≥ δu0 for u ∈ S and t ≥ δu0 and δu ≤ δu0 for
u∈
/ S. Hence the second sum in E − E 0 is nonnegative, while the first is positive. So the error
decreases; in fact, it decreases by at least (δu − t)2 + (δv − t)2 ≥ (δu − δv )2 /4 for some u ∈ S
and v ∈ U \ S. If we choose the largest gap in the sequence of the δu ordered decreasingly,
then this gain is at least
µ

1
(max δu − min δu )
m

Ã

¶2
≥

1
m

r

E
m

!2
=

E
.
m3

Thus we have
µ
0

E(r ) ≤

1
1− 3
m

¶
E(r).

(6.9)

If we iterate this procedure, we get a sequence of vectors r1 , r2 , · · · ∈ Σ for which E(rk ) →
0. No subsequence of the rk can tend to a boundary point of Σ. Indeed, by Lemma 6.1.10,
for such a sequence δ(r(k) ) would tend to the boundary of P , and so by Corollary 6.1.8, the
error would stay bounded away from 0. Similarly, if a subsequence tends to an interior point
r ∈ Σ, then δ(r) = 0, and by Claim 6.1.9, this limit is unique. It follows that there is a
(unique) point r in the interior of Σ with δ(r) = 0, and the sequence rk tends to this point.

6.1.6

*Another algorithmic proof

We present a second proof of Theorem 6.1.2, due to Colin de Verdière, which is shorter, but
uses a less transparent (more tricky) optimization argument. One advantage is that we can
use an “off the shelf” optimization algorithm for smooth convex functions to compute the
representation.
Define
Zx
φ(x) = 2

arctan(et ) dt.

−∞

It is easy to verify that φ is monotone increasing, convex, and
φ(x) = max{0, πx} + O(1).

(6.10)

6.1. COIN REPRESENTATION

83

Let x ∈ RV , y ∈ RF . Using the numbers βip introduced above in the proof of Lemma
6.1.6, consider the function
´
X ³
F (x, y) =
φ(yp − xi ) − βip (yp − xi ) .
i,p: p∈F (i)

Claim. If |x| + |y| → ∞ while (say) x1 = 0, then F (x, y) → ∞.
We need to fix one of the xi , since if we add the came value to each xi and yp , then the
value of F does not change.
To prove the claim, we use (6.10):
´
X ³
F (x, y) =
φ(yp − xi ) − βip (yp − xi )
i,p: p∈F (i)

=

X

³

i,p: p∈F (i)

=

X

³

´
max{0, π(yp − xi )} − βip (yp − xi ) + O(1)
´
max{−βip (yp − xi ), (π − βip )(yp − xi )} + O(1).

i,p: p∈F (i)

Since −βip is negative but π − βip is positive, each term here is nonnegative, and a given
term tends to infinity if |xi − yp | → ∞. If x1 remains 0 but |x| + |y| → inf ty, then at least
one difference |xi − yp | must tend to infinity. This proves the Claim.
It follows from this Claim that F has a minimum at some point (x, y). Let i be an internal
node, then
X
X
X
∂
F (x, y) = −
φ0 (yp − xi ) +
βip = −2
arctan(eyp −xi ) + 2π
∂xi
p∈F (i)

p∈F (i)

p∈F (i)

(using (6.3)), and so
X
2
arctan(eyp −xi ) = 2π.

(6.11)

p∈F (i)

It follows by a similar computation that
X
π
2
arctan(eyp −xi ) =
3

(6.12)

p∈F (i)

for the three boundary nodes i, and
X
2
arctan(eyp −xi ) = (dp − 2)π
i∈V (p)

for every bounded face p. We can rewrite this condition as
X
2
arctan(exi −yp ) = 2π,
i∈V (p)

since arctan(exi −yp ) + arctan(eyp −xi ) = π/2. This completes the proof.

(6.13)

84

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

6.1.7

*From rubber bands to touching circles

It is not hard to prove (Exercise 6.2) that every double circuit representation of a planar
graph is in fact a rubber band representation. Can we go the other way?
It is not true that every rubber band representation gives rise to a representation by
touching circles. Consider a graph G that is a triangulation of the plane with outer triangle
abc. When constructing a rubber band representation, we have the freedom of choosing the
rubber band strengths arbitrarily. Not all of these lead to different drawings; we have 2(n−3)
equations
X
cij (xi − xj ) = 0
(i 6= a, b, c),
(6.14)
j∈N (i)

which we now consider as equations in the unknowns cij , for a fixed position of the nodes. It
can be shown that these equations are independent (Exercise 6.3), and hence an (m−2n+6)dimensional linear space of the cij lead to one and the same drawing. But this still leaves
us with a manifold of drawings with dimension m − (m − 2n + 6) = 2n − 6, while the
representation with touching circles is uniquely determined.
Let us fix the rubber band strengths arbitrarily, and consider the resulting drawing. Recall
that the positions xi of the nodes can be obtained by minimizing the quadratic function
X
cij |xi − xj |2 ,
(6.15)
ij∈E(G)

or equivalently, by solving the linear equations
X
cij (xi − xj ) = 0
(i 6= a, b, c)

(6.16)

j∈N (i)

(while keeping xa , xb and xc fixed).
When will this come from a touching circle representation? Clearly a necessary and
sufficient condition is that the system of equations
ri + rj = |xi − xj |

(6.17)

has a solution in the ri such that all of them are positive. This system consists of 3n − 6
equations with n unknowns, so it is very much overdetermined. We can compute the best
approximate solution by minimizing the quadratic function
X
(ri + rj − |xi − xj |)2
(6.18)
ij∈E(G)

in the variables ri . The function is strictly convex (since G is nonbipartite), hence the
optimum solution is unique, and satisfies
X
(ri + rj − |xi − xj |) = 0.
(6.19)
j∈N (i)

6.1. COIN REPRESENTATION

85

for all i. This means that these ri satisfy (6.17) “on the average” at each node, but of course
not for particular edges.
Intuitively, if ri + rj < |xi − xj | for an edge, then it is too long, so we want to increase
its strength, while in the other case, we want to decrease it.
[A natural update is
c0ij =

|xi − xj |
cij .
ri + rj

With these new strengths, we get new positions x0i for the nodes by solving (6.16), and then
new approximate radii ri0 by solving (6.19).]
Set ∆xi = xi −x0i , ∆ri = ri −ri0 , ∆cij = cij −c0ij and ∆dij = dij −d0ij , where dij = |xi −xj |.
Then
X
X
(ri0 + rj0 − |x0i − x0j |)2
(ri + rj − |xi − xj |)2 −
∆E =
ij∈E(G)

=

X

ij∈E(G)

(∆ri + ∆rj − ∆dij )(ri + ri0 + rj + rj0 − dij − d0ij ).

ij∈E(G)

Here all terms containing ∆ri sum to 0 by (6.19), and similarly for all terms containing ∆ri .
Thus
X
∆E = −
∆dij (ri + ri0 + rj + rj0 − dij − d0ij ).
ij∈E(G)

Exercise 6.2 Prove that every Andre’ev representation of a planar graph is a rubber band
representation with appropriate rubber band strengths.
Exercise 6.3 Show that equations 6.14 are independent.

6.1.8

Conformal transformations

The double circuit representation of a planar graph is uniquely determined, once a triangular
infinite face is chosen and the circuits representing the nodes of this triangular face are fixed.
This follows from Lemma 6.1.9. Similar assertion is true for double circuit representations
in the sphere. However, in the sphere there is no difference between faces, and we may not
want to “normalize” by fixing a face. Often it is more useful to apply a circle-preserving
transformation that distributes the circles on the sphere in a “uniform” way. The following
Lemma shows that this is possible with various notions of uniformity.
Lemma 6.1.12 Let F : (B 3 )n → B 3 be a continuous map with the property that whenever
n − 2 of the vectors ui are equal to u ∈ S 2 then uT F (u1 , . . . , un ) ≥ 0. Let (C1 , . . . , Cn ) be a
family of openly disjoint caps on the sphere. Then there is a circle preserving transformation
A of the sphere such that F (v1 , . . . , vn ) = 0, where vi = v(ACi ) is the center of ACi .

86

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Examples of functions F to which this lemma are the center of gravity of u1 , . . . , un , or the
center of gravity of their convex hull, or the center of the inscribed ball.
Proof. For every interior point p of the unit ball, we define a circle-preserving transformation Ap of the unit sphere as follows. Take a tangent plane T at p0 = p/kpk, and project the
sphere stereographically on T ; blow up the plane from center p0 by a factor λ = 1/(1 − kpk);
and project it back stereographically to the sphere. Let vi (p) denote the center of the cap
Ap (Ci ). (Warning: this is not the image of the center of Ci in general! Circle-preserving
transformations don’t preserve the centers of circles.)
We want to show that the range of F contains the origin. Suppose not. Let 0 <
ε1 , ε2 , · · · < 1 be a sequence tending to 0. Let Bk be the ball with radius 1 − εk about
the origin. For x ∈ Bk , define
fk (x) = (1 − εk )F (v(Ax C1 ), . . . , v(Ax Cn ))/|F (v(Ax C1 ), . . . , v(Ax Cn ))|.
Then fk is a continuous map from Bk to Bk , and so by Brouwer’s Fixed Point Theorem, it has a fixed point pk . Clearly |pk | = 1 − εk . We may assume by compactness
that pk → p ∈ S 2 and also that v(Apk Ci ) → wi ∈ S 2 . By the continuity of F , we have
F (v(Apk C1 ), . . . , v(Apk Cn )) → F (w1 , . . . , wn ). Furthermore,
F (w1 , . . . , wn )
F (v(Apk C1 ), . . . , v(Apk Cn ))
1
= lim
= lim
fk (pk ) = lim pk = p.
k→∞
|F (w1 , . . . , wn )| k→∞ |F (v(Apk C1 ), . . . , v(Apk Cn )) k→∞ 1 − εk
By elementary geometry, for every point x ∈ S 2 , x 6= p, the images Apk x → −p. Hence
it follows that if Ci does not contain p, then wi = −p. So at most two of {w1 , . . . , wn } are
different from the South pole, and hence F (w1 , . . . , wn )/|F (w1 , . . . , wn )| is contained in the
Southern hemisphere. This is a contradiction.
¤

6.1.9

Applications of circle packing

Planar separators
Koebe’s Theorem has several important applications. We start with a simple proof by Miller
and Thurston [151] of the Planar Separator Theorem 1.2.1 of Lipton and Tarjan [127] (we
present the proof with a weaker bound of 3n/4 on the sizes of the components instead of
2n/3; see [187] for an improved analysis of the method).
We need the notion of the “statistical center”, which is important in many other studies
in geometry. Before defining it, we prove a simple lemma.
Lemma 6.1.13 For every set S ⊆ Rd of n points there is a point c ∈ Rn such that every
closed halfspace containing c contains at least n/(d + 1) elements of S.

6.1. COIN REPRESENTATION

87

Proof. Let H be the family of all closed halfspaces that contain more than dn/(d+1) points
of S. The intersection of any (d + 1) of these still contains an element of S, so in particular
it is nonempty. Thus by Helly’s Theorem, the intersection of all of them is nonempty. We
claim that any c ∈ ∩H satisfies the conclusion of the Lemma.
If H be any open halfspace containing c, then Rd \ H ∈
/ H, which means that H contains
at least n/(d + 1) points of S. If H is a closed halfspace containing c, then it is contained
in an open halfspace H 0 that intersects S in exactly the same set, and applying the previous
argument to H 0 we are done.
¤
A point c as in Lemma 6.1.13 is sometimes called a “statistical center” of the set S.
To make this point well-defined, we call the center of gravity of all points c satisfying the
conclusion of Lemma 6.1.13 the statistical center of the set (note: points c satisfying the
conclusion of Lemma form a convex set, whose center of gravity is well defined).
Proof of Theorem 1.2.1. Let (Ci : i ∈ V ) be a Koebe representation of G on the unit
sphere, and let ui be the center of Ci on the sphere, and ρi , the spherical radius of Ci . By
Lemma 6.1.12, we may assume that the statistical center of the points ui is the origin.
Take any plane H through 0. Let S denote the set of nodes i for which Ci intersects H,
and let S1 and S2 denote the sets of nodes for which Ci lies on one side and the other of H.
Clearly there is no edge between S1 and S2 , and so the subgraphs G1 and G2 are disjoint and
their union is G \ S. Since 0 is a statistical center of the ui , it follows that |S1 |, |S2 | ≤ 3n/4.
It remains to make sure that S is small. To this end, we choose H at random, and
estimate the expected size of S.
What is the probability that H intersects Ci ? If ρi ≥ π/2, then this probability is 1, but
there is at most one such node, so we can safely ignore it, and suppose that ρi < π/2 for
every i. By symmetry, instead of fixing Ci and choosing H at random, we can fix H and
choose the center of Ci at random. Think of H as the plane of the equator. Then Ci will
intersect H if and only if it is center is at a latitude at most ρi (North or South). The area
of this belt around the equator is, by elementary geometry, 4π sin ρi , and so the probability
that the center of Ci falls into here is 2 sin ρi . It follows that the expected number of caps
P
Ci intersected by H is i∈V 2 sin ρi .
To get an upper bound on this quantity, we use the surface area of the cap Ci is 2π(1 −
cos ρi ) = 4π sin2 (ρi /2), and since these are disjoint, we have
X³
i∈V

sin

ρi ´ 2
< 1.
2

(6.20)

Using that sin ρi ≤ 2 sin ρ2i , we get by Cauchy-Schwartz
X
i∈V

√

2 sin ρi ≤ 2 n

Ã

X

i∈V

!1/2
2

(sin ρi )

√

≤4 n

Ã

X³

i∈V

ρi ´ 2
sin
2

!1/2

√
< 4 n.

88

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

√
So the expected size of S is less than 4 n, and so there is at least one choice of H for which
√
|S| < 4 n.
¤
Laplacians of planar graphs
The Planar Separator theorem was first proved by direct graph-theoretic arguments (and
other elegant graph-theoretic proofs are available [14]). For the following theorem on the
eigenvalue gap of the Laplacian of planar graphs by Spielman and Teng [186] there is no
proof known avoiding Koebe’s theorem.
Theorem 6.1.14 For every connected planar graph G = (V, E) on n nodes and maximum
degree D, the second smallest eigenvalue of LG is at most 8D/n.
Proof.
Let Ci : i ∈ V be a Koebe representation of G on the unit sphere, and let ui
be the center of Ci , and ρi , the spherical radius of Ci . By Lemma 6.1.12 may assume that
P
i ui = 0.
The second smallest eigenvalue of LG is given by
P
2
ij∈E (xi − xj )
P
λ2 = min
,
2
P x6=0
i∈V xi
x =0
i

i

Let ui = (ui1 , ui2 , ui3 ), then this implies that
X
X
(uik − ujk )2 ≥ λ2
u2ik
ij∈E

i∈V

holds for every coordinate k, and summing over k, we get
X
X
kui − uj k2 ≥ λ2
kui k2 = λ2 n.
ij∈E

(6.21)

i∈V

On the other hand, we have
¶2
µ
³
ρi
ρj
ρj
ρi ´ 2
ρi + ρj
= 4 sin cos
+ sin
cos
kui − uj k2 = 4 sin
2
2
2
2
2
³
³
³
ρi
ρj ´ 2
ρi ´ 2
ρj ´ 2
≤ 4 sin + sin
≤ 8 sin
+ 8 sin
,
2
2
2
2
and so by (6.20)
X
ij∈E

kui − uj k2 ≤ 8D

X³
i∈V

sin

ρi ´2
≤ 8D.
2

Comparison with (6.21) proves the theorem.

¤

This theorem says that planar graphs are very bad expanders. The result does not
translate directly to eigenvalues of the adjacency matrix or the transition matrix of the
random walk on G, but for graphs with bounded degree it does imply the following:

6.1. COIN REPRESENTATION

89

Corollary 6.1.15 Let G be a connected planar graph on n nodes with maximum degree D.
Then the second largest eigenvalue of the transition matrix is at least 1 − 8D/n, and the
mixing time of the random walk on G is at least Ω(n/D).
Among other applications, we mention a bound on the cover time of the random walk on
a planar graph by Jonasson and Schramm [107].
Exercise 6.4 Show by an example that the bound in Lemma 6.1.13 is sharp.

6.1.10

Circle packing and the Riemann Mapping Theorem

Koebe’s Circle Packing Theorem and the Riemann Mapping Theorem in complex analysis
are closely related. More exactly, we consider the following generalization of the Riemann
Mapping Theorem.
Theorem 6.1.16 (The Koebe-Poincaré Uniformization Theorem) Every open domain in the sphere whose complement has a finite number of connected components is conformally equivalent to a domain obtained from the sphere by removing a finite number of
disjoint disks and points.
In fact, the Circle Packing Theorem and the Uniformization Theorem are mutually limiting cases of each other (Koebe [115], Rodin and Sullivan [173]). The exact proof of this fact
has substantial technical difficulties, but it is not hard to describe the idea.
1. To see that the Uniformization Theorem implies the Circle Packing Theorem, let G be
a planar map and G∗ its dual. We may assume that G and G∗ are 3-connected, and that G∗
has straight edges (these assumptions are not essential, just convenient). Let ε > 0, and let
U denote the ε-neighborhood of G∗ . By Theorem 6.1.16, there is a conformal map of U onto
a domain D0 ⊆ S 2 which is obtained by removing a finite number of disjoint caps and points
from the plane (which we consider as degenerate caps). If ε is small enough, then these caps
are in one-to-one correspondence with the nodes of G. We normalize using Lemma 6.1.12
and assume that the center of gravity of the cap centers is 0.
Letting ε → 0, we may assume that the cap representing any given node v ∈ V (G)
converges to a cap Cv . It is not hard to argue that these caps are non-degenerate, caps
representing different nodes tend to openly disjoint caps, and caps representing adjacent
nodes tend to caps that are touching.
2. In the other direction, let U = S 2 \ K1 \ · · · \ Kn , where K1 , . . . , Kn are disjoint closed
connected sets which don’t separate the sphere. Let ε > 0. It is not hard to construct a
family C(ε) of openly disjoint caps such that the radius of each cap is less than ε and their
tangency graph G is a triangulation of the sphere.

90

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Let Hi denote the subgraph of G consisting of those edges intersecting Ki0 . If ε is small
enough, then the subgraphs Hi are node-disjoint, and each Hi is nonempty except possibly
if Ki is a singleton. It is also easy to see that the subgraphs Hi are connected.
Let us contract each nonempty connected Hi to a single node wi . If Ki is a singleton set
and Hi is empty, we add a new node wi to G in the triangle containing Ki0 , and connect it
to the nodes of this triangle. The spherical map G0 obtained this way can be represented as
the tangency graph of a family of caps D = {Du : u ∈ V (G0 )}. We can normalize so that
the center of gravity of the centers of Dw1 , . . . , Dwn is the origin.
Now let ε → 0. We may assume that each Dwi = Dwi (ε) tends to a cap Dwi (0). Furthermore, we have a map fε that assigns to each node u of Gε the center of the corresponding
cap Du . One can prove (but this is nontrivial) that these maps fε , in the limit as ε → 0,
give a conformal map of U onto S 2 \ Dw1 (0) \ · · · \ Dwn (0).

6.2

*Extensions

There are extensions by Andre’ev [16] and Thurston [195] to circles meeting at other angles.

6.2.1

Orthogonal circles

One of the many extensions of Koebe’s Theorem characterizes triangulations of the plane
that have a representation by orthogonal circles: more exactly, circles representing adjacent
nodes must intersect at 90◦ , other pairs, at > 90◦ (i.e., their centers must be farther apart)
[16, 195, 118] (Figure 6.5.

Figure 6.5: Representing a planar graph by orthogonal circles

Such a representation, if it exists, can be projected to a representation by orthogonal
circles on the unit sphere; with a little care, one can do the projection so that each disk
bounded by one of the circles is mapped onto a “cap” which covers less than half of the
sphere. Then each cap has a unique pole: the point in space from which the part of the
sphere you see is exactly the given cap. The key observation is that two circles are orthogonal
if and only if the corresponding poles have inner product 1 (Figure 6.6). This translates a
representation with orthogonal circles into a representation by vectors of length larger than

6.2. *EXTENSIONS

91

1, where adjacent nodes are represented by vectors with inner product 1, non-adjacent nodes
by vectors with inner product less than 1.

Ci

Cj

ui

uj

Figure 6.6: Poles of circles

This in turn can be translated into semidefinite matrices. We only state the final result
of these transformations. Consider the following two sets of semidefinite constraints:
Y

º

0

Yij

= 1

∀ ij ∈ E,

Yij

<

1

∀ ij ∈
/ E, i 6= j,

Yii

>

1

(6.22)

and the weaker set of constraints
Y

º

0

Yij

= 1

∀ ij ∈ E,

Yij

<

∀ ij ∈
/ E, i 6= j,

1

(6.23)

(6.24)
To formulate the theorem, we need two simple definitions. A cycle C in a graph G is
called separating, if G \ V (C) has at least two connected components, where any chord of
C is counted as a connected component here. The cycle C is called strongly separating, if
G \ V (C) has at least two connected components, each of which has at least 2 nodes. If G is
a 3-connected planar map, then its non-separating cycles are exactly the boundaries of the
faces.
The following was proved in [118]:
Theorem 6.2.1 Let G be a 3-connected planar graph.
(a) If (6.23) has a solution of rank 3, then G is planar.

92

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS
(b) Assume that G is a maximal planar graph. Then (6.22) has a solution of rank 3 if

and only if G has no separating 3- and 4-cycles.
(c) Assume that G is a maximal planar graph. Then (6.23) has a solution with rank 3 if
and only if G has no strongly separating 3- and 4-cycles.

6.2.2

Tangency graphs of general convex domains

Let H be a family of closed convex domains in the plane, such that their interiors are disjoint.
If we represent each of these domains by a node, and connect two of these nodes if the
corresponding domains touch, we get a GH , which we call the tangency graph of the family.
Exercise 6.5 Let H be a family of convex domains in the plane with smooth boundaries
and disjoint interiors. Then the tangency graph of H is planar.
In what follows, we restrict ourselves to the case when the members of H are all homothetical copies of a centrally symmetric convex domain. It is natural in this case to represent
each domain by its center.
Exercise 6.6 Let H be a family of homothetical centrally symmetric convex domains in the
plane with smooth boundaries and disjoint interiors. Then the centers of the bodies give a
straight line embedding in the plane of its tangency graph (Figure 6.7).

Figure 6.7: Straight line embedding of a planar graph from touching convex figures.

Schramm [179] proved the following deep converse to this fact.
Theorem 6.2.2 For a smooth strictly convex domain D, every planar graph can be represented as the tangency graph of a family of homothetical copies of D.

6.3. SQUARE TILINGS

93

Schramm [180] also proved the following very general extension of Theorem 6.1.3:
Theorem 6.2.3 (Caging the Egg) Given a smooth strictly convex body C in R3 , every
3-connected planar graph has a Steinitz representation such that all its edges touch C.
The smoothness of the domain is a condition that cannot be dropped. For example, K4
cannot be represented by touching squares. A strong result about representation by touching
squares, also due to Schramm, will be stated precisely and proved in section 6.3.

6.3

Square tilings

We can also represent planar graphs by squares, rather then circles, in the plane (with some
mild restrictions). There are in fact two quite different ways of doing this: the squares can
correspond to the edges (a classic result of Brooks, Smith, Stone and Tutte), or the squares
can correspond to the nodes (a quite recent result of Schramm).

6.3.1

Current flow through a rectangle

A beautiful connection between square tilings and harmonic functions was described in the
classic paper of Brooks, Smith, Stone and Tutte [35]. They considered tilings of squares by
smaller squares, and used a physical model of current flows to show that such tilings can
be obtained from any connect planar graph. Their ultimate goal was to construct tilings
of a square with squares whose edge-lengths are all different; this will not be our concern;
we’ll allow squares that are equal and also the domain being tiled can be a rectangle, not
necessarily a square.
Consider tiling T of a rectangle R with a finite number of squares, whose sides are parallel
to the coordinate axes. We associate a planar map with this tiling as follows. Represent any
maximal horizontal segment composed of edges of the squares by a single node (say, positioned
at the midpoint of the segment). Each square “connects” two horizontal segments, and we
can represent it by an edge connecting the two corresponding nodes, directed top-down. We
get a directed graph GT (Figure 6.8), with a single source s (representing the upper edge of
the rectangle) and a single sink t (representing the upper edge). It is not hard to see that
GT is planar.
A little attention must be paid to points where four squares meet. Suppose that A, B, C, D
share a corner p, where A is the upper left, and B, C, D follow clockwise. In this case, we may
consider the lower edges of A and B to belong to a single horizontal segment, or to belong to
different horizontal segments. In the latter case, we may or may not imagine that there is an
infinitesimally small square sitting at p. What this means is that we have to declare if the
four edges of GT corresponding to A, B, C and D form two pairs of parallel edges, an empty

94

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS
10
3
3

4
1

2

3

2
5

3
2
10

Figure 6.8: The Brooks–Smith–Stone–Tutte construction

quadrilateral, or a quadrilateral with a horizontal diagonal. We can orient this horizontal
edge arbitrarily (Figure 6.9).

Figure 6.9: Possible declarations about four squares meeting at a point

If we assign the edge length of each square to the corresponding edge, we get a flow f
from s to t: If a node v represents a segment I, then the total flow into v is the sum of edge
length of squares attached to I from the top, while the total flow out of v is the sum of edge
length of squares attached to I from the bottom. Both of these sums are equal to the length
of I.
Let h(v) denote the distance of node v from the upper edge of R. Since the edge-length
of a square is also the difference between the y-coordinates of its upper and lower edges, the
function h is harmonic:
h(i) =

1 X
h(j)
di
j∈N (i)

for every node different from s and t (Figure 6.8).

6.3. SQUARE TILINGS

95

Theorem 6.3.1 For every connected planar map G with two specified nodes s and t on the
unbounded face, there is a unique tiling T of a rectangle such that G is GT .

6.3.2

Tangency graphs of square tilings

Let R be a rectangle in the plane, and consider a tiling of R by squares. Let us add four
further squares attached to each edge of R from the outside, sharing the edge with R. We
want to look at the tangency graph of this family of squares.
Since the squares do not have a smooth boundary, the assertion of Exercise 6.5 does not
apply, and the tangency graph of the squares may not be planar. Let us try to draw the
tangency graph in the plane by representing each square by its center and connecting the
centers of touching squares by a straight line segment. We get into trouble when four squares
share a vertex, in which case two edges will cross at this vertex. In this case we specify
arbitrarily one diametrically opposite pair as “infinitesimally overlapping”, and connect the
centers of these two but not the other two centers. We call this a resolved tangency graph.
Every resolved tangency graph is planar, and it is easy to see that it has exactly one face
that is a quadrangle (namely, the infinite face), and its other faces are triangles; briefly, it is
a triangulation of a quadrilateral (Figure 6.10).
10

3
9

3

3
2

4
1

9

2

5
3

2

10

Figure 6.10: The resolved tangency graph of a tiling of a rectangle by squares

Under some mild conditions, this fact has a converse, due to Schramm [181].
Theorem 6.3.2 Every planar triangulation of a quadrilateral such that no 3-cycle or 4-cycle
contains a point in the interior can be represented as a resolved tangency graph of a square
tiling of a rectangle.
Before proving this theorem, we need some lemmas from combinatorial optimization.
Let G be a planar triangulation of a quadrilateral q1 q2 q3 q4 . We assign a real variable xi
to each node i (this will eventually mean the side length of the square representing i, but at

96

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

the moment, it is just a variable). Consider the following conditions:
X

xi ≥ 0

for all nodes i,

(6.25)

xi ≥ 1

for all q1 –q3 paths P

(6.26)

i∈P

(In this proof, when we talk about paths we mean their node sets.) Let P ⊆ RV denote the
solution set of these inequalities. It is clear that P is an ascending polyhedron.
Lemma 6.3.3 The vertices of P are the incidence vectors of q2 –q4 paths. The blocker of P
is defined by the inequalities
X

xi ≥ 0

for all nodes i,

(6.27)

xi ≥ 1

for all q2 –q4 paths Q.

(6.28)

i∈Q

The vertices of P bl are the incidence vectors of q1 –q3 paths.
These facts can be derived from the Max-Flow-Min-Cut Theorem (or can be proved
directly using the uncrossing method (see Claim 6.3.6 below).
Proof of Theorem 6.3.2. Consider the solution x of (6.25)–(6.26) minimizing the objective
P 2
2
2
function
i xi , and let R be the minimum value. By Theorem 13.7.12, y = (1/R )x
minimizes the same objective function over P bl . Let us rescale these vectors to get a =
Ry. Then we have
X

=

1
R

for all q1 –q3 paths P

(6.29)

ai ≥ R

for all q2 –q4 paths Q

(6.30)

ai ≥

i∈P

X

1
Rx

i∈Q

X

a2i = 1.

(6.31)

i∈V

Furthermore,
aT a =

1 X 2
x = 1.
R2 i i

(6.32)

For each path P = (v1 , . . . , vk ), we define is a-length by
`a (P ) =

1
1
av + av2 + · · · + avk−1 + avk .
2 1
2

Note that if we cut a path into two, then their a-lengths add up to the a-length of the original
path. For any two nodes, their a-distance as the minimum a-length of a path connecting them.

6.3. SQUARE TILINGS

97

We know that x is in the polyhedron defined by (6.25)–(6.26), and so it can be written
as a convex combination of vertices, which are incidence vectors of node sets of q2 –q4 paths.
Hence a can be written as
X
a=
λP 1 P ,
(6.33)
P ∈P

P
where P is a set of q2 –q4 paths, λi > 0 and i λi =
with weight 0.) Similarly, we have a decomposition
X
a=
µQ 1Q ,

1
R.

(Note that we don’t include paths

(6.34)

Q∈Q

P
where the Q is a set of q1 –q3 paths, µj > 0 and j µj = R. Trivially a node v has av > 0 if
and only if it is contained in one of the paths in P (equivalently, in one of the paths in Q).
Claim 6.3.4 |P ∩ Q| = 1 for all P ∈ P, Q ∈ Q.
It is trivial that |P ∩ Q| ≥ 1. By (6.32),
³X
´³X
´ X
1 = aT a =
λP 1P
µQ 1Q =
λP µQ |P ∩ Q|
P

≥

X

Q

λP µQ =

P,Q

³X
P

λP

´³X

P,Q

´
µQ = 1.

Q

We must have equality here, which proves the Claim.
Claim 6.3.5 All paths in P ∈ P have a(P ) = R, and all paths Q ∈ Q have a(Q) = 1/R.
Let (say) Q ∈ Q, then
X
X
X
X
1
a i = a T 1Q =
λP 1T
λP |P ∩ Q| =
λP = .
P 1Q =
R
i∈Q

P ∈P

P ∈P

P ∈P

One consequence of this Claim is that the paths in P and Q are chordless, since if (say)
P ∈ P had a cord uv, then bypassing the part of P between u and v would decrease its total
weight, which would contradict (6.29).
Next, we simplify the decompositions (6.33) and (6.34). In the plane embedding of G with
F = q1 q2 q3 q4 as the infinite face, every q1 –q3 path Q cuts the interior of F into two parts
(one of them may be empty for the paths q1 q2 q3 and q1 q4 q3 ). We think of F as a diamond
standing on its vertex q3 , so we can call the two parts the “left side” and “right side” of Q.
For two q1 –q3 paths we say that Q0 is to the right of Q, if every node of it is either on Q or
on the right side of Q. This defines a partial order on the set of q1 –q3 paths. We say that
a family Q of q1 –q3 paths is laminar, if it is totally ordered. Similarly, every q2 –q4 path P
has an “upper side” and a “lower side”, and we can define when a family of q2 –q4 paths is
laminar.
The following step is known in combinatorial optimization as “uncrossing”.

98

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Claim 6.3.6 In (6.33) and (6.34) the families P and Q can be chosen so that they are
laminar.
Consider any two paths Q, Q0 ∈ Q, and suppose that neither of them is to the right of
the other. Then they must intersect. Let i be a common node of them, then their parts
between q1 and i must have the same `a -length (else, we could interchange these parts and
get a path contradicting (6.29). Hence their common points are in the same order on both of
them, and interchanging their parts between consecutive common points arbitrarily results
in two paths of the same length. This way we can define the path Q ∧ Q0 , which is obtained
by taking always the parts that are to the left, and the path Q ∨ Q0 , which is obtained by
taking always the parts that are to the right.
Now assume that µ ≤ µ0 , and replace µQ 1Q + µQ0 1Q0 with µQ 1Q∧Q0 + µQ 1Q∨Q0 Q +
(λQ0 − λQ )1Q0 in (6.33). It is easy to check that we get another valid decomposition, and
that Q ∧ Q0 , Q ∧ Q0 and Q0 are laminar. We call this procedure uncrossing Q and Q0 .
What needs to be argued is that repeating this uncrossing, in a finite number of steps we
arrive at a laminar family. This is possible but a bit tedious; instead, we take a shortcut as
follows. Let L(Q) denote the set of nodes to the left of the q1 –q3 path Q. Then L(Q ∧ Q0 ) =
L(Q) ∩ L(Q0 ) and L(Q ∨ Q0 ) = L(Q) ∪ L(Q0 ), from which it follows that uncrossing increases
the sum
X

λQ |L(Q)|2 .

(6.35)

Q∈Q

So if we start with a decomposition (6.33) with (6.35) maximal (which clearly exists), then
Q is laminar.
From now on, we assume that P and Q are laminar families.
Claim 6.3.7 Every node i has ai > 0.
Suppose that ai = 0. Then clearly no path in P ∪ Q goes through i. Let Q1 [Q2 ] be the
rightmost [leftmost] path in Q to the left [right] of i, and similarly, let P1 [P2 ] be the lowest
[highest] path in P above [below] i. Each Pt intersects each Qs in exactly one node uts ; let
Pt0 be the part of Pt connecting the intersection points ut1 and ut2 (this may be just a single
node if ut1 = ut2 ). We define Q0s analogously.
The path Pt0 cannot have more than one edge. Indeed, suppose that it has an interior
node j. Then aj > 0 (since j ∈ Pt ), and so there is path Q ∈ Q through j. But then
Q is between Q1 and Q2 in the ordering of Q, which contradicts the choice of Q1 and Q2 .
Similarly, the paths Q0s have at most one edge.
Thus the cycle C = P10 ∪ P20 ∪ Q01 ∪ Q02 is either a triangle or a quadrilateral, and it is
separating since i is in its interior. This is a contradiction, which proves the Claim.

6.3. SQUARE TILINGS

99

Claim 6.3.8 Let i, j ∈ V . Then there is a path P ∈ P going through both i and j if and
only if |da (i, q1 ) − da (j, q1 )| < 21 ai + 12 aj .
Let Pi be the set of those paths in P that go through i, and Pi0 , the set of those paths
in P that go above i. Define Pj and Pj0 similarly. We have ai = λ(Pi ). Furthermore, if Q0
is the part of a path Q ∈ P through i between q1 and i, then every path in Pj0 intersects Q0
in exactly one point, and so da (q1 , i) = λ(Pi0 ) + 12 λ(Pi ) + 12 aq1 . Similarly, aj = λ(Pj ) and
da (q1 , j) = λ(Pj0 ) + 21 λ(Pj ) + 12 aq1 .
Suppose Pi and Pj are disjoint, then by the assumption that P is laminar, one of them is
above the other. Say Pi is above Pj , then Pi0 ∪ Pi ⊆ Pj0 and hence da (i, q1 ) < da (j, q1 ). Thus
1
1
|da (i, q1 ) − da (j, q1 )| = λ(Pj0 ) + λ(Pj ) − λ(Pi0 ) − λ(Pi )
2
2
1
1
1
1
0
0
≥ λ(Pi ) + λ(Pi ) + λ(Pj ) − λ(Pi ) − λ(Pi ) = ai + aj .
2
2
2
2
On the other hand, if Pi and Pj have a path in common, then we have Pj0 ⊂ Pi0 ∪ Pi
(strict containment!), and so
1
1
da (j, q1 ) − da (i, q1 ) = λ(Pj0 ) + λ(Pj ) − λ(Pi0 ) − λ(Pi )
2
2
1
1
1
1
< λ(Pi0 ) + λ(Pi ) + λ(Pj ) − λ(Pi0 ) − λ(Pi ) = ai + aj .
2
2
2
2
The same bound for the difference da (i, q1 ) − da (j, q1 ) follows similarly.
Note the following consequence:
Claim 6.3.9 The inequalities |da (i, q1 ) − da (j, q1 )| < 21 ai + 12 aj and |da (i, q2 ) − da (j, q2 )| <
1
1
2 ai + 2 aj cannot simultaneously hold.
Indeed, by Claim 6.3.8 this would imply that ij is contained in one of the paths in P as
well as in one of the paths in Q, contradicting Claim 6.3.4.
Let ij be any edge. Then clearly |da (i, q1 )−da (j, q1 )| ≤ 21 ai + 21 aj and similarly |da (i, q2 )−
da (j, q2 )| ≤ 12 ai + 12 aj . We know by Claim 6.3.9 that at least one of these inequalities holds
with equality. Using Claim 6.3.8, we see that there will be three types of edges:
(i) |da (i, q1 ) − da (j, q1 )| = 21 ai + 12 aj , but |da (i, q2 ) − da (j, q2 )| < 21 ai + 12 aj ; in this case
ij lies on one of the paths in Q.
(ii) |da (i, q2 ) − da (j, q2 )| = 21 ai + 12 aj , but |da (i, q1 ) − da (j, q1 )| < 21 ai + 21 aj ; in this case
ij lies on one of the paths in P.
(iii) |da (i, q2 ) − da (j, q2 )| = |da (i, q1 ) − da (j, q1 )| = 21 ai + 12 aj , and no path in P or Q goes
through ij.
For each node i, consider the point pi = (da (q1 , i), da (q2 , i)) in the plane, and let Si denote
the square with center pi and side ai .

100

CHAPTER 6. REPRESENTING GRAPHS BY TOUCHING DOMAINS

Claim 6.3.10 The squares Si (i ∈ V ) have no interior point in common.
Indeed, for Si and Sj to intersect, we would need |da (i, q1 ) − da (j, q1 )| < 12 ai + 21 aj and
|da (i, q2 ) − da (j, q2 )| < 12 ai + 21 aj , which is impossible.
Claim 6.3.11 If ij ∈ E, then Si and Sj have a boundary point in common.
This is trivial in all three alternatives (i)–(iii) above: in cases (i) and (ii), the two squares
have a (horizontal, resp. vertical) common boundary segment, in case (iii), they have a
common vertex.
It follows that the four squares Sqi are attached from the outside to the four edges of a
rectangle R, and all the other squares Si are contained in R. We show that they tile the
rectangle.
We can redraw G in the plane so that node i is at position pi , and every edge is a straight
segment. We know from Claims 6.3.10 and 6.3.11 that every edge ij will be covered by the
squares Si and Sj .
Every point of R is contained in a finite face F of G, which is a triangle abc. The
squares Sa , Sb and Sc are centered at the nodes, and each edge is covered by the two squares
centered at its endpoints. Elementary geometry implies that these three squares cover the
whole triangle.
Finally, we show that an appropriately resolved tangency graph of the squares Si is equal
to G. By the above, it contains G (where for edges of type (iii), the 4-corner is resolved so
as to get the edge of G). But since G is a triangulation of the quadrilateral q1 q2 q3 q4 , the
only additional edges of the tangency graph could be q1 q3 or q2 q4 , which are not edges of the
tangency graph.
Thus G is a subgraph of the (resolved) tangency graph of the squares. Since G is a
triangulation, it cannot be a proper subgraph, so it is the tangency graph.
¤
Exercise 6.7 Prove that if G is a resolved tangency graph of a square tiling of a rectangle,
then every triangle in G is a face.
Exercise 6.8 Construct a resolved tangency graph of a square tiling of a rectangle, which
contains a quadrilateral with a further node in its interior.

Chapter 7

Analytic functions on graphs
7.1

Circulations and homology

Let S be a closed orientable compact surface, and consider a map on S, i.e., a graph G =
(V, E) embedded in S so that each face is a disc. We can describe the map as a triple
G = (V, E, F), where V is the set of nodes, E is the set of edges, and F is the set of faces
of G. We fix a reference orientation of G; then each edge e ∈ E has a tail t(e) ∈ V , a head
h(e) ∈ V , a right shore r(e) ∈ F, and a left shore l(e) ∈ F.
The embedding of G defines a dual map G∗ . Combinatorially, we can think of G∗ as the
triple (F, E, V ), where the meaning of “node” and “face”, “head” and “right shore”, and
“tail” and “left shore” is interchanged.
Let G be a finite graph with a reference orientation. For each node v, let δv ∈ RE denote
the coboundary of v:


if h(e) = v,
1,
(δv)e = −1, if t(e) = v,


0,
otherwise.
Thus |δv|2 = dv is the degree of v.
For every face F ∈ F, we denote by ∂F ∈ RE the boundary of F :


if r(e) = F ,
1,
(∂F )e = −1, if l(e) = F ,


0,
otherwise.
Then dF = |∂F |2 is the length of the cycle bounding F .
A vector ϕ ∈ RE is a circulation if
ϕ · δv =

X
e: h(e)=v

ϕ(e) −

X

ϕ(e) = 0

e: t(e)=v

101

(∀v ∈ V ).

102

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

Each vector ∂F is a circulation; circulations that are linear combinations of vectors ∂F
are called null-homologous. Two circulations ϕ and ϕ0 are called homologous if ϕ − ϕ0 is
null-homologous.
A vector ϕ ∈ RE is rotation-free if for every face F ∈ F, we have
ϕ · ∂F = 0.
This is equivalent to saying that ϕ is a circulation on the dual map G∗ .
Rotation-free circulations can be considered as discrete holomorphic 1-forms and they are
related to analytic functions. These functions were introduced for the case of the square grid
a long time ago [71, 54, 55]. For the case of a general planar graph, the notion is implicit in
[35]. For a detailed treatment see [148].
To explain the connection, let ϕ be a rotation-free circulation on a graph G embedded in
a surface. The ϕ can be thought of as a discrete analogue of a holomorphic 1-form. Consider
a planar piece of the surface. Then on the set F 0 of faces contained in this planar piece, we
have a function σ : F 0 → R such that ∂σ = ϕ, i.e., ϕ(e) = σ(r(e)) − σ(l(e)) for every edge e.
Similarly, we have a function π : V 0 → R (where V 0 is the set of nodes in this planar piece),
such that δπ = ϕ, i.e., ϕ(e) = π(t(e)) − π(h(e)) for every edge e. We can think of π and σ as
the real and imaginary parts of a (discrete) analytic function, which satisfy
δπ = ∂σ = ϕ,

(7.1)

which can be thought of as a discrete analogue of the Cauchy–Riemann equations.
Thus we have the two orthogonal linear subspaces: A ⊆ RE generated by the vectors δv
(v ∈ V ) and B ⊆ RE generated by the vectors ∂F (F ∈ F). Vectors in B are 0-homologous
circulations. The orthogonal complement A⊥ is the space of all circulations, and B⊥ is the
space of circulations on the dual graph. The intersection C = A⊥ ∩ B ⊥ is the space of
rotation-free circulations. So RE = A ⊕ B ⊕ C. From this picture we conclude the following.
Lemma 7.1.1 Every circulation is homologous to a unique rotation-free circulation.
It also follows that C is isomorphic to the first homology group of S (over the reals), and
hence we get the following:
Theorem 7.1.2 The dimension of the space C of rotation-free circulations is 2g.

7.2

Discrete holomorphic forms from harmonic functions

We can use harmonic functions to give a more explicit description of rotation-free circulations
in a special case. For any edge e of G, let ηe be the orthogonal projection of 1e onto C.

7.2. DISCRETE HOLOMORPHIC FORMS FROM HARMONIC FUNCTIONS

103

Lemma 7.2.1 Let a, b ∈ E be two edges of G. Then (ηa )b is given by
(πb )h(a) − (πb )t(a) + (πb∗ )r(a) − (πb∗ )l(a) + 1,
if a = b, and by
(πb )h(a) − (πb )t(a) + (πb∗ )r(a) − (πb∗ )l(a) ,
if a 6= b.
Proof.
Let x1 , x2 and x3 be the projections of 1b on the linear subspaces A, B and
C, respectively. The vector x1 can be expressed as a linear combination of the vectors δv
(v ∈ V ), which means that there is a vector y ∈ RV so that x1 = M y. Similarly, we can
write x2 = N z. Together with x = x3 , these vectors satisfy the following system of linear
equations:


 x + M y + N z = 1b
(7.2)
M Tx = 0

 T
N x=0
Multiplying the first m equations by the matrix M T , and using the second equation and the
fact that M T N = 0, we get
M T M y = M T 1b ,

(7.3)

and similarly,
N T N z = N T 1b .

(7.4)

Here M T M is the Laplacian of G and N T N is the Laplacian of G∗ , and so (7.3) implies that
y = πb + c1 for some scalar c. Similarly, z = πb∗ + c∗ 1 for some scalar c0 . Thus
x

= 1b − M T (πb + c1) − N T (πb∗ + c∗ 1)
= 1b − M T πb − N T πb∗ ,

which is just the formula in the lemma, written in matrix form.

¤

The case a = b of the previous formula has the following formulation:
Corollary 7.2.2 For an edge a of a map G, let Ra denote the effective resistance between
the endpoints of a, and let Ra∗ denote the effective resistance of the dual map between the
endpoints of the edge dual to a. Then
(ηa )a = 1 − Ra − Ra∗ .

104

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

7.3

Operations

7.3.1

Integration

Let f and g be two functions on the nodes of a discrete weighted map in the plane. Integration
is easiest to define along a path P = (v0 , v1 , . . . , vk ) in the diamond graph G♦ (this has the
advantage that it is symmetric with respect to G and G∗ ). We define
Z
k−1
X1
f dg =
(f (vi+1 ) + f (vi ))(g(vi+1 ) − g(vi )).
2
P
i=0
The nice fact about this integral is that for analytic functions, it is independent of the
path P , depends on the endpoints only. More precisely, let P and P 0 be two paths on G♦
with the same beginning node and endnode. Then
Z
Z
f dg.
(7.5)
f dg =
P

P0

This is equivalent to saying that
Z
f dg = 0

(7.6)

P

if P is a closed path. It suffices to verify this for the boundary of a face of G♦ , which only
takes a straightforward computation. It follows that we can write
Z v
f dg
u

as long as the homotopy type of the path from u to v is determined (or understood).
Similarly, it is also easy to check the rule of integration by parts: If P is a path connecting
u, v ∈ V ∪ V ∗ , then
Z
Z
f dg = f (v)g(v) − f (u)g(u) −
g df.
(7.7)
P

P

Let P be a closed path in G♦ that bounds a disk D. Let f be an analytic function and g
an arbitrary function. Define ĝ(e) = g(he ) − g(te ) − i(g(le ) − g(re )) (the “analycity defect”
of g on edge e. Then it is not hard to verify the following generalization of (7.6):
Z
X
f dg =
(f (he ) − f (te ))ĝ(e).
(7.8)
P

e⊂D

This can be viewed as a discrete version of the Residue Theorem. For further versions, see
[148].
Take two analytic function f and g, and construct the polygons f (u)Pu (multiplication
by the complex number f (u) corresponds to blowing up and rotating) as in section 7.5.3.
The resulting polygons will not meet at the appropriate vertices any more, but we can try
to translate them so that they do. Now equation (7.6) tells us that we can do that (Figure
7.1(b)). Conversely, every “deformation” of the picture such that the polygons Pu remain
similar to themselves defines an analytic function on G.

7.3. OPERATIONS

105

Figure 7.1: The deformation of the touching polygon representation given by another analytic function.

7.3.2

Critical analytic functions

These have been the good news. Now the bad part: for a fixed starting node u, the function
Z v
F (v) =
f dg
u

is uniquely determined, but it is not analytic in general. In fact, a simple computation shows
that for any edge e,
F (he ) − F (te )
F (le ) − F (re )
−i
`e
`e∗
h
i
f (he ) − f (te )
=i
g(te ) + g(he ) − g(re ) − g(le ) .
`e

F̂ (e) =

(7.9)

So it would be nice to have an analytic function g such that the factor in brackets in (7.9) is
0 for every edge:
g(te ) + g(he ) = g(re ) + g(le )

(7.10)

Rv
Let us call such an analytic function critical. What we found above is that u f dg is an
analytic function of v for every analytic function f if and only if g is critical.
This notion was introduced in a somewhat different setting by Duffin [55] under the name
of rhombic lattice. Mercat [148] defined critical maps: these are maps which admit a critical
analytic function.
Geometrically, this condition means the following. Consider the function g as a mapping
of G ∪ G∗ into the complex plane C. This defines embeddings of G, G∗ and G♦ in the plane
with following (equivalent) properties:
(a) The faces of G♦ are rhomboids.
(b) Every edge of G♦ has the same length.
(c) Every face of G is inscribed in a unit circle.

106

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

(d) Every face of G∗ is inscribed in a unit circle.
Criticality can be expressed in terms of holomorphic forms as well. Let ϕ be a (complex
valued) holomorphic form on a weighted map G. We say that ϕ is critical if the following
condition holds: Let e = xy and f = yz be two edges of G bounding a corner at y, with (say)
directed so that the corner is on their left, then
`e ϕ(e) + `f ϕ(f ) = `e∗ ϕ(e∗ ) − `f ∗ ϕ(f ∗ ).

(7.11)

Note that both f and f ∗ are directed into hf , which explains the negative sign on the right
hand side. To digest this condition, consider a plane piece of the map and a primitive function
g of ψ. Then (7.11) means that
g(y 0 ) − g(y) = g(q) − g(q 0 ),
which we can rewrite in the following form:
g(x) + g(y) − g(p) − g(q) = g(x) + g(y 0 ) − g(p) − g(q 0 ).
This means that g(he ) + g(te ) − g(le ) − g(re ) is the same for every edge e, and since we are
free to add a constant to the value of g at every node in V ∗ (say), we can choose the primitive
function g so that g is critical.
Whether or not a weighted map in the plane has a critical holomorphic form depends on
the weighting. Which maps can be weighted this way? Kenyon and Schlenker [113] answer
this question. Consider any face F0 of the diamond graph G♦ , and a face F1 incident with
it. This is a quadrilateral, so there is a well-defined face F2 so that F0 and F2 are attached
to F1 along opposite edges. Repeating this, we get a sequence of faces (F0 , F1 , F2 . . . ). Using
the face attached to F0 on the opposite side to F1 , we can extend this to a two-way infinite
sequence (. . . , F−1 , F0 , F1 , . . . ). We call such a sequence a track.
Theorem 7.3.1 A planar map has a rhomboidal embedding in the plane if and only if every
track consists of different faces and any two tracks have at most one face in common.

7.3.3

Polynomials, exponentials and approximation

Once we can integrate, we can define polynomials. More exactly, let G be a map in the plane,
and let us select any node to be called 0. Let Z denote a critical analytic function on G such
that Z(0) = 0. Then we have
Z x
1 dZ = Z(x).
0

Now we can define higher powers of Z by repeated integration:
Z x
:n:
Z (x) = n
Z :n−1: dZ.
0

7.4. NONDEGENERACY PROPERTIES OF ROTATION-FREE CIRCULATIONS

107

We can define a discrete polynomial of degree n as any linear combination of
1, Z, Z :2: , . . . , Z :n: . The powers of Z of course depend on the choice of the origin, and the
formulas describing how it is transformed by shifting the origin are more complicated than in
the classical case. However, the space of polynomials of degree see is invariant under shifting
the origin [149]).
Further, we can define the exponential function exp(x) as a discrete analytic function
Exp(x) on V ∩ V ∗ satisfying
dExp(x) = Exp(x)dZ.
More generally, it is worth while to define a 2-variable function Exp(x, λ) as the solution of
the difference equation
dExp(λ, x) = λExp(λ, x)dZ.
It can be shown that there is a unique such function, and there are various more explicit
formulas, including
Exp(λ, x) =

∞
X
Z :n:
,
n!
n=0

(at least as long as the series on the right hand side is absolute convergent).
Mercat [149, 150] uses these tools to show that exponentials form a basis for all discrete
analytic functions, and to generalize results of Duffin [54] about approximability of analytic
functions by discrete analytic functions.

7.4

Nondegeneracy properties of rotation-free circulations

We state and prove two key properties of rotation-free circulations: one, that the projection of
a basis vector to the space of rotation-free circulations is non-zero, and two, that rotation-free
circulations are spread out essentially over the whole graph in the sense that every connected
piece of the graph where a non-zero rotation-free circulation vanishes can be isolated from
the rest by a small number of points. These results were proved in [24, 25].
We start with a simple lemma about maps. For every face F , let aF denote the number of
times the orientation changes if we move along the the boundary of F . For every node v, let
bv denote the number of times the orientation changes in their cyclic order as they emanate
from v.
Lemma 7.4.1 Let G = (V, E, F) be any digraph embedded on an orientable surface S of
genus g. Then
X
X
(aF − 2) +
(bv − 2) = 4g − 4.
F ∈F

v∈V

108

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

Proof. Clearly
X
X
aF =
(dv − bv ),
F

v

and so, using Euler’s formula,
X
X
X
aF +
bv =
dv = 2m = 2n + 2f + 4g − 4.
F

v

v

Rearranging and dividing by 2, we get the equality in the lemma.

¤

If G is planar, then Ra + Ra∗ = 1, a well known fact. For any other underlying surface,
we have
(ηa )a = ηa · 1a = |ηa |2
(since ηa is a projection of 1a ), and so it follows that Ra + Ra∗ ≤ 1. It follows from theorem
7.4.2 below that strict inequality holds here.
If g = 0, then there is no nonzero rotation-free circulation by Theorem 7.1.2, and hence
ηe = 0 for every edge e. But for g > 0 we have:
Theorem 7.4.2 If g > 0, then ηe 6= 0 for every edge e.
Proof. Suppose that ηe = 0 for some edge e. Then by Lemma 7.2.1, there are vectors
π = π(e) ∈ RV and π ∗ = π ∗ (e) ∈ RF such that
∗
∗
πh(a) − πt(a) = πr(a)
− πl(a)

(7.12)

for every edge a 6= e, but
∗
∗
πh(e) − πt(e) = 1 + πr(e)
− πl(a)
.

(7.13)

We define a convenient orientation of G. Let E(G) = E1 ∪ E2 , where E1 consists of
edges a with ϕ(h(a)) 6 ϕ(t(a)), and E2 is the rest. Every edge a ∈ E1 is oriented so that
πh(a) > πt(a). Consider any connected component C of the subgraph formed by edges in E2 .
Let u1 , . . . , uk be the nodes of C that are incident with edges in E1 . Add a new node v to C
and connect it to u1 , . . . , uk to get a graph C 0 . Clearly C 0 is 2-connected, so it has an acyclic
orientation such that every node is contained in a path from v to u1 . The corresponding
orientation of C is acyclic and every has the property that it has no source or sink other than
possibly u1 , . . . , uk .
Carrying this out for every connected component of G0 , we get an orientation of G. We
claim this orientation is acyclic. Indeed, if we had a directed cycle, then walking around it
π would never decrease, so it would have to stay constant. But then all edges of the cycle
would belong to E2 , contradicting the way these edges were oriented.

7.4. NONDEGENERACY PROPERTIES OF ROTATION-FREE CIRCULATIONS

109

We also claim this orientation has only one source and one sink. Indeed, if a node
v 6= h(e), t(e) is incident with an edge of E1 , then it has at least one edge of E1 entering it
and at least one leaving it. If v is not incident with any edge of E1 , then it is an internal node
of a component C, and so it is not a source or sink by the construction of the orientation of
C.
Take the union of G and the dual graph G∗ . This gives a graph H embedded in S. Clearly
H inherits an orientation from G and from the corresponding orientation of G∗ .
We are going to apply Lemma 7.4.1. Every face of H will aF = 2 (this just follows
from the way how the orientation of G∗ was defined). Those nodes of H which arise as the
intersection of an edge of G with an edge of G∗ will have bv = 2.
Consider a node v of G. If v = h(a) then clearly all edges are directed toward v, so
bh(a) = 0. Similarly, we have bt(v) = 0. We claim that bv = 2 for every other node. Since
obviously v is not a source or a sink, we have bv ≥ 2. Suppose that bv > 2. Then we have
for edges e1 , e2 , e3 , e4 incident with v in this cyclic order, so that e1 and e2 form a corner of
a face F , e3 and e4 form a corner of a face F 0 , h(e1 ) = h(e3 ) = v and t(e2 ) = t(e3 ) = v.
Consider π ∗ of the faces incident with v. We may assume that π ∗ (F ) ≤ π ∗ (F 0 ). From the
orientation of the edges e1 and e2 it follows that π ∗ (F ) is larger than π ∗ of its neighbors. Let F
be the union of all faces F 00 with π ∗ (F 00 ) ≥ π ∗ (F ). The boundary of F is an eulerian subgraph,
and so it can be decomposed into edge-disjoint cycles D1 , . . . , Dt . Since the boundary goes
through v twice (once along e1 and e2 , once along two other edges with the corner of F 0 on
the left hand side), we have t ≥ 2, and so one of these cycles, say D1 , does not contain e.
But then by the definition of the orientation and by (7.12), D1 is a directed cycle, which is
a contradiction.
A similar argument shows that if v is a node corresponding to a face not incident with e,
then bv = 0; while if v comes from r(e) or from l(e), then bv = 2.
So substituting in Lemma 7.4.1, only two terms on the left hand side will be non-zero,
yielding −4 = 4g − 4, or g = 0.
¤
Corollary 7.4.3 If g > 0, then there exists a rotation-free circulation that does not vanish
on any edge.
Corollary 7.4.4 If g > 0, then for every edge e, (ηe )e ≥ n−n f −f .
Indeed, combining with the remark after Corollary 7.2.2, we see that (ηe )e > 0 if g > 0.
But (ηe )e = 1 − Re − Re∗ is a rational number, and it is easy to see that its denominator is
not larger than nn f f .
Theorem 7.4.5 Let G be a graph embedded in an orientable surface S of genus g > 0 so
that all faces are discs. Let ϕ be a non-zero rotation-free circulation on G and let G0 be the

110

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

subgraph of G on which ϕ does not vanish. Suppose that ϕ vanishes on all edges incident with
a connected subgraph U of G. Then U can be separated from G0 by at most 4g − 3 points.
The assumption that the connectivity between U and the rest of the graph must be
linear in g is sharp in the following sense. Suppose X is a connected induced subgraph of G
separated from the rest of G by ≤ 2g nodes, and suppose (for simplicity) that X is embedded
in a subset of S that is topologically a disc. Contract X to a single point x, and erase the
resulting multiplicities of edges. We get a graph G0 still embedded in S so that each face is
a disc. Thus this graph has a (2g)-dimensional space of circulations, and hence there is a
non-zero rotation-free circulation ψ vanishing on 2g − 1 of the edges incident with x. Since
this is a circulation, it must vanish on all the edges incident with x. Uncontracting X, and
extending ψ with 0-s to the edges of X, it is not hard to check that we get a rotation-free
circulation.
Proof. Let W be the connected component of G \ V (G0 ) containing U , and let Y denote
the set of nodes in V (G) \ V (W ) adjacent to W .
Consider an edge e with ϕ(e) = 0. If e is not a loop, then we can contract e and get a
map on the same surface with a rotation-free flow on it. If G − e is still a map, i.e., every
face is a disc, then ϕ is a rotation-free flow on it. If G − e is not a map, then both sides of
e must be the same face. So we can eliminate edges with ϕ(e) = 0 unless h(e) = t(e) and
r(e) = l(e) (we call these edges strange loops). In this latter case, we can change ϕ(e) to any
non-zero value and still have a rotation-free flow.
Applying this reduction procedure, we may assume that W = {w} consists of a single
node, and the only edges with ϕ = 0 are the edges between w and Y , or between two nodes
of Y . We cannot try to contract edges between nodes in Y (we don’t want to reduce the size
of Y ), but we can try to delete them; if this does not work, then every such edge must have
r(e) = l(e).
Also, if more than one edge remains between w and a node y ∈ Y , then each of them has
r(e) = l(e) (else, one of them could be deleted). Note that we may have some strange loops
attached at w. Let D be the number of edges between w and Y .
Re-orient each edge with ϕ 6= 0 in the direction of the flow ϕ, and orient the edges between
w and Y alternatingly in an out from w. Orient the edges with ϕ = 0 between two nodes of
Y arbitrarily. We get a digraph G1 .
It is easy to check that G1 has no sources or sinks, so bv ≥ 2 for every node v, and of
course bw ≥ |Y | − 1. Furthermore, every face either has an edge with ϕ > 0 on its boundary,
or an edge with r(e) = l(e). If a face has at least one edge with ϕ > 0, then it cannot be
bounded by a directed cycle, since ϕ would add up to a positive number on its boundary.
If a face boundary goes through an edge with r(e) = l(e), then it goes through it twice in
different directions, so again it is not directed. So we have aF ≥ 2 for every face.

7.5. GEOMETRIC REPRESENTATIONS AND DISCRETE ANALYTIC FUNCTIONS111
Substituting in Lemma 7.4.1, we get that |Y | − 1 ≤ 4g − 4, or |Y | ≤ dw ≤ 4g − 3. Since
Y separates U from G0 , this proves the theorem.

7.4.1

¤

Rubber bands and analytic functions

We apply the method to toroidal graphs.
Let G be a toroidal map. We consider the torus as R2 /Z2 , endowed with the metric
coming from the euclidean metric on R2 . Let us replace each edge by a rubber band, and
let the system find its equilibrium. Topology prevents the map from collapsing to a single
point. In mathematical terms, we are minimizing
X

`(ij)2 ,

(7.14)

ij∈E(G)

where the length `(ij) of the edge ij is measured in the given metric, and we are minimizing over all continuous mappings of the graph into the torus homomorphic to the original
embedding.
It is not hard to see that the minimum is attained, and the minimizing mapping is unique
up to isometries of the torus. We call it the rubber band mapping. Clearly, the edges are
mapped onto geodesic curves. A nontrivial fact is that if G is a simple 3-connected toroidal
map, then the rubber band mapping is an embedding.
We can lift this optimizing embedding to the universal cover space, to get a planar map
which is doubly periodic, and the edges are straight line segments. Moreover, every node is at
the center of gravity of its neighbors. This follows simply from the minimality of (7.14). This
means that both coordinate functions are harmonic and periodic, and so their coboundaries
are rotation-free circulations on the original graph. Since the dimension of the space C of
rotation-free circulations on a toroidal map is 2, this construction gives us the whole space
C.
This last remark also implies that if G is a simple 3-connected toroidal map, then selecting
any basis ϕ1 , ϕ2 in C, the primitive functions of ϕ1 and ϕ2 give a doubly periodic straight-line
embedding of the universal cover map in the plane.

7.5
7.5.1

Geometric representations and discrete analytic
functions
Square tilings and analytic functions

A beautiful connection between square tilings and rotation-free flows was described in the
classic paper of Brooks, Smith, Stone and Tutte [35]. For our purposes, periodic tilings of
the whole plane are more convenient to use.

112

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

Consider tiling of the plane with squares, whose sides are parallel to the coordinate axes.
Assume that the tiling is discrete, i.e., every bounded region contains only a finite number
of squares. We associate a map in the plane with this tiling as follows. As in Section 6.3,
we represent each maximal horizontal segment composed of edges of the squares by a single
node at the midpoint of the segment. Each square “connects” two horizontal segments, and
we can represent it by an edge connecting the two corresponding nodes, directed top-down.
(We handle points where four squares meet just as in the finite case.) We get an (infinite)
directed graph G (Figure 7.2).
It is not hard to see that G is planar. If we assign the edge length of each square to the
corresponding edge, we get a circulation: If a node v represents a segment I, then the total
flow into v is the sum of edge length of squares attached to I from the top, while the total
flow out of v is the sum of edge length of squares attached to I from the bottom. Both of
these sums are equal to the length of I (let’s ignore the possibility that I is infinite for the
moment). Furthermore, since the edge-length of a square is also the difference between the
y-coordinates of its upper and lower edges, this flow is rotation-free.
10

0
2
8
6

11
8
10
11

3
3

8
2

6

14
9

10

6

20
25

Figure 7.2: The Brooks–Smith–Stone–Tutte construction

Now suppose that the tiling is double periodic with period vectors a, b ∈ R2 (i.e., we
consider a square tiling of the torus). Then so will be the graph G, and so factoring out the
period, we get a map on the torus. Since the tiling is discrete, we get a finite graph. This also
fixes the problem with the infinite segment I: it will become a closed curve on the torus, and
so we can argue with its length on the torus, which is finite now. The flow we constructed
will also be periodic, so we get a rotation-free circulation on the torus.
We can repeat the same construction using the vertical edges of the squares. It is not
hard to see this gives the dual graph, with the dual rotation-free circulation on it.
This construction can be reversed. Take a toroidal map G∗ and any rotation-free circulation on it. Then this circulation can be obtained from a doubly periodic tiling of the plane

7.5. GEOMETRIC REPRESENTATIONS AND DISCRETE ANALYTIC FUNCTIONS113
by squares, where the edge-length of a square is the flow through the corresponding edge.
(We suppress details.)
If an edge has 0 flow, then the corresponding square will degenerate to s single point.
Luckily, we know (Corollary 7.4.3) that for a simple 3-connected toroidal map, there is always
a nowhere-zero rotation-free circulation, so these graphs can be represented by a square tiling
with no degenerate squares.

7.5.2

Circle packings and analytic functions

We can also relate to discrete holomorphic forms are circle representations.
It is again best to go to the universal cover map Ĝ. Then the result says that for every
3-connected toroidal graph G we can construct two (infinite, but discrete) families F and
F ∗ of circles in the plane so that they are double periodic modulo a lattice L = Za + Zb,
F (mod L) corresponds to the nodes of G, F ∗ (mod L) corresponds to the faces of G, and
for ever edge e, there are two circles C, C 0 representing he and te , and two circles D and D0
representing re and le so that C, C 0 are tangent at a point p, D, D0 are tangent at the same
point p, and C, D are orthogonal.
If we consider the centers the circles in F as nodes, and connect two centers by a straight
line segment if the circles touch each other, then we get a straight line embedding of the
universal cover map in the plane (appropriately periodic modulo L). Let f (i) denote the
point representing node i of the universal cover map. or of its dual.
To get a holomorphic form out of this representation, consider the plane as the complex
plane, and define ϕ(ij) = ρ(j) − ρ(i) for every edge of Ĝ or Ĝ∗ . Clearly ϕ is invariant under
L, so it can be considered as a function on E(G). By the orthogonality property of the circle
representation, ϕ(e)/ϕ(e∗ ) is a positive multiple of i. In other words,
ϕ(e)
ϕ(e∗ )
=i
|ϕ(e)|
|ϕ(e∗ )|
It follows that if we consider the map G with weights
`e = |ϕ(e)|,

`e∗ = |ϕ(e∗ )|,

then ϕ is a discrete holomorphic form on this weighted map.
It would be nice to be able to turn this construction around, and construct a circle
representation using discrete holomorphic forms.

7.5.3

Touching polygon representations

Kenyon’s ideas in [112] give a another geometric interpretation of analytic functions. Let G
be a map in the plane and let f be an analytic function on G. Let us assume that there is
a straight-line embedding of G in the plane with convex faces, and similarly, a straight-line

114

CHAPTER 7. ANALYTIC FUNCTIONS ON GRAPHS

embedding of G∗ with convex faces. Let Pu denote the convex polygon representing the face
of G (or G∗ ) corresponding to u ∈ V ∗ (or u ∈ V )). Shrink each Fu from the point f (u) by
a factor of 2. Then we get a system of convex polygons where for every edge uv ∈ G♦ , the
two polygons Pu and Pv share a vertex at the point (f (u) + f (v))/2 (Figure 7.1(a)). There
are two kinds of polygons (corresponding to the nodes in V and V ∗ , respectively). It can be
shown that the interiors of the polygons Pu will be disjoint (the point f (u) is not necessarily
in the interior of Pu ). The white areas between the polygons correspond to the edges of G.
They are rectangles, and the sides of the rectangle corresponding to edge e are f (he ) − f (te )
and f (le ) − f (re ).

7.6

Novikov’s discrete analytic functions

A similar, but different theory of discrete analytic functions was developed by Dynnikov and
Novikov [57].

7.7

Discrete analytic functions from circle packings

There is a book on this subject by Kenneth Stephenson [190].

Chapter 8

The Colin de Verdière Number
In 1990, Colin de Verdière [44] introduced a spectral invariant µ(G) of a graph G (for a
survey, see [102]).

8.1
8.1.1

The definition
Motivation

Let G be a connected graph. We know that (by the Perron–Frobenius Theorem) the largest
eigenvalue of the adjacency matrix has multiplicity 1. What about the second largest?
The eigenvalues of most graphs are all different, so the multiplicity of any of them gives
any information about the graph in rare cases only. Therefore, it makes sense to try to
maximize the multiplicity of the second largest eigenvalue by weighting the edges by positive
numbers. The diagonal entries of the adjacency matrix don’t carry any information, so we
allow to put there any real numbers. The off-diagonal matrix entries that correspond to
nonadjacent nodes remain 0.
There is a technical restriction, called the Strong Arnold Property, which excludes very
degenerate choices of edgeweights and diagonal entries. We’ll discuss this condition later.
If we maximize the multiplicity of the largest eigenvalue this way, we get the number of
connected components of the graph. So for the second largest, we expect to get some parameter related to connectivity. However, the situation is more complex (and more interesting).
Since we don’t put any restriction on the diagonal entries, we may add a constant to
the diagonal entries to shift the spectrum so that the second largest eigenvalue becomes 0,
without changing its multiplicity. The multiplicity in question is then the corank of the
matrix (the dimension of its nullspace).
Finally, we multiply the matrix by −1, to follow convention.
This discussion hopefully makes most of the formal definition in the next section clear.
115

116

CHAPTER 8. THE COLIN DE VERDIÈRE NUMBER

8.1.2

Formal definition

For a connected graph G = (V, E), we consider a matrix M ∈ RV ×V with the following
properties:
(
< 0, if ij ∈ E,
(M1) Mij
= 0, if ij ∈
/ E, i 6= j;
(M2) M has exactly one negative eigenvalue (of multiplicity 1).
(M3) [Strong Arnold Property] If X is a symmetric n × n matrix such that Xij = 0
whenever i = j or ij ∈ E, and M X = 0, then X = 0.
The Colin de Verdiére number µ(G) of the graph is defined as the maximum corank of
such a matrix.

8.1.3

The Strong Arnold Property
[2]

Condition (M3) requires some explanation. Let RV denote the space of all symmetric V ×V
[2]
matrices. Consider the manifold Rk of all matrices in RV with rank at most k := rk(M ),
[2]
and the linear space OG of all symmetric n × n matrices A in RV such that Aij = 0 for all
ij ∈ E.
We need the following lemma describing the tangent space of Rk and its orthogonal
complement.
Lemma 8.1.1 Let M ∈ Rk , and let S denote the matrix of the orthogonal projection onto
the nullspace of M .
(a) The tangent space T (M ) of Rk at M consists of all matrices of the form M U + U T M ,
where U is a (not necessarily symmetric) V × V matrix.
(b) The normal space N (M ) of Rk at M consists of all symmetric V × V matrices X
such that M X = 0.
Proof. (a) Suppose that Y = M U + U T M . Consider the family
M (t) = (I + tU )T M (I + tU ).
Clearly rk(M (t)) ≤ rk(M ) ≤ k and equality holds if |t| is small enough. Furthermore,
M 0 (0) = M U + U T M = Y . Hence Y ∈ T (M ).
Conversely, let Y ∈ T (M ). Then there is a one-parameter differentiable family M (t) of
symmetric matrices, defined in a neighborhood of t = 0, so that M (t) ∈ Rk , M (0) = M and
M 0 (0) = Y . Let S(t) denote the matrix of the orthogonal projection onto the nullspace of
M (t), and set S = S(0). By definition, we have M (t)S(t) = 0, and hence by differentiation,

8.1. THE DEFINITION

117

we get M 0 (0)S(0) + M (0)S 0 (0) = 0, or Y S + M S 0 (0) = 0. Multiplying by S from the left,
we get SY S = 0. So we can write
Y =

1
((I − S)Y (I + S) + (I + S)Y (I − S)).
2

Notice that I − S is the orthogonal projection onto the range of M , and hence we can write
I − S = M V with some matrix V . By transposition, we have I − S = V T M . Then
Y =

1
1
M V Y (I + S) + (I + S)Y V T M = M U + U T M,
2
2

where U = 12 V Y (I + S).
(b) We have X ∈ N (M ) iff X · Y = 0 for every Y ∈
T T (M ). By (a), this means that X · (M U + U T M ) = 0 for every U . This can be written as
0 = Tr(X(M U +U T M )) = Tr(XM U )+Tr(XU T M ) = Tr(XM U )+Tr(M U X) = 2Tr(XM U ).
This holds for every U if and only if XM = 0.

¤

Aij = 0 ∀ ij ∉ E
M

rk( A) = rk( M )

Figure 8.1: The Strong Arnold Property

Lemma 8.1.2 For a connected graph G and a matrix M satisfying (M1) and (M2), the
following are equivalent.
(a) (M3) holds.
(b) The manifolds Rk and OG intersect transversally at M .
(c) For every symmetric matrix B there exists a matrix A ∈ OG such that xT (A−B)x = 0
for all vectors x in the nullspace of M .
Proof. Let TM denote the tangent space of Rk at M .

118

CHAPTER 8. THE COLIN DE VERDIÈRE NUMBER

Then Xij = 0 for all ij ∈ E ∪ ∆ means that X is orthogonal to OG . Moreover, using
some elementary linear algebra and differential geometry one can easily show that M X = 0
is equivalent to saying that X is orthogonal to TM , the tangent space of Rk at M .
Hence condition (M3) is equivalent to requiring that TM and OG span the space of all
symmetric n × n matrices.
¤
Exercise 8.1 Let f (G) denote the maximum multiplicity or the largest eigenvalue of any
matrix obtained from the adjacency matrix of a graph G, where 1’s are replaced by arbitrary
positive numbers and the diagonal entries are changed arbitrarily. Prove that f (G) is the
number of connected components of the graph G.
Exercise 8.2

8.2

Basic properties

Theorem 8.2.1 The Colin de Verdière number is minor-monotone.
Proof.
¤
Theorem 8.2.2 If µ(G) > 2, then µ(G) is invariant under subdivision.
Proof.
¤
The following result was proved by Bacher and Colin de Verdière [19].
Theorem 8.2.3 If µ(G) > 3, then µ(G) is invariant under ∆ − Y transformation.
Proof.
¤

8.3

Small values

Graphs with Colin de Verdière number up to 4 are characterized.
Theorem 8.3.1 Let G be a connected graph.
(a) µ(G) ≤ 1 if and only if G is a path;
(b) µ(G) ≤ 2 if and only if G is outerplanar;
(c) µ(G) ≤ 3 if and only if G is planar;
(d) µ(G) ≤ 4 if and only if G is linklessly embedable.

8.4. NULLSPACE REPRESENTATION

119

Statements (a)–(c) were proved by Colin de Verdière [43]; an elementary proof is due to
Hein van der Holst [95]); (d) is due to Lovász and Schrijver [138].
Proof.
¤

8.4

Nullspace representation

Every weighted adjacency matrix M with a d-dimensional nullspace gives rise to an embedding of the graph in d-space: we take a basis v1 , . . . , vd of the null space, write them down
as column vectors. Each node i of the graph corresponds to a row ui of the matrix obtained,
which is a vector in Rd , and so we get the nullspace representation of the graph. Note that
this representation is uniquely determined up to a linear transformation of Rd .
Saying this in a fancier way, each coordinate function is a linear functional on the
nullspace, and so the nullspace representation is in fact a representation in the dual space of
the nullspace.
The nullspace representation u satisfies the equations
X
Mij uj = 0
for all i ∈ V.
(8.1)
j∈V

In the case when M is a Colin der Verdière matrix of the graph G, we have Mij < 0 for every
edge ij, and so we can write this as
X
(−Mij )uj = Mii ui
for all i ∈ V,
j∈N (i)

and so each vector ui is in the cone spanned by its neighbors, or in the negative of this cone
(depending on the sign of Mii ). So from this matrix we get a representation of the graph in
µ(G)-space.

8.4.1

A nodal theorem for Colin de Verdière matrices

Van der Holst [95] proved a key lemma about Colin de Verdière matrices, which was subsequently generalized [98, 139]. We state a generalization i a geometric form.
Lemma 8.4.1 Let G be a connected graph with µ(G) = µ, let M be a Colin de Verdière
matrix of G, and let u be a nullspace representation belonging to M . Let H be a closed
halfspace in Rd containing the origin. Then the subgraph G0 of G induced by nodes in the
interior of H is nonempty, and either
— G0 is connected, or
— 0 is on the boundary of H, and there is a linear subspace S ⊆ H of dimension µ−2 and
r ≥ 3 half-hyperplanes T1 , . . . , Tr with boundary S, so that each node of G lies on ∪i Ti , and

120

CHAPTER 8. THE COLIN DE VERDIÈRE NUMBER

Figure 8.2: Two possibilities for the nullspace representation of a connected graph:
for every hyperplane through the origin, either both sides are connected, or...

each edge of G is contained in one of the Ti . Furthermore, each node of G on S is connected
to nodes on every Ti , or else only to nodes on S. The halfspace H contains at least two of
the half-hyperplanes Ti .
(See Figure 8.2.)
Proof.
¤
Corollary 8.4.2 Let G be a connected graph with µ(G) = µ, let M be a Colin de Verdière
matrix of G, and let u be a nullspace representation belonging to M .
(a) (Van der Holst’s Lemma) Every (µ − 1)-dimensional subspace of Rµ spanned by µ − 1
vectors ui separates G into two non-empty connected subgraphs.
(b) Every (µ−1)-dimensional subspace of Rµ not containing any of the vectors ui separates
G into two non-empty connected subgraphs.
(c) Every open halfspace of Rµ containing the origin contains a non-empty connected
subgraph of G.
Sometimes it is more useful to set this in algebraic terms.
Corollary 8.4.3 (a) Let x ∈ ker M have minimal support.

Then both supp+ (x) and

supp− (x) are nonempty and induce connected subgraphs of G.
(b) Let x ∈ ker M have supp(x) = V . Then both supp+ (x) and supp− (x) are nonempty
and induce connected subgraphs of G.
In fact, one can prove a stronger lemma in algebraic terms, which we state as an exercise.

8.5. GRAM REPRESENTATION

121

Exercise 8.3 Let G be a connected graph with µ(G) = µ, let M be a Colin de Verdière
matrix of G, and let x ∈ RV be any vector with M x ≤ 0. Then supp+ (x) is nonempty, and
either
— supp+ (x) spans a connected subgraph, or
— G has a cutset S with the following properties: Let G1 , . . . , Gr (r ≥ 2) be the connected
components of G − S. There exist non-zero, non-negative vectors x1 , . . . , xr and y in RV
such that supp(xi ) = V (Gi ) and supp(y) ⊆ S, M x1 = M x2 = · · · = M xr = −y, and x is a
P
P
linear combination x = i αi xi , where i αi ≥ 0, and at least two αi are positive.

8.4.2

Steinitz representations and Colin de Verdière matrices

For 3-connected planar graphs, the nullspace representation gives a Steinitz representation
[139, 134]; in fact, Steinitz representations naturally correspond to Colin de Verdière matrices.

8.5

Gram representation

Every Colin de Verdiére matrix of a graph gives rise to another geometric representation of
the graph, this time in the (n − µ(G) − 1)-dimensional space [118]. This is related to the
Koebe–Andre’ev representation if the complement of the graph is a maximal planar graph.

8.6

Related graph parameters

Van der Holst’s lemma motivates this related graph parameter, which is again related to
planarity and other geometric representations. This was studied by Van der Holst, Laurent
and Schrijver [97].

122

CHAPTER 8. THE COLIN DE VERDIÈRE NUMBER

Chapter 9

Orthogonal representations
9.1

Orthogonal representations: definition

Let G = (V, E) be a simple graph. We will denote by G = (V, E) its complement.
An orthogonal representation of a graph G = (V, E) in Rd assigns to each i ∈ V a nonzero
vector ui ∈ Rd such that uT
i uj = 0 whenever ij ∈ E. An orthonormal representation is an
orthogonal representation in which all the representing vectors have unit length. Clearly we
can always scale the vectors forming an orthogonal representation this way, and usually this
does not change any substantial feature of the problem.
Sometimes we specify a further unit vector c ∈ Rd , and call it the handle of the orthogonal
representation. (For the origin of the name, see Example 9.1.4 below).
Note that we did not insist that different nodes are mapped onto different vectors, nor
that adjacent nodes are mapped on non-orthogonal vectors. If these conditions also hold, we
call the orthogonal representation faithful.
Example 9.1.1 Every graph has a trivial orthogonal (in fact, orthonormal) representation
in RV , in which node i is represented by the standard basis vector ei .
Of course, we are interested in “nontrivial” orthogonal representations, which are more
“economical” than the trivial one.
Example 9.1.2 Figure 9.1 below shows that for the graph obtained by adding a diagonal
to the pentagon a simple orthogonal representation in 2 dimensions can be constructed.
This example can be generalized as follows.
Example 9.1.3 Let k = χ(G), and let {B1 , . . . , Bk } be a family of disjoint complete subgraphs covering all the nodes. Let {e1 , . . . , ek } be the standard basis of Rk . Then mapping
every x ∈ Bi to ei is an orthonormal representation.
123

124

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

b

a=b=c
c

a
d
e

0

c=d

Figure 9.1: An (almost) trivial orthogonal representation

A more “geometric” orthogonal representation is described by the following example.
Example 9.1.4 Consider an “umbrella” in R3 with a unit vector c as its handle, and 5 ribs
of unit length (Figure 9.2). Open it up to the point when non-consecutive ribs are orthogonal.
This way we get 5 unit vectors u0 , u1 , u2 , u3 , u4 , assigned to the nodes of C5 so that each ui
forms the same angle with c and any two non-adjacent nodes are labeled with orthogonal
vectors. These vectors give an orthogonal representation of C5 in 3-space.

Figure 9.2: An orthogonal representation of C5 .

9.2

Smallest cone and the theta function

When looking for “economic” orthogonal representations, we can define “economic” in several
ways. For example, we may want to find an orthogonal representation in a dimension as low
as possible (even though this particular way of phrasing the question does not seem to be
the most fruitful). Other definitions of “economic” are also often related to interesting graph
properties.

9.2. SMALLEST CONE AND THE THETA FUNCTION

125

We start with the problem from information theory that motivated th introduction of
orthogonal representations [131].

9.2.1

Shannon capacity

Consider a noisy channel through which we are sending messages over a finite alphabet V .
The noise may blur some letters so that certain pairs can be confused. We want to select as
many words of length k as possible so that no two can possibly be confused. As we shall see,
the number of words we can select grows as Θk for some Θ ≥ 1, which is called the Shannon
zero-error capacity of the channel.
In terms of graphs, we can model the problem as follows. We consider V as the set of
nodes of a graph, and connect two of them by an edge if they can be confused. This way we
obtain a graph G = (V, E). We denote by α(G) the maximum number of independent points
(the maximum size of a stable set) in the graph G. If k = 1, then the maximum number of
non-confusable messages is α(G).
To describe longer messages, we define the strong product G£H of two graphs G = (V, E)
and H = (W, F ) as the graph with V (G £ H) = V × W , with (i, u)(j, v) ∈ E(G £ H) iff
ij ∈ E and uv ∈ F , or ij ∈ E and u = v, or i = j and uv ∈ F . (If, for the purposes of this
problem, we define two nodes to be adjacent if they are either equal or connected by and
edge, then we can say that (i, u) and (j, v) are adjacent in G £ H if and only if i is adjacent
to j in G and u is adjacent to v in H.)
It is easy to see that this multiplication is associative and commutative (if we don’t
distinguish isomorphic graphs). The product of k copies of G is denoted by Gk .
Then α(Gk ) is the maximum number of words of length k, composed of elements of V ,
so that for every two words there is at least one i (1 ≤ i ≤ k) such that the i-th letters are
different and non-adjacent in G, i.e., non-confusable. It is easy to see that
α(G £ H) ≥ α(G)α(H).

(9.1)

This implies that
α(Gk+l ) ≥ α(Gk )α(Gl ),

(9.2)

α(Gk ) ≥ α(G)k .

(9.3)

and

The Shannon capacity of a graph G is the value
Θ(G) = lim α(Gk )1/k
k→∞

Inequality (9.2) implies that the limit exists, and inequality (9.3) implies that
Θ(G) ≥ α(G).

(9.4)

126

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

It is not known whether Θ(G) can be computed for all graphs by any algorithm (polynomial or not), although there are several special classes of graphs for which this is not
hard.
Example 9.2.1 Let C4 denote a 4-cycle with nodes (a, b, c, d). By (9.4), we have Θ(G) ≥ 2.
On the other hand, if we use a word, then all the 2k words obtained from it by replacing a and
b by each other, as well as c and d by each other, are excluded. Hence α(C4k ) ≤ 4k /2k = 2k ,
which implies that Θ(C4 ) = 2.
The argument for bounding Θ from above in this last example can be generalized as
follows. Let χ(G) denote the minimum number of complete subgraphs covering the nodes of
G. (This is the same as the chromatic number of the complementary graph.) Trivially
α(G) ≤ χ(G),

(9.5)

and it is easy to see that
χ(G £ H) ≤ χ(G)χ(H).

(9.6)

Hence
α(Gk ) ≤ χ(Gk ) ≤ χ(G)k ,
and thus
Θ(G) ≤ χ(G).

(9.7)

It follows that if α(G) = χ(G), then Θ(G) = α(G); for such graphs, nothing better can be
done than reducing the alphabet to the largest mutually non-confusable subset.
Example 9.2.2 The smallest graph for which Θ(G) cannot be computed by these means
is the pentagon C5 . If we set V (C5 ) = {0, 1, 2, 3, 4} with E(C5 ) = {01, 12, 23, 34, 40}, then
C52 contains the stable set {(0, 0), (1, 2), (2, 4), (3, 1), (4, 3)}. So α(C5 )2k = α(C52 )k ≥ 5k , and
√
hence Θ(C5 ) ≥ 5.
We will show that equality holds here [131], but first we need some definitions.

9.2.2

Definition and basic properties of the theta function

The smallest angle of a rotational cone (in arbitrary dimension) which contains all vectors in
an orthogonal representation of the graph gives rise to the theta-function of the graph [131].
Formally, we define
ϑ(G) = min max
(ui ),c i∈V

1
,
(cT ui )2

9.2. SMALLEST CONE AND THE THETA FUNCTION

127

where the minimum is taken over all orthonormal representations (ui : i ∈ V ) of G and all
unit vectors c. (Of course, we could fix c, but this is not always convenient.)
From the trivial orthogonal representation (Example 9.1.1) we get that
ϑ(G) ≤ |V |.
Tighter inequalities can be proved:
Theorem 9.2.3 For every graph G,
α(G) ≤ ϑ(G) ≤ χ(G)).
Proof. First, let S ⊆ V be a maximum independent set of nodes in G. Then in every
orthonormal representation (ui ), the vectors {ui : i ∈ S} are mutually orthogonal unit
vectors. Hence
X
1 = cT c ≥
(cT ui )2 ≥ |S| min(cT ui )2 ,
i∈S

i

and so
max

1

i∈V (cT ui )2

≥ |S| = α(G).

This implies the first inequality.
The second inequality follows from Example 9.1.3, using c = k1 (e1 + . . . ek ) as the handle.
¤
From Example 9.1.4 we get, using elementary trigonometry that
√
ϑ(C5 ) ≤ 5.

(9.8)

We’ll see that equality holds here.
Lemma 9.2.4 For any two graphs G and H,
ϑ(G £ H) ≤ ϑ(G)ϑ(H).
We will prove later (Corollary 9.2.17) that equality holds here.
Proof.

The tensor product of two vectors (u1 , . . . , un ) ∈ Rn and (v1 , . . . , vm ) ∈ Rm is the

vector
u ◦ v = (u1 v1 , . . . , u1 vm , u2 v1 , . . . , u2 vm , . . . , un v1 , . . . , un vm ) ∈ Rnm .
The inner product of two tensor products can be expressed easily: if u, x ∈ Rn and v, y ∈ Rm ,
then
(u ◦ v)T (x ◦ y) = (uT x)(v T y).

(9.9)

128

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Now let (ui : i ∈ V ) be an optimal orthogonal representation of G with handle c (ui , c ∈ Rn ),
and let (vj : j ∈ V (H)) be an optimal orthogonal representation of H with handle d
(vj , d ∈ Rm ). It is easy to check, using (9.9), that the vectors ui ◦ vj ((i, j) ∈ V (G) × V (H))
form an orthogonal representation of G £ H. Furthermore, taking c ◦ d as its handle, we have
by (9.9) again that
¡
¢2
(c ◦ d)T (ui ◦ vj ) = (cT ui )2 (d ◦ vj )2 ≥

1
1
·
,
ϑ(G) ϑ(H)

and hence
1
ϑ(G £ H) ≤ max ¡
¢2 ≤ ϑ(G)ϑ(H).
i,j
T
(c ◦ d) (ui ◦ vj )
¤
This inequality has some important corollaries. First, we have
α(Gk ) ≤ ϑ(Gk ) ≤ ϑ(G)k ,
which implies
Corollary 9.2.5 For every graph G,
Θ(G) ≤ ϑ(G).
Second, the product graph G £ G has independent points in the diagonal, implying that
ϑ(G £ G) ≥ α(G £ G) ≥ |V |.
Hence
Corollary 9.2.6 For every graph G,
ϑ(G)ϑ(G) ≥ |V |.
√
Since C5 ∼
= C5 , Corollary 9.2.6 implies that ϑ(C5 ) ≥ 5, and together with (9.8), we get
√
that ϑ(C5 ) = 5. Equality does not hold in general in Corollary 9.2.6, but it does when G
has a node-transitive automorphism group. We postpone the proof of this fact until some
further formulas for ϑ will be developed (Corollary 9.2.14).
Exercise 9.1 If ϑ(G) = 2, then G is bipartite.
Exercise 9.2 (a) If H is an induced subgraph of G, then ϑ(H) ≤ ϑ(G). (b) If H is a
spanning subgraph of G, then ϑ(H) ≥ ϑ(G).

9.2. SMALLEST CONE AND THE THETA FUNCTION

129

Exercise 9.3 Let G be a graph and v ∈ V . (a) ϑ(G − v) ≥ ϑ(G) − 1. (b) If v is an isolated
node, then equality holds. (c) If v is adjacent to all other nodes, then ϑ(G − v) = ϑ(G).
Exercise 9.4 Let G = (V, E) be a graph and let V = S1 ∪ · · · ∪ Sk e a partition of V . (a)
P
Then ϑ(G) ≤ i ϑ(G[Si ]). (b) If no edge connects nodes in different sets Si , then equality
holds. (c) Suppose that any two nodes in different sets Si are adjacent. How can ϑ(G) be
expressed in terms of the ϑ(G[Si ])?

9.2.3

More expressions for ϑ

We prove a number of formulas for ϑ(G), which together have a lot of implications. These
will be proved together.
For every graph G, let M(G) denote the set of symmetric matrices A ∈ RV ×V ,such that
Aij = −1 if i = j or ij ∈ E.
Proposition 9.2.7 (Minimizing the largest eigenvalue) For every graph G = (V, E),
ϑ(G) = min{λmax (A) : A ∈ M(G)}.
Consider the following semidefinite programs:
minimize
subject to

t
Y º0
Yij = −1 (∀ ij ∈ E)

(9.10)

Yii = t − 1
and
maximize

X

Zij

i,j∈V

subject to

Zº0
Zij = 0

(9.11)
(∀ ij ∈ E)

tr(Z) = 1
It is not hard to check that these are duals.
Proposition 9.2.8 (Semidefinite programs) For every graph G, ϑ(G) is the optimum of
either one of the programs (9.10), (9.11).
Finally, we use orthonormal representations of the complementary graph:

130

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Proposition 9.2.9 (Complementary graph) For every graph G,
ϑ(G) = max

X

(dT vi )2 ,

i∈V

where the maximum extends over all orthonormal representations (vi : i ∈ V ) of the complementary graph G and all unit vectors d.
Proof. We prove Propositions 9.2.7, 9.2.8 and 9.2.9 together. First we show:
ϑ(G) ≤ min{λmax (A) : A ∈ M(G)}.

(9.12)

Let A be a matrix attaining the minimum on the right hand side, and let t = λmax (A). Then
the matrix tI − A is positive semidefinite, and so it can be written as a Gram matrix, i.e.,
there are vectors xi ∈ Rn such that
(
−1
if ij ∈ E,
xT
i xj = (tI − A)ij =
t − 1 if i = j
(no condition if ij ∈ E). Let c be a vector orthogonal to all the xi (we increase the dimension
of the space if necessary), and set
1
ui = √ (c + xi ).
t
T
Then uT
i ui = 1 and ui uj = 0 for ij ∈ E, so (ui ) is an orthonormal representation of G.
√
Furthermore, with handle c we have cT ui = 1/ t, which implies that ϑ(G) ≤ t.

Next, we claim that
min{λmax (A) : A ∈ M(G)} ≤ min{t : (t, Y ) satisfies (9.10)}.

(9.13)

Indeed, if (t, Y ) is an optimum solution of (9.10), then A = tI − Y is a matrix satisfying
the conditions in Proposition 9.2.7, and λmax (A) ≤ t.
Our next step is:
X

min{t : (t, Y ) satisfies (9.10)} = max{

Zij : Z satisfies (9.11)}.

(9.14)

i,j∈V

Indeed, the second program is the dual of the first. The first program has a strictly feasible
solution (just choose t large enough), so by the Duality Theorem of semidefinite programming,
the two programs have the same objective value.
Now we get to orthonormal representations of the complementary graph.
nX
o
X
max{
Zij : Z satisfies (9.11)} ≤ max
(dT vi )2 : (vi , d) ONR of G .
i,j∈V

i∈V

(9.15)

9.2. SMALLEST CONE AND THE THETA FUNCTION

131

Indeed, let Z be an optimum solution of (9.11) with objective function value t. We can write
Z is a Gram matrix: Zij = ziT zj where zi ∈ Rk for some k ≥ 1. By the properties of Z, the
vectors zi form an orthogonal representation of G. We have
³X ´T ³X ´ ¯X ¯2
X
¯
¯
t=
ziT zj =
zi
zi = ¯
zi ¯ ,
i,j

i

i

(9.16)

i

and
1 = tr(Z) =

X

ziT zi =

i

X

|zi |2 .

i

Let us rescale the vectors zi to get the orthonormal representation vi = zi /|zi | (if zi = 0 then
√
P
we take a unit vector orthogonal to everything else as vi ). We define d = ( i zi )/ t (which
is a unit vector by (9.16)). Using Chauchy–Schwarz,
³X
´³X
´
X
(dT vi )2 =
|zi |2
(dT vi )2
i

i

≥

³X

i

|zi |dT vi

i

´2

=

³X

dT z i

´2

i

³ X ´2
= dT
zi = t.
i

This proves that the maximum of the right hand side of (9.15) is at least t.
The last step is to prove:
nX
o
max
(dT vi )2 : (vi , d) ONR of G ≤ ϑ(G).

(9.17)

i∈V

Using the definition of ϑ, it suffices to prove that if (ui : i ∈ V ) is an orthonormal representation of G in Rn with handle c, and (vi : i ∈ V ) is an orthonormal representation of G in
Rm with handle d, then
X
1
(dT vi )2 ≤ max T 2 .
(9.18)
i∈V (c ui )
i∈V

By the same computation as in the proof of Lemma 9.2.4, we get that the vectors ui ◦ vi
(i ∈ V ) are mutually orthogonal unit vectors, and hence
X
X¡
¢2
(cT ui )2 (dT vj )2 =
(c ◦ d)T (ui ◦ vi ) ≤ 1.
i

i

On the other hand,
X
X
(cT ui )2 (dT vj )2 ≥ min(cT ui )2
(dT vj )2 ,
i

which implies that
X
(dT vj )2 ≥
i

This proves (9.18).

i

i

1
1
= max T 2 .
i
mini (cT ui )2
(c ui )
¤

132

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Exercise 9.5 Prove that ϑ(G) is the maximum of the largest eigenvalues of matrices (viT vj ),
taken over all orthonormal representations (vi ) of G.
Exercise 9.6 (Alon and Kahale) Let G be a graph, v ∈ V , and let H be obtained from
G by removing v and all its neighbors. Prove that
ϑ(G) ≤ 1 +

9.2.4

p

|V (H)|ϑ(H).

More properties

Perhaps the most important consequence of the formulas proved in the preceding section is
that the value of ϑ(G) is polynomial time computable [80]. More precisely,
Theorem 9.2.10 There is a polynomial time algorithm that computes, for every graph G
and every ε > 0, a real number t such that
|ϑ(G) − t| < ε.
The significance of this fact is underlined if we combine it with Theorem 9.2.3: The two
important graph parameters α(G) and χ(G) are both NP-hard, but they have a polynomial
time computable quantity sandwiched between them.
Algorithms proving this theorem can be based on almost any of our formulas for ϑ. The
simplest is to refer to Proposition 9.2.8, and the polynomial time solvability of semidefinite
programs (see Section 13.8.3 in the Appendix).
We can write Proposition 9.2.9 as
min max

1

(ui ),c i∈V (cT ui )2

= max

(vi ),d

X

(dT vi )2

(9.19)

i∈V

(where (ui ) ranges through orthonormal representations of G, (vi ), through orthonormal
representations of G, and c, d are unit vectors in the corresponding euclidean spaces). From
the fact that equality holds here, it follows that equality holds in the arguments above. In
particular, we obtain the following corollaries:
Proposition 9.2.11 Every graph G has an orthonormal representation (ui ) with handle c
such that for every node i,
1
cT ui = p
.
ϑ(G)
Proof. The representation constructed in the proof of (9.12) must be optimal with t = ϑ(G),
and it has the property in the Corollary.

¤

9.2. SMALLEST CONE AND THE THETA FUNCTION

133

This corollary can be used to give another definition of ϑ, discovered independently by
Karger, Motwani and Sudan. Consider the vectors
s
´
ϑ(G) ³
1
vi =
ui − p
c .
ϑ(G) − 1
ϑ(G)
It is easy to check that these are unit vectors and satisfy
viT vj =

−1
ϑ(G) − 1

for every pair i, j of nonadjacent nodes. This computation can be reversed, and we get
Proposition 9.2.12 For every graph G = (V, E), ϑ(G) can be defined as the smallest real
number t ≥ 0 for which there exists a representation i 7→ vi (i ∈ V ) such that |vi | = 1 and
viT vj = −1/(t − 1) for all ij ∈ E.
For optimal orthogonal representations in the dual definition of ϑ(G) we cannot claim
such a nice “homogeneity” as in Corollary 9.2.11, but we get something from the symmetries
of the graph.
Proposition 9.2.13 Every graph G has an orthonormal representation (ui , c) in Rn with
p
cT ui = 1/ ϑ(G), and its complement has an orthonormal representation (vi , d) in Rn with
P T 2
i (d vi ) = ϑ(G), such that every automorphism of G lifts to an orthogonal transformation
of Rn that leaves both representations invariant.
Proof. We give the proof for the orthonormal representation of the complement. The set
of optimum solutions of the semidefinite program (9.11) form a bounded convex set, which
is invariant under the transformations Z 7→ P ZP , where P is the permutation matrix of an
automorphism of G. The center of gravity of this convex set is a matrix Z which is fixed by
these transformations, i.e., it satisfies P ZP = Z for all automorphisms P . The construction
of an orthonormal representation of G in the proof of (9.15) can be done in a canonical way
(e.g., choosing the rows of Z 1/2 as the vectors zi ), and so the obtained optimal orthonormal
representation will be invariant under the automorphism group of G (acting as permutation
matrices).

¤

Corollary 9.2.14 If G has a node-transitive automorphism group, then
ϑ(G)ϑ(G) = |V |.
Proof. It follows from Proposition 9.2.13 that G has an orthonormal representation (vi , d)
P
in Rn such that i (dT vi )2 = ϑ(G), and dT vi is independent of i. So (dT vi )2 = ϑ(G)/|V | for

134

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

all nodes i, and
ϑ(G) ≤ max
i

1
|V |
.
=
(dT vi )2
ϑ(G)

Since we already know the reverse inequality (9.2.6), this proves the Corollary.

¤

Corollary 9.2.15 If G is a self-complementary graph with a node-transitive automorphism
group, then
Θ(G) = ϑ(G) =

p

|V |.

Proof. The diagonal in G £ G is independent, so α(G £ G) = α(G £ G) ≥ |V |, and hence
p
p
Θ(G) ≥ |V |. On the other hand, ϑ(G) ≤ |V | follows by Corollary 9.2.14.
¤
An example to which this corollary applies is any Paley graph: for a prime p ≡ 1 (mod 4),
we take the {0, 1, . . . , p−1} as nodes, and connect two of them iff their difference is a quadratic
residue. Thus we get an infinite family for which the Shannon capacity is non-trivial (i.e.,
Θ > α), and can be determined exactly.
The Paley graphs are quite similar to random graphs, and indeed, for random graphs ϑ
behaves similarly (Juhász [108]). (It is not known however how large the Shannon capacity
of a random graph is.)
Theorem 9.2.16 If G is a random graph on n nodes with edge density 1/2, then
√
ϑ(G) < 2 n with probability 1 + o(1).

√

n <

As a further application of the duality established in Section 9.2.3, we prove:
Proposition 9.2.17 For any two graphs G and H,
ϑ(G £ H) = ϑ(G)ϑ(H).
Proof. Let (vi , d) be an orthonormal representation of G which is optimal in the sense
P
that i (dT vi )2 = ϑ(G), and let (wj , e) be an orthonormal representation of H such that
P T
2
i (e wj ) = ϑ(H). It is easy to check that the vectors vi ◦ wj form an orthonormal representation of G £ H, and so using handle d ◦ e we get
ϑ(G £ H) ≥

X¡
i,j

¢2 X T 2 T
(d ◦ e)T (vi ◦ wj ) =
(d vi ) (e wj )2 = ϑ(G)ϑ(H).
i,j

We already know the reverse inequality, which completes the proof.

¤

The fractional chromatic number χ∗ (G) is defined as the least t for which there exists a
family (Aj : j = 1, . . . , p) of stable sets in G, and nonnegative weights (τj : j = 1, . . . , p)

9.2. SMALLEST CONE AND THE THETA FUNCTION
such that

135

P
P
{τj : Aj 3 i} ≥ 1 for all i ∈ V and j τj = t. Note that the definition χ∗

can be considered as a linear program. By linear programming duality, χ∗ (G) is equal to the
P
largest s for which there exist weights (σi : i ∈ V ) such that i∈A σi ≤ 1 for every stable
P
set A and i σi = s.
Clearly ω(G) ≤ χ∗ (G) ≤ χ(G). The following is easy to prove:
Proposition 9.2.18 ϑ(G) ≤ χ∗ (G).
Exercise 9.7 (a) Show that any stable set S provides a feasible solution of (9.11). (b) Show
that any k-coloring of G provides a feasible solution of (9.10). (c) Give a new proof of the
Sandwich Theorem 9.2.3 based on (a) and (b).
Exercise 9.8 Prove Proposition 9.2.18.

9.2.5

How good is ϑ as an approximation?

How good an approximation does ϑ provide for α? Unfortunately, it can be quite bad.
First, consider the case when α is very small. Koniagin [116] constructed a graph that has
α(G) = 2 and ϑ(G) = Ω(n1/3 ). This is the largest ϑ(G) can be; in fact, Alon and Kahale
[11], improving results of Kashin and Koniagin [111], proved that there is a constant c such
that
ϑ(G) ≤ 9n1−2/(α(G)+1) .

(9.20)

Once α is unbounded, this inequality does not say much, and in fact very little is true.
Feige [71] showed that there are graphs for which α(G) = no(1) and ϑ(G) = n1−o(1) ; in other
words, ϑ/α can be larger than n1−ε for every ε > 0. (The existence of such graphs also
follows from the results of Håstad [89] showing that it is NP-hard to determine α(G) with a
relative error less than n1−ε , where n = |V |.) By results of Szegedy [192], this also implies
that ϑ(G) does not approximate the chromatic number within a factor of n1−ε .
Let us consider the other end of the scale, when ϑ(G) is small. The following theorem
was proved by Karger, Motwani and Sudan [110]:
Theorem 9.2.19 For every graph G,
α(G) ≥

n3/(ϑ(G)+1)
.
3 ln n

Note that we have ϑ(G) ≥ n/ϑ(G) by Corollary 9.2.6.
Proof.

Let t = ϑ(G). The case when the maximum degree ∆ of the graph satisfies ∆ >

t/(t+1)

can be settled by a simple induction: Let v be a node with degree ∆, and let G0 be the

n

136

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

subgraph induced by the neighbors v. It is easy to see that ϑ(G0 ) ≤ t−1, and so (by induction
√
√
on n) we can find in G0 a stable set of size at least ∆3/t /(2 ln ∆) > n3/(t+1) /(2 ln n).
So suppose that ∆ ≤ nt/(t+1) . By Corollary 9.2.12, there are unit vectors ui ∈ Rd for
some d such that uT
i uj = −1/(t − 1) whenever ij ∈ E. The case t = 2 is easy (cf. Exercise
9.1), so we assume that t > 2.
Let w be a random point in Rd whose coordinates are independent standard Gaussian
random variables. Fix an s > 0, and consider the set S = {i : wT ui ≥ c}. The inner
product wT ui has standard Gaussian distribution, and hence the probability that a given
node belongs to S is
Z ∞
2
1
Q(s) = √
e−x /2 dx.
2π s
Thus the expected size of S is E|S| = Q(s)n. Furthermore, let ij ∈ E, then |ui + uj |2 =
(2t−4)/(t−1), and so the probability that both nodes ui and uj belong to S can be estimated
as follows:
³r 2t − 2 ´
s .
P(wT ui ≥ s, wT uj ≥ s) ≤ P(wT (ui + uj ) ≥ 2s) = Q
t−2
Hence the expected number of edges of E spanned by S satisfies
³r 2t − 2 ´
³r 2t − 2 ´ n∆
E|E[S]| ≤ Q
s |E| ≤ Q
s
.
t−2
t−2
2
We can delete at most |E[S]| nodes from S to get a clique K. The expected size of K is at
least
³r 2t − 2 ´ n∆
E|K| ≥ E(|S| − |E[S]|) = E|S| − E|E[S]| ≥ Q(s)n − Q
s
.
t−2
2
Using also the bound on ∆, it follows that
³r 2t − 2 ´ n2t+1 t + 1
ω(G) ≥ Q(s)n − Q
s
.
t−2
2
We choose s so that it maximizes the right hand side. By elementary computation we get
that
r
2t − 4
ln n
s=
t+1
is an approximately optimal choice, and it gives
n2/t
.
α(G) ≥ √
3 ln n
¤

9.2. SMALLEST CONE AND THE THETA FUNCTION

137

The previous theorem has an important application to a coloring problem. Suppose that
somebody gives a graph and guarantees that the graph is 3-colorable, without telling us its
3-coloring. Can we find this 3-coloring? (This may sound artificial, but this kind of situation
does arise in cryptography and other data security applications; one can think of the hidden
3-coloring as a “watermark” that can be verified if we know where to look.)
It is easy to argue that knowing that the graph is 3-colorable does not help: it is still
NP-hard to find the 3-coloration. But suppose that we would be satisfied with finding a
4-coloration, or 5-coloration, or (log n)-coloration; is this easier? It is known that to find a
4-coloration is still NP-hard, but little is known above this. Improving earlier results, Karger,
Motwani and Sudan [110] gave a polynomial time algorithm that, given a 3-colorable graph,
computes a coloring with O(n1/4 (ln n)3/2 ) colors. More recently, this was improved by Blum
and Karger [33] to O(n3/14 ).
The algorithm of Karger, Motwani and Sudan starts with computing ϑ(G), which is at
√
most 3 by Theorem 9.2.3. Using Theorem 9.2.19, they find a stable set of size Ω(n3/4 / ln n).
Deleting this set from G and iterating, they get a coloring of G with O(n1/4 (ln n)3/2 ) colors.

9.2.6

Perfect graphs

Recall that for a graph G, we denote by ω(G) the size of its largest clique, and by χ(G),
its chromatic number. A graph G is called perfect, if for every induced subgraph G0 of G,
we have ω(G0 ) = χ(G0 ). To be perfect is a rather strong structural property; nevertheless,
many interesting classes of graphs are perfect (bipartite graphs, their complements and their
linegraphs; interval graphs; comparability and incomparability graphs of posets; chordal
graphs). We do not discuss perfect graphs, only to the extent needed to show their connection
with orthogonal representations.
The following deep characterization perfect graphs was conjectured by Berge in 1961 and
proved by Chudnovsky, Robertson, Seymour and Thomas [41].
Theorem 9.2.20 (The Strong Perfect Graph Theorem [41]) A graph is perfect if and
only if neither the graph nor its complement contains a chordless odd cycle longer than 3.
As a corollary we can formulate the “The Weak Perfect Graph Theorem” proved much
earlier [128]:
Proposition 9.2.21 The complement of a perfect graph is perfect.
From this corollary it follows that in the definition of perfect graphs we could replace
the equation ω(G0 ) = χ(G0 ) by α(G0 ) = χ(G0 ). In particular, if G is a perfect graph, then
α(G) = χ(G), and so by Theorem 9.2.3,
Proposition 9.2.22 For every perfect graph G, Θ(G) = ϑ(G) = α(G) = χ(G).

138

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Corollary 9.2.23 The independence number and the chromatic number of a perfect graph
are polynomial time computable.

9.2.7

The TSTAB body and weighted θ-function

For every orthonormal representation (vi : i ∈ V ) of G, we consider the linear constraint
X

2
(eT
1 vi ) xi ≤ 1.

(9.21)

i∈V

It is easy to see that these inequalities are valid for STAB(G); we call them orthogonality
constraints. The solution set of non-negativity and orthogonality constraints is denoted by
TSTAB(G). It is clear that TSTAB is a closed, convex set. The incidence vector of any
stable set A satisfies (9.21). Indeed, it then says that
X
2
(eT
1 vi ) ≤ 1.
i∈A

Since the vi (i ∈ A) are mutually orthogonal, the left hand side is just the squared length
projection of e1 onto the subspace spanned by these ei , and the length of this projection is
at most the length of e1 , which is 1.
Furthermore, every clique constraint is an orthogonality constraint. Indeed,
X

xi ≤ 1

i∈B

is the constraint derived from the orthogonal representation
(
e1 , i ∈ A,
i 7→
ei , otherwise.
Hence we have
STAB(G) ⊆ TSTAB(G) ⊆ QSTAB(G)
for every graph G.
There is a dual characterization of TSTAB [82]. For every orthonormal representation
2
V
(ui : i ∈ V ), consider the vector x[u] = (eT
1 ui ) : i ∈ V ) ∈ R .
Theorem 9.2.24 TSTAB(G) = {x[u] : u is an orthonormal representation of G}.
Proof. This can be derived from semidefinite duality.

¤

Not every orthogonality constraint is a clique constraint; in fact, the number of essential
orthogonality constraints is infinite in general:

9.3. MINIMUM DIMENSION

139

Theorem 9.2.25 TSTAB(G) is polyhedral if and only if the graph is perfect. In this case
TSTAB = STAB = QSTAB.
While TSTAB is a rather complicated set, in many respects it behaves much better than,
say, STAB. For example, it has a very nice connection with graph complementation:
Theorem 9.2.26 TSTAB(G) is the antiblocker of TSTAB(G).
Maximizing a linear function over STAB(G) or QSTAB(G) is NP-hard; but, surprisingly,
TSTAB behaves much better:
Theorem 9.2.27 Every linear objective function can be maximized over TSTAB(G) (with
arbitrarily small error) in polynomial time.
The maximum of

P
i

xi over TSTAB(G) is the familiar function ϑ(G).

See [83, 114] for more detail.

9.3

Minimum dimension

Perhaps the most natural way to be “economic” in constructing an orthogonal representation
is to minimize the dimension. We can say only a little about the minimum dimension of all
orthogonal representations, but we get interesting results if we impose some “non-degeneracy”
conditions. We will study three nondegeneracy conditions: general position, faithfulness, and
the strong Arnold property.

9.3.1

Minimum dimension with no restrictions

Let dmin (G) denote the minimum dimension in which D has an orthonormal representation.
The following are easy to prove, using Theorem 2.4.3 for (b).
Proposition 9.3.1 (a) ϑ(G) ≤ dmin (G).
(b)

1
2

log χ(G) ≤ dmin (G).

(c) dmin (G £ H) ≤ dmin (G)dmin (H).
This Proposition shows that we can use dmin (G) as an upper bound on Θ(G); however, it
would not be better than ϑ(G). On the other hand, if we consider orthogonal representations
over fields of finite characteristic, the dimension may be a better bound on the Shannon
capacity than ϑ [87, 9].

140

9.3.2

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

General position orthogonal representations

The first non-degeneracy condition we study is general position: we assume that any d of the
representing vectors in Rd are linearly independent.
A result of Lovász, Saks and Schrijver [137] (see [13] for an application in quantum
computing) finds an exact condition for this type of geometric representability.
Theorem 9.3.2 A graph with n nodes has a general position orthogonal representation in
Rd if and only if it is (n − d)-connected.
The condition that the given set of representing vectors is in general position is not
easy to check (it is NP-hard). A weaker, but very useful condition will be that the vectors
representing the nodes non-adjacent to any node v are linearly independent. We say that
such a representation is in locally general position. To exclude some trivial complications, we
assume in this case that all the representing vectors are nonzero.
The following Theorem is due to Lovász, Saks and Schrijver [137].
Theorem 9.3.3 If G is a graph with n nodes then the following are equivalent:
(i) G is (n − d)-connected;
(ii) G has a general position orthogonal representation in Rd ;
(iii) G has a locally general position orthogonal representation in Rd .
Proof. We start with proving (iii)⇒(i), which illustrates the connection with connectivity.
Suppose that G is not (n − d)-connected, then there is a partition V = V0 ∪ V1 ∪ V2 such that
|V0 | ≤ n − d − 1 and no edge connects V1 and V2 .
Suppose that G has an orthonormal representation in Rd as in (iii). Since the nodes in
V1 are non-adjacent to any node of V2 , the vectors representing V1 are linearly independent.
Similarly, the vectors representing V2 are linearly independent. Since the vectors representing
V1 and V2 are mutually orthogonal, all vectors representing V1 ∪ V2 are linearly independent.
But |V1 ∪ V2 | = n − |V0 | ≥ d + 1, a contradiction.
The implication (ii)⇒(iii) is trivial.
The difficult part is (i)⇒(ii), i.e., the construction of a general position orthogonal (or
orthonormal) representation for (n − d)-connected graphs. As a matter of fact, the following
construction is almost trivial, the difficulty is the proof of its validity.
Let σ = (v1 , ..., vn ) be any ordering of the nodes of G = (V, E). Let us choose
f (v1 ), f (v2 ), . . . consecutively as follows. f (v1 ) is any vector of unit length. Suppose that
f (vi ) (1 ≤ i ≤ j) are already chosen. Then we choose f (vj + 1) randomly, subject to the
constraints that it has to be orthogonal to certain previous vectors f (vi ). These orthogonality constraints restrict f (vj+1 ) to a linear subspace Lj+1 , and we choose it from the uniform

9.3. MINIMUM DIMENSION

141

distribution over the unit sphere of Lj+1 . Note that if G is (n − d)-connected then every
node of it has degree at least n − d and hence
dim L ≥ d − #{i : i ≤ j, vi vj ∈
/ E} ≥ d − (d − 1) = 1,
and so f (vj+1 ) can always be chosen.
This way we get a random mapping f : V → Rd , i.e., a probability distribution over
(S d )V , which we denote by µσ (it may depend on the initial ordering of the nodes). We
call f the random sequential orthogonal representation of G (associated with the ordering
(v1 , ..., vn )). Theorem 9.3.3 will then follow from Theorem 9.3.4 below.

¤

Theorem 9.3.4 Let G be any graph and fix any ordering of its nodes. Let f be the random
sequential orthogonal representation of G. Then
(a) If G is not (n − d)-connected then f is not in locally general position;
(b) If G is (n − d)-connected then with probability 1, f is in general position.
As we pointed out, the distribution of a random sequential orthogonal representation may
depend on the initial ordering. The key to the proof will be that this dependence is not too
strong. We say that two probability measures µ and ν on the same probability space S are
mutually absolute continuous, if for any measurable subset A of S, µ(A) = 0 if and only if
ν(A) = 0.
Lemma 9.3.5 For any two orderings σ and τ of V , the distributions µσ and µτ are mutually
absolute continuous.
Before proving this lemma, we have to state and prove a simple technical fact. We say that
two linear subspaces A, B ⊆ Rd are orthogonal, if every a ∈ A is orthogonal to every b ∈ B.
A different relation is that A, B are weakly orthogonal, which means that the orthogonal
projection of A onto B is A ∩ B. This is equivalent to saying that A ∩ B and A ∩ B ⊥ generate
A. It is easy to see that this weak orthogonality relation is symmetric. If A and B are weakly
orthogonal, then A ∩ B, A ∩ B ⊥ and A⊥ ∩ B are orthogonal.
Lemma 9.3.6 Let A and B be weakly orthogonal linear subspaces of Rd with dim(A∩B) ≥ 2.
Select a unit vector a1 uniformly from A, and then select a unit vector b1 uniformly from
B ∩ a⊥
1 . Also, select a unit vector b2 uniformly from B, and then select a unit vector a2
uniformly from A ∩ b⊥
2 . Then the distributions of (a1 , b1 ) and (a2 , b2 ) are mutually absolute
continuous.
Proof. The special case when A ⊆ B is trivial. Suppose that A 6⊆ B and also B 6⊆ A.
Observe that if Y and Z are orthogonal spaces, a unit vector a in Y ⊕ Z can be written
uniquely in the form y cos θ + z sin θ where y is a unit vector in Y , z is a unit vector in Z,

142

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

and θ ∈ [0, π/2]. Uniform selection of a means independent uniform selection of y and z, and
an independent selection of θ from a distribution ζs,t that depends only on s = dim(Y ) and
t = dim(Z). If t = 0 then θ = 0 with probability 1. If s, t ≥ 1, the only thing we need about
ζs,t is that it is mutually absolute continuous with respect to the uniform distribution on the
interval [0, π/2].
So (a1 , b1 ) can be generated through five independent choices: a uniform unit vector x1
from A ∩ B ⊥ , a uniform unit vector z1 from A⊥ ∩ B, a pair of orthogonal unit vectors (y1 , y2 )
selected from A ∩ B (uniformly over all such pairs: this is possible since dim(A ∩ B) ≥ 2),
and two numbers θ1 selected according to ζc0 ,c1 and θ2 is selected according to ζc0 −1,c2 . The
distribution of (a2 , b2 ) is described similarly except that θ2 is selected according to ζc0 ,c2 and
θ1 is selected according to ζc0 −1,c1 .
Since c0 ≥ 2, ζc0 ,c2 and ζc0 −1,c2 are mutually absolute continuous and ζc0 ,c1 and ζc0 −1,c1
are mutually absolute continuous, from which we deduce that the distributions of (a1 , b1 )
and (a2 , b2 ) are mutually absolute continuous.
¤
Proof of Lemma 9.3.5. It suffices to prove that if τ is obtained from σ by swapping vσ(j)
and vσ(j+1) (1 ≤ j ≤ n − 1) then µ(σ) and µ(τ ) are mutually absolute continuous. We prove
this by induction on j.
Fix j ≥ 1. We may assume, without loss of generality, that σ = (v1 , . . . , vn ). For
1 ≤ i ≤ n, let Vi = {v1 , . . . , vi }.
We want to compare the distributions of f (v1 ), . . . , f (vn ) and g(v1 ), . . . , g(vn ). It suffices
to prove that the distributions of f (v1 ), . . . , f (vj+1 ) and g(v1 ), . . . , g(vj+1 ) are mutually absolute continuous, since conditioned on any given assignment of vectors to v1 , . . . , vj+1 , the
distributions µσ and µτ are identical.
Also, note that the distributions of f (v1 ), . . . , f (vj−1 ) and g(v1 ), . . . , g(vj−1 ) are identical.
Let x1 , . . . , xj−1 any selection of vectors for the first j − 1 nodes. It suffices to show that
the distributions of f (vj ), f (vj+1 ) and g(vj ), g(vj+1 ), conditioned on f (vi ) = g(vi ) = xi
(i = 1, . . . , j − 1), are mutually absolute continuous.
Case 1. vj and vj+1 are adjacent. When conditioned on f (v1 ), . . . , f (vj−1 ), the vectors
f (vj ) and f (vj+1 ) are independent, so it does not mater in which order they are selected.
Case 2. vj and vj+1 are not adjacent, but they are joined by a path that lies entirely in
Vj+1 . Let P be a shortest such path and t be its length (number of edges), so 2 ≤ t ≤ j. For
fixed j, we argue by induction on t. Let vi be any internal node of P . We transform σ to τ
by the following steps:
(1) Swap vi and vj to get σ 1 . Since this can be obtained by successive adjacent swaps
among the first j elements, µσ and µσ1 are mutually absolute continuous by the induction
hypothesis on j.

9.3. MINIMUM DIMENSION

143

(2) Swap vi and vj+1 , to get σ 2 . By the induction hypothesis on t (or by Case 1), µσ2
and µσ1 are mutually absolute continuous.
(3) Swap vj+1 and vj , to get σ 3 . As in (1), µσ3 and µσ2 are mutually absolute continuous.
(4) Swap vj and vi , to get σ 4 . As in (2), µσ4 and µσ3 are mutually absolute continuous.
(5) Swap vj+1 and vi , to get τ . As in (1), µτ and µσ4 are mutually absolute continuous.
Thus µσ and µτ are mutually absolute continuous.
Case 3. There is no path connecting vj to vj+1 in Vj+1 . Let U0 (resp. U1 ) denote the set
of nodes of Vj−1 that are adjacent to vj (resp. vj+1 ). Then Vj−1 has a partition W0 ∪ W1 so
that there is no edge between W0 and W1 , U0 ⊆ W0 and U1 ⊆ W1 . Furthermore, it follows
that V \ Vj+1 is a cutset, whence |V \ Vj+1 | ≥ n − d and so j ≤ d − 1.
For S ⊆ Vj−1 , let L(S) denote the linear subspace of Rd generated by the vectors xi ,
i ∈ S. Let Li = L(Vj−1 \ Ui ). Then the condition on f (vj ) is that it must be in L⊥
0 , and
⊥
⊥
f (vj+1 ) must be in L1 ∩ f (vj ) .
⊥
We claim that L⊥
0 and L1 are weakly orthogonal. It suffices to check that L0 and L1 are
weakly orthogonal. But this is easy, since a vector x ∈ L0 can be written as x0 + x00 , where
x0 ∈ L(W1 ) and x00 ∈ L(W0 \ U0 . The projection of this to L1 is obtained by keeping x00 and
projecting x0 onto L(W1 \ U1 , which is in L0 ∩ L1 .
⊥
⊥
⊥
⊥
Furthermore, we claim that dim(L⊥
0 ∩ L1 ) ≥ 2. Indeed, L0 ∩ L1 = (L0 + L1 ) , and so
⊥
dim(L⊥
0 ∩ L1 ) = d − dim(L0 + L1 ) ≥ d − (j − 1) ≥ 2.

So Lemma 9.3.6 applies, which completes the proof.

9.3.3

¤

Faithful orthogonal representations

An orthogonal representation is faithful if different nodes are represented by non-parallel
vectors and adjacent nodes are represented by non-orthogonal vectors.
We do not know how to determine the minimum dimension of a faithful orthogonal representation. It was proved by Maehara (1987) that if the maximum degree of the complementary graph G of a graph G is D, then G has a faithful orthogonal representation in D3
dimensions. He conjectured that the bound on the dimension can be improved to D + 1. In
other words,
Conjecture 9.3.7 If every node of G has degree at least n − d (where n is the number of
nodes), then G has a faithful orthogonal representation in Rd .
We derive from the results in Section 9.3.2 that Maehara’s conjecture is true if we
strengthen its assumption by requiring that G is (n − D − 1)-connected.

144

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Theorem 9.3.8 Every (n − d)-connected graph on n nodes has a faithful general position
orthogonal representation in Rd .
Proof.
It suffices to show that in a random sequential orthogonal representation, the
probability of the event that two nodes are represented by parallel vectors, or two adjacent
nodes are represented by orthogonal vectors, is 0. By the Lemma 9.3.5, it suffices to prove
this for the representation obtained from an ordering starting with these two nodes. But
then the assertion is obvious.
¤
This implies the following result of Rödl (1987), improving the bound of Maehara:
Corollary 9.3.9 If the maximum degree of the complementary graph G of a graph G is D,
then G has a faithful orthogonal representation in 2D + 1 dimensions.
Indeed, a graph with minimum degree n − D − 1 is at least (n − 2D)-connected.

9.3.4

Orthogonal representations with the Strong Arnold Property

The Strong Arnold Property. Consider an orthogonal representation i 7→ vi ∈ Rd of a
graph G. We can view this as a point in the Rd|V | , satisfying the quadratic equations
viT vj = 0

(ij ∈ E).

(9.22)

Each of these equation defines a hypersurface in Rd|V | . We say that the orthogonal representation i 7→ vi has the Strong Arnold Property if the surfaces (9.22) intersect transversally
at this point. This means that their normal vectors are linearly independent.
This can be rephrased in more explicit terms as follows. For each nonadjacent pair
i, j ∈ V , form the d × V matrix V i,j in which the i-th column is vj , the j-th column is vi ,
and the rest if 0. Then the Strong Arnold Property says that the matrices V i,j are linearly
independent.
Another way of saying this is that there is no symmetric V × V matrix X 6= 0 such that
Xij = 0 if i = j or ij ∈ E, and
X
Xij vj = 0
(9.23)
j

for every node i.
Algebraic width: definition. Colin de Verdière [45] introduced an interesting graph
invariant related to connectivity. Let d be the smallest dimension in which G has a faithful
orthogonal representation with the Strong Arnold Property, and define a(G) = n − d. We
call a(G) the algebraic width of the graph.
This can be rephrased in terms of matrices, making it more similar to the other Colin de
Verdiére number. We consider a matrix N ∈ RV ×V with the following properties:

9.3. MINIMUM DIMENSION
(
=0
(N1) Nij
6= 0

145

if ij ∈
/ E, i 6= j;,
if ij ∈ E.

(N2) N is positive semidefinite;
(N3) [Strong Arnold Property] If X is a symmetric n × n matrix such that Xij = 0
whenever i = j or ij ∈ E, and N X = 0, then X = 0.
Lemma 9.3.10 The algebraic width of a graph G is the maximum corank of a matrix with
properties (N1)–(N3).
The proof is quite straightforward, and is left to the reader.
Example 9.3.11 (Complete graphs) The Strong Arnold Property is void, and every representation is orthogonal, so a(Kn ) = n − 1.
Example 9.3.12 (Paths)
Basic properties. The main advantage of this nondegeneracy condition is that it implies
the following.
Lemma 9.3.13 The graph parameter a(G) is minor-monotone.
The parameter has other nice properties, of which the following will be relevant:
Lemma 9.3.14 Let G be a graph and B a complete subgraph. Let G1 , . . . , Gk be the connected components of G \ B, and let Hi be the subgraph induced by V (Gi ) ∪ B. Then
a(G) = max a(Hi ).
i

Algebraic width, tree-width and connectivity. Tree-width is a parameter related to
connectivity, introduced by Robertson and Seymour [168] as an important element in their
graph minor theory (see [136] for a survey). Colin de Verdière [45] defines the tree-width of
a graph G as the smallest r for which G is a minor of a Cartesian sum Kr ⊕ T , where T is
a tree. (This is not quite the same as the more standard notion of tree-width introduced by
Robertson and Seymour, but the difference is at most 1, as shown by van der Holst [96]).
The parameter tw(G) is minor-monotone.
The monotone connectivity κmon (G) of a graph G is defined as the maximum connectivity
of any minor of G. It is easy to see that κmon (G) ≤ tw(G). The algebraic width is sandwiched
between these two parameters:
Theorem 9.3.15 For every graph,
κmon (G) ≤ a(G) ≤ tw(G).

146

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

The upper bound was proved by Colin de Verdière [45], while the lower bound follows
easily from the results in Section 9.3.2.
Proof.
Since every general position orthogonal representation has the Strong Arnold
Property, we get by Theorem 9.3.8 that a(G) ≥ κ(G). Since a(G) is minor-monotone, this
implies that a(G) ≥ κmon .
To prove the second inequality, we use again the minor-monotonicity of a(G) to notice
that it suffices to prove that
a(Kr ⊕ T ) ≤ r

(9.24)

for any r ≥ 1 and tree T . Note that a(Kr ⊕ T ) ≥ a(Kr ) = r − 1 follows by the minor
monotonicity of r.
We use induction on |V (T )|. If T is a single node, then Kr ⊕ T is a complete r-graph and
the assertion is trivial.
The most complicated case is when T has two nodes. We have to prove then that K2 ⊕ T
has no orthogonal representation in Rr−1 with the Strong Arnold Property. Suppose that
i 7→ vi is such a representation.
Let {a1 , . . . , ar } and {b1 , . . . , br } be the two r-cliques corresponding to the two nodes of
T , where ai is adjacent to bi . For S ⊆ [r], define US = {vai : i ∈ S} and WS = {vbi : i ∈ S}.
If there is a subset S ⊆ [r] such that both US and W[r]\S are linearly independent, then
(since these sets are mutually orthogonal) we get
rk(US ∪ W[r]\S ) = rk(US ) + rk(W[r]\S ) = |US | + |W[r]\S | = r,
which is a contradiction since US ∪ W[r]\S ⊆ Rd−1 .
So we may assume that no such subset S exists. Then by the Matroid Sum Theorem,
there is a subset T ⊆ [q] such that rk(UT ) + rk(WT ) < |T |. We may assume that T = [r],
since we can delete the nodes aj and bj with j ∈
/ T . We may also assume (by scaling the
vai ) that there is a vector y ∈ lin(UT ) such that y T vai = 1 nd similarly there is a vector
z ∈ lin(WT ) such that z T vbi = 1.
Consider the matrices Vai ,bj defined above, for i, j ∈ T . These matrices must be linearly
independent. On the other hand, the r columns corresponding to the nodes ai are all from
a space with dimension rk(WT ), and the other r columns are from a space with dimension
rk(UT ). So all these matrices belong to a linear space with dimension rrk(WT ) + rrk(UT ) ≤
r(r − 1). Since the number of matrices Vai ,bj (i, j ∈ T ) is exactly r(r − 1), the must span the
space. However, this is impossible, since the matrix Y = (z, . . . , z, −y, . . . , −y) also belongs
to this space and
Y · Vi,j = z T vbj − y T vai = 1 − 1 = 0.
This concludes the proof of the case |V (T )| = 2.

9.4. THE VARIETY OF ORTHOGONAL REPRESENTATIONS

147

Finally, if T has more than two nodes, then it has an internal node v, and the clique corresponding to this node separates the graph. So 9.24 follows by Lemma 9.3.14 and induction.
¤
Colin de Verdière conjectured that equality holds in the upper bound in Theorem 9.3.15.
This was proved by Van der Holst [96] and Kotlov [117] for a(G) ≤ 2, but the following
example of Kotlov shows that it is false in general.
Example 9.3.16 The k-cube Qk has a(Qk ) = O(2k/2 ) but tw(Qk ) = Θ(2k ).
It is not known whether tw(G) can be larger than a(G)2 .

9.4

The variety of orthogonal representations

Let G be a k-connected graph with n nodes and set d = n − k. Then we know that G has a
general position orthogonal representation in Rd . One may suspect that more is true: every
orthogonal representation in Rd is the limit of general position orthogonal representations,
i.e., the set ORd (G) of all orthogonal representations is the closure of the set GORd (G) of
general position orthogonal representations of G. The following example shows that this is
not true in general (but it can be proved under additional hypotheses about the graph G).
Example 9.4.1 Let Q denote the graph of the (ordinary) 3-dimensional cube, and let G =
Q. Note that Q is bipartite; let U = {u1 , u2 , u3 , u4 } and W = {w1 , w2 , w3 , w4 } be its color
classes. The indices are chosen so that ui is adjacent to vi in G.
Since G is 4-connected, it has a general position orthogonal representation in R4 . This is
in fact easy to construct. Choose any four linearly independent vectors x1 , . . . , x4 to represent
u1 , . . . , u4 , then the vector yi representing wi is uniquely determined (up to scaling) by the
condition that it has to be orthogonal to the three vectors xj , j 6= i.
There is another type of orthogonal representation in which the elements of U are represented by vectors x1 , . . . , x4 in a 2-dimensional subspace L of R4 and the elements of W are
represented by vectors y1 , . . . , y4 in the orthogonal complement L⊥ of L. Clearly, this is an
orthogonal representation for any choice of the representing vectors in these subspaces.
We claim that such a representation is the limit of general position orthogonal representations only if the cross ratio (x1 : x2 : x3 : x4 ) is equal to the cross ratio of (y1 : y2 : y3 : y4 ).
(The cross ratio (x1 : x2 : x3 : x4 ) of four vectors x1 , x2 , x3 , x4 in a 2-dimensional subspace
can be defined as follows: we write x3 = λ1 x1 + λ2 x2 and x4 = µ1 x1 + µ2 x2 , and take
(x1 : x2 : x3 : x4 ) = (λ1 µ2 )/(λ2 µ1 ). This number is invariant under linear transformations.)
To prove this claim, we consider any general position orthogonal representation g of G.
Let M be the linear subspace spanned by the vectors g(u1 ) and g(u2 ). Then its orthogonal
complement M ⊥ is spanned by g(v3 ) and g(v4 ). Let bi be the orthogonal projection of g(ui )

148

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

onto M (i = 1, 2, 3, 4) and ci , the orthogonal projection of g(vi ) onto M ⊥ (i = 1, 2, 3, 4). If
we show that (b1 : b2 : b3 : b4 ) = (c1 : c2 : c3 : c4 ), then the claim follows since bi → xi and
ci → yi .
The proof of (b1 : b2 : b3 : b4 ) = (c1 : c2 : c3 : c4 ) is an exercise in linear algebra (or
projective geometry) and is left to the reader.
One reason for asking the question whether GORd (G) is dense in ORd (G) is the following.
The set ORd (G) is an algebraic variety in Cd , and it is a natural question whether it is
irreducible. (A set A ⊆ CN is irreducible if whenever the product p · q of two polynomials
in N variables vanishes on A, then either p or q vanishes on A; equivalently, the polynomial
ideal {p : p vanishes on A} is a prime ideal.)
Let us begin with the question of irreducibility of the set GORd (G) of general position
orthogonal representations of G. This can be settled quite easily.
Theorem 9.4.2 Let G be any graph and d ≥ 1. Then GORd (G) is irreducible.
Proof. Let G have n nodes. We may assume that G is (n − d)-connected, else GORd (G)
is empty and the assertion is vacuously true.
First we show that there exist vectors xv = xv (X) ∈ R[x]d (v ∈ V ) whose entries are
multivariate polynomials with real coefficients in variables X1 , X2 , . . . (the number of these
variables does not matter) such that whenever u and v are non-adjacent then xu · xv is
identically 0, and such that every general position representation of G arises from x by
substitution for the variables Xi . We do this by induction on n.
Let v ∈ V . Suppose that the vectors of polynomials xu (X 0 ) of length d exist for all
u ∈ V − {v} satisfying the requirements above for the graph G − v (since G − v has n − 1
nodes and is (n − d − 1)-connected, this is indeed the right induction hyposthesis). Let
x0 = (xu : u ∈ V − v). Let u1 , . . . , um be the nodes in V − {v} non-adjacent to v; clearly
m ≤ d − 1. Let x1 , . . . , xd − 1 − m be vectors of length d composed of new variables and
let y be another new variable; X will consist of X 0 and these new variables. Consider the
d × (d − 1) matrix
F = (xu1 , . . . , xum , x1 , . . . , xd−1−m ),
and let pj be the determinant of the submatrix obtained by dropping the j-th row. Then we
define xv = y(p1 , . . . , pd )T .
It is obvious from the construction and elementary linear algebra that xv is orthogonal
to every vector xu for which u and v are non-adjacent. We show that every general position
orthogonal representation of G can be obtained from x by substitution. In fact, let f be
a general position orthogonal representation of G. Then f 0 = f |V −v is a general position
orthogonal representation of G − v in Rd , and hence by the induction hypothesis, f 0 can

9.4. THE VARIETY OF ORTHOGONAL REPRESENTATIONS

149

be obtained from x0 by substituting for the variables X 0 . The vectors f (u1 ), . . . , f (um )
are linearly independent and orthogonal to f (v); let a1 , . . . , ad−1−m be vectors completing
the system f (u1 ), . . . , f (um ) to a basis of f (v)⊥ . Substitute xi = ai . Then the vector
(p1 , . . . , pd )T will become a non-zero vector parallel to f (v) and hence y can be chosen so
that xv will be equal to f (v).
Note that from the fact that x(X) is in general position for some substitution it follows
that the set of substitutions for which it is in general position is everywhere dense.
From here the proof is quite easy. Let p and q be two polynomials such that p · q vanishes
on GORd (G). Then p(x(X)) · q(x(X)) vanishes on an everywhere dense set of substitutions
for X, and hence it vanishes identically. So either p(x(X)) or q(x(X)) vanishes identically;
say the first occurs. Since every general position orthogonal representation of G arises from
x by substitution, it follows that p vanishes on GORd (G).
¤
The proof in fact gives more. Let LGORd (G) denote the set of locally general position
orthogonal representations of G in Rd .
Corollary 9.4.3 Every locally general position orthogonal representation of G can be obtained from x by substitution. Hence LGORd (G) is irreducible, and GORd (G) is dense in
LGORd (G).
Now back to the (perhaps) more natural question of the irreducibility of ORd (G).
Proposition 9.4.4 If G is not (n − d)-connected, then ORd (G) is not irreducible.
Proof.
If G is not (n − d)-connected, then V has a partition V0 ∪ V1 ∪ V2 such that
|V0 | = n − d − 1 and there is no edge between V1 and V2 . Let dr = |Vr |, then d1 + d2 = d + 1.
Let i 7→ vi be any orthogonal representation of G, and let Ar be the Gram matrix of
the vectors {vi : i ∈ Vr }. As in the proof of Theorem 9.3.3, either {vi : i ∈ V1 } or
{vi : i ∈ V2 } must be linearly dependent, and hence det(A1 ) det(A2 ) = 0 for every orthogonal
representation in Rd .
On the other hand, det(A1 ) is not zero for all orthogonal representations. Indeed, selecting
mutually orthogonal unit vectors in Rd for {vi : i ∈ V1 } (which is possible as d1 ≤ d, and
vi = 0 for i ∈ V \ V1 , we get an orthogonal representation with det(A1 ) = 1. Similarly,
¤
det(A2 ) is not always zero on ORd (G).
In the other direction, theorem 2.1 implies the following.
Lemma 9.4.5 If GORd (G) is dense in ORd (G) then ORd (G) is irreducible.
We know that GORd (G) is nonempty if and only of G is (n − d)-connected. For d ≤ 3,
more is true:

150

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Theorem 9.4.6 Let G be an (n − d)-connected graph with n nodes.
(a) If d ≤ 3, then GORd (G) is everywhere dense in ORd (G).
(b) If d = 4, and the complement of G is connected and non-bipartite, then GORd (G) is
everywhere dense in ORd (G).
On the other hand, for d = 4, not every (n − d)-connected graph gives an irreducible
variety, but those graphs whose complement do. The general description of all graphs with
ORd (G) irreducible is open.
Let us say that an orthogonal representation of a graph G in Rd is special if it does not
belong to the topological closure of GORd (G). A graph G is d-critical if it has a special
orthogonal representation in Rd , but no induced subgraph of it does so. (Observe that if a
graph has no special representation in Rd , then neither do its induced subgraphs: a special
orthogonal representation of an induced subgraph can be extended with 0 vectors to a special
orthogonal representation of G.)
The following lemma shows that special orthogonal representations are just the opposite
of being in locally general position.
Lemma 9.4.7 For every special orthogonal representation of a d-critical graph in Rd the
non-neighbors of every node are linearly dependent.
Proof. Call a node good if its non-neighbors are represented by linearly independent vectors.
Let i 7→ vi be a special orthogonal representation of G that has a good node i. We construct a
general position orthogonal representation of G in an arbitrarily small neighborhood of v. Let
m = n − |N (i)| − 1 ≤ d − 1, and extend {vj : j ∈
/ N (i)} by arbitrary vectors a1 , . . . , ad−1−m
d
to a basis B of R . Let, say,
det(B) = 1.
Now v 0 , obtained by restricting v to V (G − i), is an orthogonal representation of G − i,
and hence by the criticality of G, there exists a general position orthogonal representation
u of G − i in Rd in an arbitrarily small neighborhood of v 0 . We extend u to an orthogonal
representation u∗ of G as follows. Clearly if u is close enough to v 0 , then the vectors {uj : j ∈
V \N (i)\{i}}∪{a1 , . . . , ad−1−m } are linearly independent and hence they uniquely determine
a vector u∗i orthogonal to all of them such that the basis {uj : j ∈ N (i)}∪{a1 , . . . , ad−1−m , u∗i }
has determinant 1.
It is obvious that u∗ is an orthogonal representation of G and that if u is close enough
to v 0 , then u∗ is arbitrarily close to v. Unfortunately, it does not follow in general that this
extended u∗ is in general position; but at least every ”good” node remains “good” (if v 0 and
u are close enough). Moreover, we know that any d vectors representing nodes different from
i are linearly independent; in particular, every node adjacent to i is now “good”.

9.4. THE VARIETY OF ORTHOGONAL REPRESENTATIONS

151

Now if j is any other “good” node, then we can repeat the same argument and find an
orthogonal representation u∗∗ very close to u∗ in which every node previously good remains
good, and in addition all the neighbors of j become good. Since G is connected, repeating
this argument at most n times we obtain an orthogonal representation w of G arbitrarily close
to v in which every node is “good”, i.e., which is is locally general position. By the remark
above, such a representation is in the closure of GORd (G), and hence we find arbitrarily close
to it a general position orthogonal representation f ∗ of G.
¤
If f is a special representation of G in Rd then, by definition, there exists an ε > 0 such
that if g is another orthogonal representation of G in Rd and |f − g| < ε then g is also
special. There must be linear dependencies among the vectors f (v); if ε is small enough
then there will be no new dependencies among the vectors g(v). We say that an orthogonal
representation f is locally free-est if there exists an ε > 0 such that for every orthogonal
representation g with |f − g| < ε, and every subset U ⊆ V , f (U ) is linearly dependent iff
g(U ) is. Clearly every graph having a special representation in Rd also has a locally free-est
special representation.
Lemma 9.4.8 In a locally free-est special representation of a d-critical graph, any two representing vectors are linearly independent.
Proof. Let f be a locally free-est special orthogonal representation of the given graph G.
First assume that f (v) is the 0 vector for some v. Since the number of non-neighbors of v
is at most d − 1, we can replace f (v) by an arbitrarily short non-zero vector orthogonal to
all vectors f (u) where u is a non-neighbor of v, and shorter then ε. This is clearly another
special orthogonal representation in Rd with fewer 0 vectors, a contradiction.
Second, let v and w be two nodes with f (v) and f (w) parallel. By Lemma 9.4.7, the set
of vectors f (u), where u is a non-neighbor of v, is linearly dependent, and hence these vectors
span a linear subspace L of dimension at most d − 2. Thus there exists a vector a ∈ L⊥
not parallel to f (v). We can replace f (v) by f (v) + δa, and obtain another orthogonal
representation of G. If δ is small enough, then this new representation is also special and it
has fewer pairs of parallel representing vectors than f , a contradiction again.
¤
Corollary 9.4.9 If G is a d-critical graph with n nodes then every node has degree at most
n − 4.
Proof. Let f be a locally free-est special orthogonal representation of G in Rd . If a node
v has degree n − 3 then let u1 and u2 be the two non-neighbors of v. If f (u1 ) and f (u2 ) are
parallel then we have a contradiction at Lemma 9.4.8. If they are not parallel, we have a
contradiction at Lemma 9.4.8. If the degree of v is larger than n − 3, the argument is similar.
¤

152

CHAPTER 9. ORTHOGONAL REPRESENTATIONS

Let f be an orthogonal representation of G and v ∈ V . Let Av be the linear span of the
non-neighbors of v and Bv , its orthogonal complement. So f (v) ∈ Bv .
Lemma 9.4.10 Let G be a d-critical graph and f , a locally free-est special representation of
G. Let v ∈ V and u, a non-neighbor of v such that f (u) is linearly dependent on the vectors
representing the other non-neighbors. Then Bu ⊆ Av .
Proof. Suppose not, then Bu contains a vector b arbitrarily close to f (u) but not in Av .
Then replacing f (u) by b we obtain another orthogonal representation f 0 of G. Moreover,
b does not depend linearly on the vectors representing the other non-neighbors of v, which
contradicts the definition of locally free-est special representations.
¤
Next we turn to the case d = 4.
Theorem 9.4.11 If G is a 4-critical graph then G is 3-regular and bipartite.
Proof.

Let G have n nodes. Then Corollary 9.4.9 implies that it is regular of degree

n − 4, i.e., G is 3-regular. Consider the subspaces Av and Bv defined above. Lemma 9.4.7
implies that dim Av ≤ 2 and Lemma 9.4.8 implies that dim Av ≥ 2, so dim Av = 2, and
hence also dim Bv = 2. Thus Lemma 9.4.10 implies that for any two non-adjacent nodes u
and v, Au = Bv . So, fixing any node v, the rest of the nodes fall into two classes: those
with Au = Av and those with Au = Bv . Moreover, any edge in G connects nodes in different
classes. Hence G is bipartite as claimed.
¤
We do not know if there is any other 4-critical graph. An analysis of the cases d ≥ 5
seems even harder.

9.5

Related representations

Thresholds etc.

Chapter 10

Graph independence to linear
independence
10.1

Cover-critical and independence-critical graphs

As a useful tool in the study of graphs critical with respect to stability number, Lovász [130]
considered vector representations with the property that every set of nodes that covers all
the edges, spans the space. We call this cover-preserving.
One can dualize this notion (see section 13.2), to get the following condition on vector
representations: every independent set of nodes is represented by linearly independent vectors.
Such representations are called independence-preserving.
This looks like a play with words, but in fact such representations turn out very useful.
Obviously, every orthogonal representation has this property.
In this case, the dimension problem is trivial: an independence preserving representation
exists in dimension α(G) and higher, and a cover preserving representation exists in dimension τ (G) and lower. For example, we can consider the generic representation in the given
dimension d, in which every d vectors are linearly independent.
But these representations become interesting in conjunction with criticality. We say that
a cover-preserving representation is cover-critical, if deleting any edge from the graph, the
representation does not remain cover-preserving.
Lemma 10.1.1 (a) Let (vi : i ∈ V (G)) be a cover preserving representation of a graph G.
For some j ∈ V (G), let vj0 be a generic point in lin{vi : i ∈ N (j)}. Then replacing vj by vj0
we get a cover preserving representation.
(b) If, in addition, (vi : i ∈ V (G)) is cover-critical and vj is a generic point of the space,
then replacing vj by vj0 we get a cover-critical representation.
Proof.
153

154

CHAPTER 10. GRAPH INDEPENDENCE TO LINEAR INDEPENDENCE
¤

Lemma 10.1.2 (a) The projection of a cover preserving representation onto any subspace
is cover preserving.
(b) If the kernel of the projection is a vector vi that is contained in lin{vj : j ∈ N (i)},
then (vj : j ∈ V (G) \ {i}) is a cover-critical representation of G − i.
Proof.
¤
Lemma 10.1.3 In a cover-critical representation, the neighbors of any node are represented
by linearly independent vectors.
Proof. Let (vi : i ∈ V (G)) be a cover-critical representation of a graph G n dimension d.
Consider any node a and a neighbor b ∈ N (i). The graph G \ ab has a node-cover T that
does not span the space Rd . The vectors va and vb cannot belong to the linear span of T ,
since otherwise T ∪ {a, b} would be a nonspanning node-cover of G. In particular, T does not
contain a, and hence it must contain all neighbors of a other than b. So vb is not spanned by
the vectors vj , j ∈ N (a). Since b is an arbitrary element of N (a), it follows that the vectors
representing N (a) are linearly independent.
¤
Corollary 10.1.4 If G has a cover-critical representation in dimension d, then every node
has degree at most d.
Theorem 10.1.5 If G has a cover-critical representation in dimension d, then |E(G)| ≤
¡d+1¢
2 .
Proof. Let (vi : i ∈ V (G)) be a cover-critical representation of a graph G n dimension d.
Consider the matrices Mij = vi ∧ vj = vi vjT + vj viT (ij ∈ E).
We claim that these matrices are linearly independent. Suppose that there is a linear
dependence
X
λij Mij = 0,
(10.1)
ij∈E(G)

where λab 6= 0 for some edge ab. The graph G \ ab has a node-cover T that does not span
the space Rd ; let u ∈ Rd be orthogonal to the span of T . Clearly uT va 6= 0 and uT vb 6= 0,
else {j ∈ V (G) : uT vj } = 0 would cover all edges of G, and the representations would not
be cover preserving. Then
uT Mij u = uT vi vjT u + uT vj viT u = 2(uT vi )(uT vj ) = 0

10.1. COVER-CRITICAL AND INDEPENDENCE-CRITICAL GRAPHS

155

for every edge ij 6= ab (since one of i and j must be in T ), but
uT Mab u = 2(uT va )(uT vb ) 6= 0.
This contradicts (10.1).
Thus the matrices Mij are linearly independent. Hence their number cannot be larger
¡ ¢
¤
than the dimension of the space of symmetric d × d matrices, which is d+1
2 .
Remark 10.1.6 Representing the nodes of a complete graph on d + 1 nodes by vectors in
Rd in general position, we get a cover-critical representation, showing that the bound in
Theorem 10.1.5 is tight.
What about the number of nodes? Let (vi : i ∈ V (G)) be a cover-critical representation
of a graph G in dimension d. An isolated node plays no role, and can be omitted. (A node
represented by the 0 vector is necessarily isolated.) After deleting isolated nodes, at most
d(d + 1) nodes remain by Theorem 10.1.5.
One cannot say more in general, as the following construction shows. Let us split each
node i of G into d(i) nodes of degree 1, to get a graph G0 consisting of independent edges.
Keep the vector representing i for each of the corresponding new node (or replace it by a
parallel vector). Then we get another cover-critical representation of G0 . (Conversely, if we
identify nodes that are represented by parallel vectors in a cover-critical representation, we
get a cover-critical representation of the resulting graph.)

156

CHAPTER 10. GRAPH INDEPENDENCE TO LINEAR INDEPENDENCE

Chapter 11

Metric embeddings
Given a graph, we would like to embed it in a euclidean space so that the distances between
nodes in the graph should be the same, or at least close to, the geometric distance of the
representing vectors. It is not hard to see that one will necessarily have some distortion in
non-trivial cases. For example, the “claw” K1,3 cannot be embedded without distortion in
any dimension. Furthermore, the complete k-graph can be embedded without distortion in
dimension k − 1 or more. So we are interested in two parameters: the dimension and the
distortion.
These results are best stated in the generality of finite metric spaces. Recall that a metric
space is a set V endowed with a distance function d : V × V such that d(u, v) = 0 if and
only if u = v, d(v, u) = d(u, v), and d(u, w) ≤ d(u, v) + d(v, w) for all u, v, w ∈ V .
Let F : V1 → V2 be a mapping of the metric space (V1 , d1 ) into the metric space (V2 , d2 ).
We define the distortion of F as
d2 (F (u), F (v)) .
d2 (F (u), F (v))
max
min
.
u,v
u,v∈V1
d1 (u, v)
d1 (u, v)
Note that to have finite distortion, the map F must be injective. The distortion does not
change if all distances in one of the metric spaces are scaled by the same factor. So if we
are looking for embeddings in a Banach space, then we may consider embeddings that are
contractive, i.e., d2 (F (u), F (v)) ≤ d1 (u, v) for all u, v ∈ V1 .

11.1

Embeddings in low dimension

Often, the dimension problem is easy to handle, due to a fundamental lemma [105]:
Lemma 11.1.1 (Johnson–Lindenstrauss) For every 0 < ε < 1, every n-point set S ⊂ Rn
can be mapped into Rd with d < 60(ln n)/ε2 ) with distortion 1 + ε.
Proof. Orthogonal projection onto a random d-dimensional subspace does the job.
157

158

CHAPTER 11. METRIC EMBEDDINGS

First, let us see what happens to the distance of a fixed pair of points x, y ∈ S. We
may assume that y = 0 and |x| = 1. Instead of projecting a fixed unit vector on a random
subspace, we project a random unit vector X on a fixed subspace, say on the subspace of Rn
in which the last n − d coordinates are 0.
We can generate a random unit vector by generating n independent standard Gaussian
variables X1 , . . . , Xn , and normalizing:
X=

1
(X1 , . . . , Xn ).
|X1 + · · · + Xn |

The squared length of the projection is
|X 0 |2 =

X12 + · · · + Xd2
.
X12 + · · · + Xn2

Here the numerator is concentrated around d, and the denominator is concentrated around
p
n. Hence the length of X 0 is concentrated around d/n. In fact, both the numerator and
the denominator are from a chi-squared distribution with parameters d and n, and standard
arguments give that
2

P(||X1 |2 + · · · + |Xn |2 − n| > εn) ≤ 2e−ε

n/3

,

and similarly
2

P(||X1 |2 + · · · + |Xd |2 − d| > εd) ≤ 2e−ε
So with probability at least 1 − 4e−ε

2

d/3

d/3

.

, we have

|X1 |2 + · · · + |Xd |2
1+ε d
1−ε d
· ≤
≤
· .
1+ε n
|X1 |2 + · · · + |Xn |2
1−ε n
2

Going back to the original setup, we get that with probability at least 1 − 4e−ε d/3 , we have
r
r
r
r
1−ε d
|F (x) − F (y)|
1+ε d
≤
≤
.
1+ε n
|x − y|
1−ε n
¡ ¢
2
It follows that with probability at least 1 − n2 4e−ε d/3 , this holds for all pairs x, y ∈ S, and
then the distortion of the projection is at most
1+ε
< 1 + 3ε.
1−ε
Replacing ε by ε/3 and choosing d as in the Theorem, this probability is positive.

11.2

Embeddings with small distortion

Bourgain [29] proved the following theorem:

¤

11.2. EMBEDDINGS WITH SMALL DISTORTION

159

Theorem 11.2.1 Every metric space with n elements can be embedded in an O(log n)dimensional euclidean space with O(log n) distortion.
Proof. First we ignore the bound on the dimension.
Let m = dlog ne, and choose an integer k ∈ [m] uniformly at random. Choose a random
subset A ⊆ V , putting every element i ∈ V into A with probability 2−k . Let d(i, A) denote
the distance of i from the set A (if A = ∅, we set d(i, A) = 0). By the triangle inequality
|d(i, A) − d(j, A)| ≤ d(i, j).

(11.1)

The key to the proof is the following reverse inequality:
E|d(i, A) − d(j, A)| ≥

1
d(i, j).
4m

(11.2)

To prove this inequality, let D(v, t) denote the set of 2t nearest points to v ∈ V , and let
ρv (t) = max{d(v, x) : x ∈ D(v, t)}, and ρ(t) = max{ρi (t), ρj (t)}. Let h be the largest
integer for which ρ(h) < d(i, j)/2. Clearly h ≤ m and D(i, h) ∩ D(j, h) = ∅. Modify the
definition of D(i, h + 1) by deleting from it all points in D(j, h), and vice versa, and also set
ρ(h + 1) = d(i, j)/2.
We claim that for a fixed choice of 1 ≤ k ≤ h + 1,
EA |d(i, A) − d(j, A)| ≥

1
(ρ(k) − ρ(k − 1)).
10

(11.3)

We may assume that ρj (k) ≤ ρi (k), so that ρ(k) = ρi (k). If A intersects D(j, k − 1) but does
not intersect D(i, k), then
d(i, A) − d(j, A) ≥ ρi (k) − ρj (k − 1) ≥ ρ(k) − ρ(k − 1)
(it is easy to check that this holds also for k = h + 1). Furthermore, the probability that this
happens can be bounded using that D(i, k) ∩ D(j, k − 1) = ∅:
³
³
1 ´|D(j,k−1)| ´³
1 ´|D(i,k)|
P(A ∩ D(j, k − 1) 6= ∅, A ∩ D(i, k) = ∅) = 1 − 1 − k
1− k
2
2
k−1
k
³
³
1 ´2 ´³
1 ´2
≤ 1− 1− k
1− k
2
2
³
1
1 1
.
≈ 1− √ ) >
10
e e
This proves (11.3).
Averaging over the random choice of k, we get
E|d(i, A) − d(j, A)| =

m
h+1
X
X 1
1
EA |d(i, A) − d(j, A)| ≥
(ρ(k) − ρ(k − 1))
m
10m

k=1

k=1

1
1
=
(ρ(h + 1) − ρ(0)) =
d(i, j),
10m
20m

160

CHAPTER 11. METRIC EMBEDDINGS

which proves (11.2).
Now let A1 , . . . , AN be independently generated as A above, and consider the embedding
³ 1
´
1
F : i 7→ √ d(i, A1 ), . . . , √ d(i, AN ) .
N
N
Then by (11.1)

v
u
N
u1 X
|F (i) − F (j)| = t
(d(i, Ak ) − d(j, Ak ))2 ≤ d(i, j),
N
k=1

so F is contractive. On the other hand, by the inequality between quadratic and arithmetic
means,
v
uN
N
uX 1
1 X
(d(i, Ak ) − d(j, Ak ))2 ≥
|F (i) − F (j)| = t
|d(i, Ak ) − d(j, Ak )|.
N
N
k=1

k=1

Here by the Law of Large Numbers and (11.2),
N
1 X
1
|d(i, Ak ) − d(j, Ak )| ≈ E|d(i, A) − d(j, A)| ≥
d(i, j),
N
20m
k=1

where the relative error at the ≈ sign is less than 2 with probability at least 1 − n−2 if N is
large enough. Thus with positive probability,
1
d(i, j) ≤ |F (i) − F (j)| ≤ d(i, j)
40m
holds for every i and j, and so F has distortion at most 40m.
The condition on the dimension can be satisfied by an application of the Johnson–
Lindenstrauss Lemma.
¤
We note that essentially the same embedding works for any norm `p , 1 ≤ p ≤ ∞.
Linial, London and Rabinovitch [125] showed how to construct an embedding satisfying
the conditions in Theorem 11.2.1 algorithmically, and gave the application described in the
next section. Matoušek [147] showed that for an expander graph this is best possible.

11.3

Application to multicommodity flows

A fundamental result in the theory of multicommodity flows is the theorem of Leighton and
Rao [124]. Stated in a larger generality, as proved by Linial, London and Rabinovich [125],
it says the following.
Suppose that we have a multicommodity flow problem on a graph on n nodes. Obvious
cut-conditions provide a system of necessary conditions for the problem to be feasible; but
(unlike for the case of a single commodity), these conditions are not sufficient in general.
The theorem asserts that if the cut-conditions are satisfied, then relaxing the capacities by a
factor of O(log n), the problem becomes feasible.

11.4. VOLUME-RESPECTING EMBEDDINGS

11.4

161

Volume-respecting embeddings

A very interesting extension and application of Bourgain’s method was given by Feige [70].
Let F : V → Rn be a mapping of a finite metric space (V, d) in to the euclidean space Rn .
We want an embedding that is
(a) contractive,
(b) volume respecting, which means that every set of at most s nodes spans a simplex
whose volume is almost as large as possible.
Obviously, (b) needs an explanation. Consider any set S ⊆ V with k elements. Let T be
a shortest spanning tree on S with respect to the distance d, and
1 Y
treevol(S) =
d(u, v).
k!
uv∈E(T )

Lemma 11.4.1 For every contractive map F : V → Rn , the volume of the simplex spanned
by F (S) is at most treevol(S).
Proof.
¤
Feige proves the following theorem.
Theorem 11.4.2 Every finite metric space (V, d) with n elements has a contractive map
F : V → Rd with d = O((log n)3 ), such that for each set S with |S| ≤ log n, the volume of
the simplex spanned by F (S) is at least treevol(S)/(log n)2|S| .
Proof.
¤
As an application of this result, we describe an algorithm that finds a polylogarithmic
approximation of the bandwidth of a graph in polynomial time. The ordering of the nodes
which approximates the bandwidth is now obtained through a random projection of the
representation to the line, in a fashion similar to the Goemans–Williamson algorithm:
Theorem 11.4.3 Let G = (V, E) be a graph, and let d(u, v) denote the distance of nodes u
and v in the graph. Let F : V → Rd be a map with d = O((log n)3 ), such that for each set S
with |S| ≤ log n, the volume of the simplex spanned by F (S) is at least treevol(S)/(log n)2|S| .
Let P : Rd → L be the orthogonal projection onto a random line L through the origin, and
let (i1 , . . . , in ) be the ordering of V according to the order of the projected nodes on the line.
Then (i1 , . . . , in ) approximates the bandwidth within a factor of O((log n)5 ).
Proof.
¤

162

CHAPTER 11. METRIC EMBEDDINGS

Chapter 12

Adjacency matrix and regularity
partitions
12.1

Similarity metric

We can equip every graph G = (V, E) with a metric as follows. Let A be the adjacency
matrix of G. We define the similarity distance of two nodes i, j ∈ V as the `1 distance of the
corresponding rows of A2 (squaring the matrix seems unnatural, but it is crucial; it turns
out to get rid of random fluctuations).

12.2

Regularity partitions

The following was proved (in somewhat different form) in [140].
Theorem 12.2.1 Let G be a graph and let P = {V1 , . . . , Vk } be a partition of V .
√
(a) If d¤ (G, GP ) = ε, then there is a set S ⊆ V with |S| ≤ 8 ε|V | such that for each
√
partition class, Vi \ S has diameter at most 8 ε in the d2 metric.
(b) If there is a set S ⊆ V with |S| ≤ δ|V | such that for each partition class, Vi \ S has
diameter at most δ in the d2 metric, then d¤ (G, GP ) ≤ 24δ.
Theorem 12.2.1 suggests to define the dimension of a family G of graphs as the infimum
of real numbers d > 0 for which the following holds: for every ε > 0 and G ∈ G the node
set of G can be partitioned into a set of at most ε|V (G)| nodes and into at most ε−d sets
of diameter at most ε. (This number can be infinite.) In the cases when the graphs have a
natural dimensionality, this dimension tends to give the right value. For example, let G be
obtained by selecting n random points on the d-dimensional unit sphere, and connecting two
of these points x and y with a probability W (x, y), which is a continuous function of x and
y. With probability 1, this sequence has dimension Θ(d).
163

164

CHAPTER 12. ADJACENCY MATRIX AND REGULARITY PARTITIONS

Chapter 13

Some general issues
Is there a way to fit the many forms of geometric representations discussed in these notes into
a single theory? Perhaps not, considering the variety of possibilities how the graph structure
can be reflected by the geometry. Nevertheless, there are some general ideas that can be
pointed out.

13.1

Non-degeneracy

A common theme in connection with various representations is that imposing non-degeneracy
conditions on the representation often makes it easier to analyze and therefore more useful
(basically, by eliminating the possibility of numerical coincidence). There are at least 3 types
of non-degeneracy conditions that are used in various geometric representation; we illustrate
the different possibilities by formulating them in the case of unit distance representations in
Rd . All three are easily extended to other kinds of representations.
The most natural non-degeneracy condition is faithfulness: we want to represent the
graph so that adjacent nodes and only those are at unit distance. This is usually not strong
enough. General position means that no d + 1 of the points are contained in a hyperplane.
Perhaps the deepest non-degeneracy notion is the following. We write the condition
on the representation as a system of algebraic equations. For example, for unit distance
representations we write
kui − uj k2 = 1

(ij ∈ E).

We have nd unknowns (the coordinates of the ui ). Each of these equations defines a hypersurface in Rnd , and a representation corresponds to a point where these hypersurfaces intersect.
Now we say that this representation has the Strong Arnold Property if the hypersurfaces
intersect transversally, i.e., their normal vectors at this point are linearly independent. This
condition means that the intersection point is not just accidental, but is forced by some more
fundamental structure; for example, if the representation has the Strong Arnold Property,
165

166

CHAPTER 13. SOME GENERAL ISSUES

and we change by a small amount each constant 1 on the right hand sides of the defining
equations, we get another solvable system.

13.2

Duality

The following notion of duality is known under many aliases: dual chain group in matroid
theory, dual code in coding theory, Gale diagram in the theory of hyperplane arrangements,
etc. Let u1 , . . . , un ∈ Rd . Write down these vectors as column vectors, and let L be the row
space of the resulting matrix. Pick any basis in the orthogonal complement L⊥ of L, write
them down as row vectors, and let v1 , . . . , vn ∈ Rn−d be the columns of the resulting matrix.
One of the main properties of this construction is that a set of the ui forms a basis of Rd if
and only if the complementary set of the vi forms a basis of Rn−d .
We can carry out this construction for any vector representation of a graph G, to get a
dual vector representation. In some cases, this gives interesting constructions; for example,
from cover-preserving representations we get independence-preserving representations. But
note that (at least in the definition above) the dual is only determined up to an affine
transformation; for geometric representations with metric properties (which is the majority),
dualization does not seem to make sense. Yet it seem that in some cases more than the basic
linear structure is dualized, and we don’t have a general explanation for this. Let us briefly
mention two examples.
In [82], a duality for orthogonal representations of a graph and its complement has been
described. One of the consequences is that every graph G has an orthogonal representation
whose dual (in the sense described above) becomes an orthogonal representation of the complementary graph G, if an appropriate single row is added. This result is connected to the
duality theory of semidefinite programming.
In [118], it was pointed out that there seems to be a duality between the Colin de Verdère
numbers of planar graphs and their complements. Again (up to a single row) the nullspace
representation and the Gram representation derived from a Colin de Verdère matrix of a graph
are dual to each other; but while the Gram representation has strong metric properties, it is
unclear how to impose those on the nullspace representation.

13.3

Algorithms

To represent a graph geometrically is a natural goal in itself, but in addition it is an important
tool in the study of various graph properties, including their algorithmic aspects. There are
several levels of this interplay between algorithms and geometry.
— Often the aim is to find a way to represent a graph in a “good” way. We refer to
Kuratowski’s characterization of planar graphs, its more recent extensions most notably by
Robertson and Seymour, and to Steinitz’s theorem representing 3-connected planar graphs

13.3. ALGORITHMS

167

by 3-dimensional polyhedra. Many difficult algorithmic problems in connection with these
representations have been studied.
— In other cases, graphs come together with a geometric representation, and the issue is
to test certain properties, or compute some parameters, that connect the combinatorial and
geometric structure. A typical question in this class is rigidity of bar-and-joint frameworks,
an area whose study goes back to the work of Cauchy and Maxwell.
— Most interesting are the cases when a good geometric representation of a graph leads
to algorithmic solutions of purely graph-theoretic questions that, at least on the surface, do
not seem to have anything to do with geometry. Our discussions contained several examples
of this (but the list will be far from complete): graph connectivity, graph coloring, finding
maximum cliques in perfect graphs, giving capacity bounds in information theory, approximating the maximum cut and the bandwidth, planarity, linkless embedability, and rigidity
of frameworks.

168

CHAPTER 13. SOME GENERAL ISSUES

Appendix: Background material
13.4

Background from linear algebra

13.4.1

Basic facts about eigenvalues

Let A be an n × n real matrix. An eigenvector of A is a vector such that Ax is parallel to
x; in other words, Ax = λx for some real or complex number λ. This number λ is called the
eigenvalue of A belonging to eigenvector v. Clearly λ is an eigenvalue iff the matrix A − λI
is singular, equivalently, iff det(A − λI) = 0. This is an algebraic equation of degree n for λ,
and hence has n roots (with multiplicity).
The trace of the square matrix A = (Aij ) is defined as
tr(A) =

n
X

Aii .

i=1

The trace of A is the sum of the eigenvalues of A, each taken with the same multiplicity as
it occurs among the roots of the equation det(A − λI) = 0.
If the matrix A is symmetric, then its eigenvalues and eigenvectors are particularly well
behaved. All the eigenvalues are real. Furthermore, there is an orthogonal basis v1 , . . . , vn
of the space consisting of eigenvectors of A, so that the corresponding eigenvalues λ1 , . . . , λn
are precisely the roots of det(A − λI) = 0. We may assume that |v1 | = · · · = |vn | = 1; then
A can be written as
n
X
A=
λi vi viT .
i+1

Another way of saying this is that every symmetric matrix can be written as U T DU , where
U is an orthogonal matrix and D is a diagonal matrix. The eigenvalues of A are just the
diagonal entries of D.
To state a further important property of eigenvalues of symmetric matrices, we need the
following definition. A symmetric minor of A is a submatrix B obtained by deleting some
rows and the corresponding columns.
Theorem 13.4.1 (Interlacing eigenvalues) Let A be an n × n symmetric matrix with
eigenvalues λ1 ≥ · · · ≥ λn . Let B be an (n − k) × (n − k) symmetric minor of A with
169

170

CHAPTER 13. SOME GENERAL ISSUES

eigenvalues µ1 ≥ · · · ≥ µn−k . Then
λi ≤ µi ≤ λi+k .
We conclude this little overview with a further basic fact about nonnegative matrices.
Theorem 13.4.2 (Perron-Frobenius) If an n × n matrix has nonnegative entries then it
has a nonnegative real eigenvalue λ which has maximum absolute value among all eigenvalues.
This eigenvalue λ has a nonnegative real eigenvector. If, in addition, the matrix has no blocktriangular decomposition (i.e., it does not contain a k × (n − k) block of 0-s disjoint from the
diagonal), then λ has multiplicity 1 and the corresponding eigenvector is positive.

13.4.2

Semidefinite matrices

A symmetric n × n matrix A is called positive semidefinite, if all of its eigenvalues are
nonnegative. This property is denoted by A º 0. The matrix is positive definite, if all of its
eigenvalues are positive.
There are many equivalent ways of defining positive semidefinite matrices, some of which
are summarized in the Proposition below.
Proposition 13.4.3 For a real symmetric n × n matrix A, the following are equivalent:
(i) A is positive semidefinite;
(ii) the quadratic form xT Ax is nonnegative for every x ∈ Rn ;
(iii) A can be written as the Gram matrix of n vectors u1 , ..., un ∈ Rm for some m; this
T
means that aij = uT
i uj . Equivalently, A = U U for some matrix U ;
(iv) A is a nonnegative linear combination of matrices of the type xxT ;
(v) The determinant of every symmetric minor of A is nonnegative.
Let me add some comments. The least m for which a representation as in (iii) is possible
is equal to the rank of A. It follows e.g. from (ii) that the diagonal entries of any positive
semidefinite matrix are nonnegative, and it is not hard to work out the case of equality: all
entries in a row or column with a 0 diagonal entry are 0 as well. In particular, the trace of
a positive semidefinite matrix A is nonnegative, and tr(A) = 0 if and only if A = 0.
The sum of two positive semidefinite matrices is again positive semidefinite (this follows
e.g. from (ii) again). The simplest positive semidefinite matrices are of the form aaT for
some vector a (by (ii): we have xT (aaT )x = (aT x)2 ≥ 0 for every vector x). These matrices
are precisely the positive semidefinite matrices of rank 1. Property (iv) above shows that
every positive semidefinite matrix can be written as the sum of rank-1 positive semidefinite
matrices.

13.4. BACKGROUND FROM LINEAR ALGEBRA

171

The product of two positive semidefinite matrices A and B is not even symmetric in
general (and so it is not positive semidefinite); but the following can still be claimed about
the product:
Proposition 13.4.4 If A and B are positive semidefinite matrices, then tr(AB) ≥ 0, and
equality holds iff AB = 0.
Property (v) provides a way to check whether a given matrix is positive semidefinite.
This works well for small matrices, but it becomes inefficient very soon, since there are many
symmetric minors to check. An efficient method to test if a symmetric matrix A is positive
semidefinite is the following algorithm. Carry out Gaussian elimination on A, pivoting always
on diagonal entries. If you ever find a negative diagonal entry, or a 0 diagonal entry whose
row contains a non-zero, stop: the matrix is not positive semidefinite. If you obtain an
all-zero matrix (or eliminate the whole matrix), stop: the matrix is positive semidefinite.
If this simple algorithm finds that A is not positive semidefinite, it also provides a certificate in the form of a vector v with v T Av < 0. Assume that the i-th diagonal entry of
the matrix A(k) after k steps is negative. Write A(k) = EkT . . . E1T AE1 . . . Ek , where Ei are
elementary matrices. Then we can take the vector v = E1 . . . Ek ei . The case when there is
a 0 diagonal entry whose row contains a non-zero is similar.
It will be important to think of n × n matrices as vectors with n2 coordinates. In this
space, the usual inner product is written as A · B. This should not be confused with the
matrix product AB. However, we can express the inner product of two n × n matrices A and
B as follows:
A·B =

n X
n
X

Aij Bij = tr(AT B).

i=1 j=1

Positive semidefinite matrices have some important properties in terms of the geometry
of this space. To state these, we need two definitions. A convex cone in Rn is a set of vectors
which along with any vector, also contains any positive scalar multiple of it, and along with
any two vectors, also contains their sum. Any system of homogeneous linear inequalities
aT
1 x ≥ 0,

...

aT
mx ≥ 0

defines a convex cone; convex cones defined by such (finite) systems are called polyhedral.
For every convex cone C, we can form its polar cone C ∗ , defined by
C ∗ = {x ∈ Rn : xT y ≥ 0 ∀y ∈ C}.
This is again a convex cone. If C is closed (in the topological sense), then we have (C ∗ )∗ = C.
The fact that the sum of two such matrices is again positive semidefinite (together with
the trivial fact that every positive scalar multiple of a positive semidefinite matrix is positive

172

CHAPTER 13. SOME GENERAL ISSUES

semidefinite), translates into the geometric statement that the set of all positive semidefinite
matrices forms a convex closed cone Pn in Rn×n with vertex 0. This cone Pn is important,
but its structure is quite non-trivial. In particular, it is non-polyhedral for n ≥ 2; for n = 2 it
is a nice rotational cone (Figure 13.1; the fourth coordinate x21 , which is always equal to x12
by symmetry, is suppressed). For n ≥ 3 the situation becomes more complicated, because
Pn is neither polyhedral nor smooth: any matrix of rank less than n − 1 is on the boundary,
but the boundary is not differentiable at that point.

x22

x11

x12
Figure 13.1: The semidefinite cone for n = 2.

The polar cone of P is itself; in other words,
Proposition 13.4.5 A matrix A is positive semidefinite iff A · B ≥ 0 for every positive
semidefinite matrix B.

13.4.3

Cross product

This construction is probably familiar from physics. For a, b ∈ R3 , we define their cross
product as the vector
a × b = |a| · |b| · sin φ · u,

(13.1)

where φ is the angle between a and b (0 ≤ φ ≤ π), and u is a unit vector in R3 orthogonal
to the plane of a and b, so that the triple (a, b, u) is right-handed (positively oriented). The
definition of u is ambiguous if a and b are parallel, but then sin φ = 0, so the cross product
is 0 anyway. The length of the cross product gives the area of the parallelogram spanned by
a and b.
The cross product is distributive with respect to linear combination of vectors, it is
anticommutative: a × b = −b × a, and a × b = 0 if and only if a and b are parallel. The cross

13.5. GRAPH THEORY

173

product is not associative; instead, it satisfies the Expansion Identity
(a × b) × c = (a · c)b − (b · c)a,

(13.2)

which implies the Jacobi Identity
(a × b) × c + (b × c) × a + (c × a) × b = 0.

(13.3)

Another useful replacement for the associativity is the following.
(a × b) · c = a · (b × c) = det(a, b, c)

(13.4)

(here (a, b, c) is the 3 × 3 matrix with columns a, b and c.
We often use the cross product in the special case when the vectors lie in a fixed plane
Π. Let k be a unit vector normal to Π, then a × b is Ak, where A is the signed area of the
parallelogram spanned by a and b (this means that T is positive iff a positive rotation takes
the direction of a to the direction of b, when viewed from the direction of k). Thus in this
case all the information about a × b is contained in this scalar A, which in tensor algebra
would be denoted by a ∧ b. But not to complicate notation, we’ll use the cross product in
this case as well.

13.5

Graph theory

13.5.1

Basics

13.5.2

Szemerédi partitions

13.6

Eigenvalues of graphs

13.6.1

Matrices associated with graphs

We introduce the adjacency matrix, the Laplacian and the transition matrix of the random
walk, and their eigenvalues.
Let G be a (finite, undirected, simple) graph with node set V (G) = {1, . . . , n}. The
adjacency matrix of G is be defined as the n × n matrix AG = (Aij ) in which
(
1,
Aij =
0,

if i and j are adjacent,
otherwise.

We can extend this definition to the case when G has multiple edges: we just let Aij be the
number of edges connecting i and j. We can also have weights on the edges, in which case
we let Aij be the weight of the edges. We could also allow loops and include this information
in the diagonal, but we don’t need this in this course.

174

CHAPTER 13. SOME GENERAL ISSUES

The Laplacian of the graph is defined as the n × n matrix LG = (Lij ) in which
(
di ,
if i = j,
Lij =
−Aij , if i 6= j.
Here di denotes the degree of node i. In the case of weighted graphs, we define di =

P
j

Aij .

So LG = DG − AG , where DG is the diagonal matrix of the degrees of G.
The transition matrix of the random walk on G is defined as the n × n matrix PG = (Pij )
in which
Pij =

1
Aij .
di

−1
So PG = DG
A.
The matrices AG and LG are symmetric, so their eigenvalues are real. The matrix PG is
not symmetric, but it is conjugate to a symmetric matrix. Let
−1/2

NG = DG

−1/2

AG DG

,

then NG is symmetric, and
−1/2

PG = DG

1/2

NG DG .

The matrices AG and LG and NG are symmetric, so their eigenvalues are real. The
matrices PG and NG have the same eigenvalues, and so all eigenvalues of PG are real. We
denote these eigenvalues as follows:
AG : λ 1 ≥ λ 2 ≥ · · · ≥ λ n ,
LG : µ1 ≤ µ2 ≤ · · · ≤ µn ,
AG : ν1 ≥ ν2 ≥ · · · ≥ νn ,
Exercise 13.1 Compute the spectrum of complete graphs, cubes, stars, paths.
We’ll often use the (generally non-square) incidence matrix of G. This notion comes in
two flavors. Let V (G) = {1, . . . , n} and E(G) = {e1 , . . . , em , and let BG denote the n × m
matrix for which
(
1 if i is and endpoint of ej ,
(BG )ij =
0 otherwise.
Often, however, the following matrix is more useful: Let us fix an orientation of each edge,
−
→
− denote the n × m matrix for which
to get an oriented graph G . Then let B→
G


if i is the head of ej ,
1
− )ij =
(B→
−1
if i is the tail of ej ,
G


0
otherwise.

13.6. EIGENVALUES OF GRAPHS

175

Changing the orientation only means scaling some columns by −1, which often does not
matter much. For example, it is easy to check that independently of the orientation,
T
− B→
LG = B→
−.
G G

(13.5)

It is worth while to express this equation in terms of quadratic forms:
x T LG x =

n
X

(xi − xj )2 .

(13.6)

ij∈E(G)

13.6.2

The largest eigenvalue

Adjacency matrix
The Perron–Frobenius Theorem implies immediately that if G is connected, then the largest
eigenvalue λmax of AG of AG has multiplicity 1. This eigenvalue is relatively uninteresting,
it is a kind of “average degree”. More precisely, let dmin denote the minimum degree of G,
let d be the average degree, and let dmax be the maximum degree.
Proposition 13.6.1 For every graph G,
p
max{d, dmax } ≤ λmax ≤ dmax .
Proof.
¤
Exercise 13.2 Compute the largest eigenvalue of a star.
Laplacian
For the Laplacian LG , this corresponds to the smallest eigenvalue, which is really uninteresting, since it is 0:
Proposition 13.6.2 The Laplacian LG is singular and positive semidefinite.
Proof. The proof follows immediately from (13.5) or (13.6), which show that LG is positive
semidefinite. Since 1 = (1, . . . , 1)T is in the null space of LG , it is singular.
¤
If G is connected, then 0, as an eigenvalue of LG , has multiplicity 1; we get this by
applying the Perron–Frobenius Theorem to cI − LG , where c is a large real number. The
eigenvector belonging to this eigenvalue is 1 = (1, . . . , 1)T (and its scalar multiples).
We note that for a general graph, the multiplicity of the 0 eigenvalue of the Laplacian is
equal to the number of connected components. Similar statement is not true for the adjacency
matrix (if the largest eigenvalues of the connected components of G are different, then the
largest eigenvalue of the whole graph has multiplicity 1). This illustrates the phenomenon
that the Laplacian is often better behaved algebraically than the adjacency matrix.

176

CHAPTER 13. SOME GENERAL ISSUES

Transition matrix
The largest eigenvalue of PG is 1, and it has multiplicity 1 for connected graphs. It is
straightforward to check that the right eigenvector belonging to it is 1, and the left eigenvector
is given by πi = di /(2m) (where m is the number of edges). This vector π describes the
stationary distribution of a random walk, and it is very important in the theory of random
walks (see later).

13.6.3

The smallest eigenvalue

Proposition 13.6.3 (a) A graph is bipartite if and only if its spectrum is symmetric about
the origin.
(b) A connected graph G is bipartite if and only if λmin (G) = −λmax (G).
Proof.
¤
The “only if” part of Proposition 13.6.3 can be generalized: The ratio between the largest
and smallest eigenvalue can be used to estimate the chromatic number (Hoffman [101]).
Theorem 13.6.4
χ(G) ≥ 1 +

λmin
.
λmax

Proof. Let k = χ(G), then AG can be partitioned as


0
M12 . . . M1k
M21
0
M2k 


 ..

..
..
 .

.
.
Mk1 Mk2
0,
where Mij is an mi × mj matrix (where mi is the number of points with color i).
Let v be an eigenvector belonging to λ1 . Let us break v into pieces v1 , . . . , vk of length
m1 , . . . , mk , respectively. Set
 
 
|vi |
w1
 0 
 
 
wi =  .  ∈ Rmi w =  ...  .
 .. 
wk
0
Let Bi be any orthogonal matrix such that
Bi wi = vi

(i = 1, . . . , k),

13.6. EIGENVALUES OF GRAPHS

177

and

B1


B=
0

0
B2
..

.




.


Bk
Then Bw = v and
B −1 ABw = B −1 Av = λ1 B −1 v = λ1 w
so w is an eigenvector of B −1 AB. Moreover, B −1 AB has the form


0
B1−1 A12 B2 . . . B1−1 A1k Bk
B2−1 A21 B1
0
B2−1 A2k Bk 


.

..
..
..


.
.
.
Bk−1 Ak1 B1

Bk−1 Ak2 B2

...

0

Pick the entry in the upper left corner of each of the k 2 submatrices Bi−1 Aij Bj (Aii = 0),
these form a k × k submatrix D. Observe that


|v1 |


u =  ... 
|vk |
is an eigenvector of D; for w is an eigenvector of B −1 AB and has 0 entries on places corresponding to those rows and columns of B −1 AB, which are to be deleted to get D. Moreover,
the eigenvalue belonging to u is λ1 .
Let α1 ≥ · · · ≥ αk be the eigenvalues of D. Since D has 0’s in its main diagonal,
α1 + · · · + αk = 0.
On the other hand, λ1 is an eigenvalue of D and so
λ1 ≤ α1 ,
while by the Interlacing Eigenvalue Theorem
λn ≤ αk , . . . , λn−k+2 ≤ α2 .
Thus
λn + · · · + λn−k+2 ≤ αk + · · · + α2 = −α1 ≤ −λ1 .
¤

178

CHAPTER 13. SOME GENERAL ISSUES

Remark 13.6.5 The proof did not use that the edges were represented by the number 1,
only that the non-edges and diagonal entries were 0. So if we want to get the strongest
possible lower bound on the chromatic number that this method provides, we can try to find
a way of choosing the entries in A corresponding to edges of G in such a way that the right
hand side is minimized. This can be done efficiently.
The smallest eigenvalue is closely related to the characterization of linegraphs. The correspondence is not perfect though. To state the result, we need some definitions. Let G be
a simple graph. A pending star in G is a maximal set of edges which are incident with the
same node and whose other endpoints have degree 1. The linegraph L(G) of G is defined on
V (L(G)) = E(G), where to edges of G are adjacent in L(G) if and only if they have a node
in common. A graph H is called a modified linegraph of G if it is obtained from L(G) by
deleting a set of disjoint edges from each clique corresponding to a pending star of G.
Part (a) of the following theorem is due to Hoffman [100], part (b), to Cameron, Goethals,
Seidel and Shult [37].
Proposition 13.6.6 (a) Let H be the generalized linegraph of G. Then λmin (H) ≥ −2; if
|E(G)| > |V (G)|, then λmin (H) = −2.
(b) Let H be a simple graph such that λmin (H) ≥ −2. Assume that |V (H)| ≥ 37. Then
G is a modified linegraph.
Proof. We only give the proof for part (a), and only in the case when H = L(G). It is
easy to check that we have
T
AL(G) = BG
BG − 2I.
T
Since BG
BG is positive semidefinite, all of its eigenvalues are non-negative. Hence, the
eigenvalues of AL(G) are ≥ −2. Moreover, if |V (G)| < |E(G)|, then

r(B T B) = r(B) ≤ |V (G)| < |E(G)|
(r(X) is the rank of the matrix X). So, B T B has at least one 0 eigenvalue, i.e. AL(G) has
at least one −2 eigenvalue.
¤
Exercise 13.3 Modify the proof above to get (a) in general.

13.6.4

The eigenvalue gap

The gap between the second and the first eigenvalues is an extremely important parameter
in many branches of mathematics.
If the graph is connected, then the largest eigenvalue of the adjacency matrix as well as the
smallest eigenvalue of the Laplacian have multiplicity 1. We can expect that the gap between

13.6. EIGENVALUES OF GRAPHS

179

this and the nearest eigenvalue is related to some kind of connectivity measure of the graph.
Indeed, fundamental results due to Alon–Milman [12], Alon [8] and Jerrum–Sinclair [106]
relate the eigenvalue gap to expansion (isoperimetric) properties of graphs. These results
can be considered as discrete analogues of Cheeger’s inequality in differential geometry.
There are many related (but not equivalent) versions of these results. We illustrate
this connection by two versions that are of special interest: a spectral characterization of
expanders and a bound on the mixing time of random walks on graphs. For this, we discuss
very briefly expanders and also random walks and their connections with eigenvalues (see [4]
and [143] for more).
The multiplicity of the second largest eigenvalue will be discussed in connection with the
Colin de Verdière number.
Expanders
An expander is a regular graph with small degree in which the number of neighbors of any
set containing at most half of the nodes is at least a constant factor of its size. To be precise,
an ε-expander is a graph G = (V, E) in which for every set S ⊂ V with |S| ≤ |V |/2, the
number of nodes in V \ S adjacent to some node in S is at least ε|S|.
Expanders play an important role in many applications of graph theory, in particular
in computer science. The most important expanders are d-regular expanders, where d ≥
3 is a small constant. Such graphs are not easy to construct. One method is to do a
random construction: for example, we can pick d random perfect matchings on 2n nodes
(independently, uniformly over all perfect matchings), and let G be the union of them. Then a
moderately complicated computation shows that G is an ε-expander with positive probability
for a sufficiently small ε. Deterministic constructions are much more difficult to obtain; the
first construction was found by Margulis [146]; see also [144]. Most of these constructions
are based on deep algebraic facts.
Our goal here is to state and prove a spectral characterization of expanders, due to
Alon [8], which plays an important role in analyzing some of the above mentioned algebraic
constructions. note that since we are considering only regular graphs, the adjacency matrix,
the Laplacian and the transition matrix are easily expressed, and so we shall only consider
the adjacency matrix.
Theorem 13.6.7 Let G be a d-regular graph.
(a) If d − λ2 ≥ 2εd, then G is an ε-expander.
(b) If G is an ε-expander, then d − λ2 ≥ ε2 /5.
Proof. The proof is similar to the proof of Theorem 13.6.8 below.

¤

180

CHAPTER 13. SOME GENERAL ISSUES

Edge expansion (conductance)
We study the connection of the eigenvalue gap of the transition matrix with a quantity that
can be viewed as an edge-counting version of the expansion. Let 1 = λ1 ≥ λ2 ≥ . . . ≥ λn be
the eigenvalues of PG .
The conductance of a graph G = (V, E) is defined as follows. For two sets S1 , S2 ⊆ V , let
eG (S1 , S2 ) denote the number of edges ij with i ∈ S1 , j ∈ S2 . For every subset S ⊆ V , let
P
d(S) = i∈S di , and define
Φ(G) = min

∅⊂S⊂V

2meG (S, V \ S)
.
d(S) · d(V \ S)

For a d-regular graph, this can be written as
Φ(G) = min

∅⊂S⊂V

n eG (S, V \ S)
.
d |S| · |V \ S|

The following basic inequality was proved by Jerrum and Sinclair [106]:
Theorem 13.6.8 For every graph G,
Φ(G)2
≤ 1 − λ2 ≤ Φ(G)
16
We start with a lemma expressing the eigenvalue gap of PG in a manner similar to the
Rayleigh quotient.
Lemma 13.6.9 For every graph G we have
X
1 − λ2 = min
(xi − xj )2 ,
(i,j)∈E

where the minimum is taken over all vectors x ∈ RV such that
X
X
di xi = 0,
di x2i = 1.
i∈V

(13.7)

i∈V
1/2

−1/2

Proof. As remarked before, the symmetrized matrix NG = DG PG DG
has the same
eigenvalues as PG . For a symmetric matrix, the second largest eigenvalue can be obtained as
λ2 = max y T NG y,
where y ranges over all vectors of unit length orthogonal to the eigenvector belonging to
√
the largest eigenvalue. This latter eigenvector is given (up to scaling) by vi = di , so the
conditions on y are
X
Xp
di yi = 0,
yi2 = 1.
(13.8)
i∈V

i∈V

13.6. EIGENVALUES OF GRAPHS

181

√
Let us write yi = xi di , then the conditions (13.8) on y translate into conditions (13.7) on
x. Furthermore,
X
X
yi
yj
(xi − xj )2 = 2m
( √ − p )2
d
dj
i
(i,j)∈E
(i,j)∈E
=

X

di

i

X
yi2
yi yj
−2
(√ p
di
di dj
(i,j)∈E

= 1 − y T NG y.
The minimum of the left hand side subject to (13.7) is equal to the minimum of the right
hand side subject to (13.8), which proves the Lemma.
¤
Now we can prove the theorem.
Proof. The upper bound is easy: let ∅ 6= S ⊂ V be a set with
2meG (S, V \ S)
= Φ(G).
d(S) · d(V \ S)
Let x be a vector on the nodes defined by
q
 d(V \S)
if i ∈ S,
2md(S)
q
xi =
d(S)
−
if i ∈ V \ S.
2md(V \S)
It is easy to check that
X
X
di xi = 0,
di x2i = 1.
i∈V

i∈V

Thus by Lemma 13.6.9,
1 − λ2 ≥

X

Ãs
(xi − xj )2 = eG (S, V \ S)

ij∈E

=

s
d(V \ S)
+
2md(S)

d(S)
2md(V \ S)

!2

2meG (S, V \ S)
= Φ(G).
d(S)d(V \ S)

It is easy to see that the statement giving the lower bound can be written as follows: let
P
y ∈ RV and let ŷ = (1/2m) i di yi . Then we have
X

(yi − yj )2 ≥

(i,j)∈E

Φ2 X
(yi − ŷ)2 .
16 i

(13.9)

To prove this, we need a lemma that can be thought of as a linear version of (13.9). For
every real vector y = (y1 , . . . , yn ), we define its median (relative to the degree sequence di )
as a the member yM of the sequence for which
X
X
dk ≤ m,
dk < m.
k: yk ≤yM

k: yk >yM

182

CHAPTER 13. SOME GENERAL ISSUES

Lemma 13.6.10 Let G = (V, E) be a graph with conductance Φ(G). Let y ∈ RV , and let
yM be the median of y. Then
X
ΦX
|yi − yj | ≥
di |yi − yM |.
2 i
(i,j)∈E

Proof. [of the Lemma] We may label the nodes so that y1 ≤ y2 ≤ . . . ≤ yn . We also may
assume that yM = 0 (the assertion of the Lemma is invariant under shifting the entries of
y). Substituting
yj − yi = (yi+1 − yi ) + · · · + (yj − yj−1 ),
we have
X

|yi − yj | =

n−1
X

e(≤ k, > k)(yk+1 − yk ).

k=1

(i,j)∈E

By the definition of Φ, this implies
X

|yi − yj | ≥

n−1
Φ X
d(≤ k)d(> k)(yk+1 − yk )
2m
k=1

(i,j)∈E

≥

Φ X
Φ X
d(≤ k)m(yk+1 − yk ) +
md(> k)(yk+1 − yk )
2m
2m
k<M

k≥M

Φ X
Φ X
=
di yi −
di yi
2
2
i≤M

i>M

ΦX
di |yi |.
=
2 i
¤
Now we return to the proof of the lower bound in Theorem 13.6.8. Let x be a unit
length eigenvector belonging to λ2 . We may assume that the nodes are labeled so that
P
x1 ≥ x2 ≥ . . . ≥ xn . Let xM be the median of x. Note that the average (1/(2m)) i di xi = 0.
¡
¢
¡
¢
Set zi = max{0, xi − xM } and ui = max{0, xM − xi } . Then
X
X
X
X
X
di zi2 +
di u2i =
di (xi − xM )2 =
x2i + 2mx2M ≥
di x2i = 1,
i

i

i

i

and so we may assume (replacing x by −x if necessary) that
X

di zi2 ≥

i

1
.
2

By Lemma 13.6.10
X
ΦX
|zi2 − zj2 | ≥
di zi2 .
2 i
(i,j)∈E

i

13.6. EIGENVALUES OF GRAPHS

183

On the other hand, using the Cauchy-Schwartz inequality,
X
X
|zi2 − zj2 | =
|zi − zj | · |zi + zj |
(i,j)∈E

(i,j)∈E


≤

1/2 

X

(zi − zj )2 



(i,j)∈E

1/2

X

(zi + zj )2 

.

(i,j)∈E

Here the second factor can be estimated as follows:
X
X
X
(zi + zj )2 ≤ 2
(zi2 + zj2 ) = 2
di zi2 .
(i,j)∈E

i

(i,j)∈E

Combining these inequalities, we obtain

2
. X
X
X
(zi + zj )2
(zi − zj )2 ≥ 
|zi2 − zj2 |
(i,j)∈E

(i,j)∈E

≥
Since

Φ2
4

Ã

X

!2
di zi2

i

X

(i,j)∈E

. X
Φ2 X
Φ2
2
di zi2 =
di zi2 ≥
.
8 i
16
i

(xi − xj )2 ≥

(i,j)∈E

from here we can conclude by Lemma 13.6.9.

X

(zi − zj )2 ,

(i,j)∈E

¤

The quantity Φ(G) is NP-complete to compute. An important theorem of Leighton and
Rao gives an approximate min-max theorem for it, which also yields a polynomial time
approximation algorithm, all with an error factor of O(log n).
Random walks
A random walk on a graph G is a random sequence (v 0 , v 1 , . . . ) of nodes constructed as follows:
We pick a starting point v 0 from a specified initial distribution σ, we select a neighbor v 1 of
it at random (each neighbor is selected with the same probability 1/d(v 0 )), then we select a
neighbor v 2 of this node v 1 at random, etc. We denote by σ k the distribution of v k .
In the language of probability theory, a random walk is a finite time-reversible Markov
chain. (There is not much difference between the theory of random walks on graphs and
the theory of finite Markov chains; every Markov chain can be viewed as random walk on a
directed graph, if we allow weighted edges, and every time-reversible Markov chain can be
viewed as random walks on an edge-weighted undirected graph.)
Let π denote the probability distribution in which the probability of a node is proportional
to its degree:
π(v) =

d(v)
.
2m

184

CHAPTER 13. SOME GENERAL ISSUES

This distribution is called the stationary distribution of the random walk. It is easy to check
that if v 0 is selected from π, then after any number of steps, v k will have the same distribution
π. This explains the name of π. Algebraically, this means that π is a left eigenvector of PG
with eigenvalue 1:
π T PG = π T .
Theorem 13.6.11 If G is a connected nonbipartite graph, then σ k → π for every starting
distribution σ.
It is clear that the conditions are necessary.
Before proving this theorem, let us make some remarks on one of its important applications, namely sampling. Suppose that we want to pick a random element uniformly from
some finite set. We can then construct a connected nonbipartite regular graph on this set,
and start a random walk on this graph. A node of the random walk after sufficiently many
steps is therefore essentially uniformly distributed.
(It is perhaps surprising that there is any need for a non-trivial way of generating an
element from such a simple distribution as the uniform. But think of the first application
of random walk techniques in real world, namely shuffling a deck of cards, as generating
a random permutation of 52 elements from the uniform distribution over all permutations.
The problem is that the set we want a random element from is exponentially large. In many
applications, it has in addition a complicated structure; say, we consider the set of lattice
points in a convex body or the set of linear extensions of a partial order. Very often this
random walk sampling is the only known method.)
With this application in mind, we see that not only the fact of convergence matters, but
also the rate of this convergence, called the mixing rate. The proof below will show how this
relates to the eigenvalue gap. In fact, we prove:
Theorem 13.6.12 Let λ1 ≥ λ2 ≥ · · · ≥ λn be the eigenvalues of PG , and let µ =
max{λ2 , λn }. Then for every starting node i and any node j, and every t ≥ 0, we have
s
π(j) t
|Pr(v t = j) − π(j)| ≤
µ.
π(i)
More generally, for every set A ⊆ V ,
s
π(A) t
|Pr(v t ∈ A) − π(A)| ≤
µ.
π(i)
Proof.

We prove the first inequality; the second is left to the reader as an exercise. We

know that the matrix NG has the same eigenvalues as PG , and it is symmetric, so we can

13.6. EIGENVALUES OF GRAPHS

185

write it as
NG =

n
X

λk vk vkT ,

k=1

where v1 , . . . , vn are mutually orthogonal eigenvectors. It is easy to check that we can choose
v1i =

√

πi

(we don’t know anything special about the other eigenvectors). Hence we get
−1/2 t 1/2
Pr(v t = j) = (P t )ij = eT
N d ej =
iD

n
X

−1/2
1/2
λtk (eT
vk )(eT
vk )
iD
jD

k=1
n
X

=

1
λtk p

π(i)

k=1

p
vki π(j)vkj = π(j) +

s

n

π(j) X t
λk vki vkj .
π(i)
k=2

Here the first term is the limit; we need to estimate the second. We have
n
n ¯
n ¯
¯X
¯
¯
¯
X
X
¯
¯
¯
¯
¯
¯
λtk vki vkj ¯ ≤ µt
¯
¯vki vkj ¯ ≤ µt
¯vki vkj ¯ ≤ µt
k=2

k=2

k=1

Ã

n
X

!1/2
2
vki

¡ 2 ¢1/2
vkj
= µt .

k=1

This proves the inequality.

¤

If we want to find a bound on the number of steps we need before, say,
|Pr(v k ∈ A) − π(A)| < ε
holds for every j, then it suffices to find a k for which
√
µ k < ε πi .
Writing mu = 1 − γ, and using that 1 − γ < e−γ , it suffices to have
√
e−γk < ε πi ,
and expressing k,
k>

1´
1³ 1 1
ln + ln
γ
ε 2 πi

So we see that (up to logarithmic factors), it is the reciprocal of the eigenvalue gap that
governs the mixing time.
In applications, the appearance of the smallest eigenvalue λn is usually not important,
and what we need to work on is bounding the eigenvalue gap 1−λ2 . The trick is the following:
If the smallest eigenvalue is too small, then we can modify the walk as follows. At each step,
we flip a coin and move with probability 1/2 and stay where we are with probability 1/2.

186

CHAPTER 13. SOME GENERAL ISSUES

The stationary distribution of this modified walk is the same, and the transition matrix PG
is replaced by 21 (PG + I). For this modified walk, all eigenvalues are nonnegative, and the
eigenvalue gap is half of the original. So applying the theorem to this, we only use a factor
of 2.
Explanation of conductance: In a stationary random walk on G, we cross every edge
in every direction with the same frequency, once in every 2m steps on the average. So
Q(S, V \ S) is the frequency with which we step out from S. If instead we consider a
sequence of independent samples from π, the frequency with which we step out from S is
π(S)π(V \ S). The ratio of these two frequencies is one of many possible ways comparing a
random walk with a sequence of independent samples.
Exercise 13.4 Let G = (V, E) be a simple graph, and define
ρ(G) = min

∅⊂S⊂V

eG (S, V \ S)
.
|S| · |V \ S|

Let λ2 denote the second smallest eigenvalue of the Laplacian LG of a graph G. Then
λ2 ≤ nρ(G) ≤

13.6.5

p

λ2 dmax .

The number of different eigenvalues

Multiplicity of eigenvalues usually corresponds to symmetries in the graph (although the
correspondence is not exact). We prove two results in this direction. The following theorem
was proved by Mowshowitz [155] and Sachs [174]:
Theorem 13.6.13 If all eigenvalues of A are different, then every automorphism of A has
order 1 or 2.
Proof. Every automorphism of G can be described by a permutation matrix P such that
AP = P A. Let u be an eigenvector of A with eigenvalue λ. Then
A(P u) = P Au = P (λu) = λ(P u),
so P u is also an eigenvector of A with the same eigenvalue. Since P u has the same length as
u, it follows that P u = ±u and hence P 2 u = u. This holds for every eigenvector u of A, and
¤
since there is a basis consisting of eigenvectors, it follows that P 2 = I.
A graph G is called strongly regular, if it is regular, and there are two nonnegative integers
a and b such that for every pair i, j of nodes the number of common neighbors of i and j is
(
a, if a and b are adjacent,
b, if a and b are nonadjacent.

13.6. EIGENVALUES OF GRAPHS

187

Example 13.6.14 Compute the spectrum of the Petersen graph, Paley graphs, incidence
graphs of finite projective planes.
The following characterization of strongly regular graphs is easy to prove:
Theorem 13.6.15 A connected graph G is strongly regular if and only if it is regular and
AG has at most 3 different eigenvalues.
Proof. The adjacency matrix of a strongly regular graph satisfies
A2 = aA + b(J − A − I) + dI.

(13.10)

The largest eigenvalue is d, all the others are roots of the equation
λ2 − (a − b)λ − (d − b),

(13.11)

Thus there are at most three distinct eigenvalues.
Conversely, suppose that G is d-regular and has at most three different eigenvalues. One
of these is d, with eigenvector 1. Let λ1 and λ2 be the other two (I suppose there are two
more—the case when there is at most one other is easy). Then
B = A2 − (λ1 + λ2 )A + λ1 λ2 I
is a matrix for which Bu = 0 for every eigenvector of A except 1 (and its scalar multiples).
Furthermore, B1 = c1, where c = (d − λ1 )(d − λ2 ). Hence B = (c/n)J, and so
A2 = (λ1 + λ2 )A − λ1 λ2 I + (c/n)J.
This means that (A2 )ij (i 6= j) depends only on whether i and j are adjacent, proving that
¤
G is strongly regular.
We can get more out of equation (13.11). We can solve it:
p
a − b ± (a − b)2 + 4(d − b)
λ1,2 =
.
2

(13.12)

Counting induced paths of length 2, we also get the equation
(d − a − 1)d = (n − d − 1)b.

(13.13)

Let m1 and m2 be the multiplicities of the eigenvalues λ1 and λ2 . Clearly
m1 + m2 = n − 1
Taking the trace of A, we get
d + m1 λ1 + m2 λ2 = 0,

(13.14)

188

CHAPTER 13. SOME GENERAL ISSUES

or
2d + (n − 1)(a − b) + (m1 − m2 )

p

(a − b)2 + 4(d − b) = 0.

(13.15)

If the square root is irrational, the only solution is d = (n − 1)/2, b = (n − 1)/4, a = b − 1.
There are many solutions where the square root is an integer.
A nice application of these formulas is the “Friendship Theorem”:
Theorem 13.6.16 If G is a graph in which every two nodes have exactly one common
neighbor, then it has a node adjacent to every other node.
Proof. First we show that two non-adjacent nodes must have the same degree. Suppose
that there are two non-adjacent nodes u, v of different degree. For every neighbor w of u
there is a common neighbor w0 of w and v. For different neighbors w1 and w2 of u, the nodes
w10 and w20 must be different, else w − 1 and w − 2 would have two common neighbors. So v
has at least as many neighbors as u. By a symmetric reasoning, we get du = dv .
If G has a node v whose degree occurs only once, then by the above, v must be connected
to every other node, and we are done. So suppose that no such node exists.
If G has two nodes u and v of different degree, then it contains two other nodes x and y
such that du = dx and dv = dy . But then both x and u are common neighbors of v and y,
contradicting the assumption.
Now if G is regular, then it is strongly regular, and a = b = 1. From (13.15),
√
d + (m1 − m2 ) d − 1 = 0.
The square root must be integral, hence d = k 2 + 1. But then k | k 2 + 1, whence k = 1,
d = 2, and the graph is a triangle, which is not a counterexample.
¤
Exercise 13.5 Prove that every graph with only two different eigenvalues is complete.
Exercise 13.6 Describe all disconnected strongly regular graphs. Show that there are disconnected graphs with only 3 distinct eigenvalues that are not strongly regular.

13.6.6

Spectra of graphs and optimization

There are many useful connections between the eigenvalues of a graph and its combinatorial
properties. The first of these follows easily from interlacing eigenvalues.
Proposition 13.6.17 The maximum size ω(G) of a clique in G is at most λ1 +1. This bound
remains valid even if we replace the non-diagonal 0’s in the adjacency matrix by arbitrary
real numbers.

13.7. CONVEX POLYTOPES

189

The following bound on the chromatic number is due to Hoffman.
Proposition 13.6.18 The chromatic number χ(G) of G is at least 1 − (λ1 /λn ). This bound
remains valid even if we replace the 1’s in the adjacency matrix by arbitrary real numbers.
The following bound on the maximum size of a cut is due to Delorme and Poljak [50,
49, 153, 161], and was the basis for the Goemans-Williamson algorithm discussed in the
introduction.
Proposition 13.6.19 The maximum size γ(G) of a cut in G is at most |E|/2 − (n/4)λn .
This bound remains valid even if we replace the diagonal 0’s in the adjacency matrix by
arbitrary real numbers.
Observation: to determine the best choice of the “free” entries in 13.6.17, 13.6.18 and
13.6.19 takes a semidefinite program. Consider 13.6.17 for example: we fix the diagonal
entries at 0, the entries corresponding to edges at 1, but are free to choose the entries
corresponding to non-adjacent pairs of vertices (replacing the off-diagonal 1’s in the adjacency
matrix). We want to minimize the largest eigenvalue. This can be written as a semidefinite
program:
minimize
subject to

t
tI − X º 0,
Xii = 0

(∀i ∈ V ),

Xij = 1

(∀ij ∈ E).

It turns out that the semidefinite program constructed for 13.6.18 is just the dual of this,
and their common optimum value is the parameter ϑ(G) introduced before. The program
for 13.6.19 gives the approximation used by Goemans and Williamson (for the case when all
weights are 1, from which it is easily extended).

13.7

Convex polytopes

13.7.1

Polytopes and polyhedra

The convex hull of a finite set of points in Rd is called a (convex) polytope. The intersection
of a finite number of halfspaces in Rd is called a (convex) polyhedron.
Proposition 13.7.1 Every polytope is a polyhedron. A polyhedron is a polytope if and only
if it is bounded.

190

CHAPTER 13. SOME GENERAL ISSUES

For every polytope, there is a unique smallest affine subspace that contains it, called its
affine hull. The dimension of a polytope is the dimension of it affine hull. A polytope in Rd
that has dimension d (equivalently, that has an interior point) is called a d-polytope.
A hyperplane H is said to support the polytope if it has a point in common with the
polytope and the polytope is contained in one of the closed halfspaces with boundary H.
A face of a polytope is its intersection with a supporting hyperplane. A face of a polytope
that has dimension one less than the dimension of the polytope is called a facet. A face of
dimension 0 (i.e., a single point) is called a vertex.
Proposition 13.7.2 Every face of a polytope is a polytope. Every vertex of a face is a vertex
of the polytope. Every polytope has a finite number of faces.
Proposition 13.7.3 Every polytope is the convex hull of its facets. The set of vertices is
the unique minimal finite set of points whose convex hull is the polytope.
Let P be a d-polytope. Then every facet F of P spans a (unique) supporting hyperplane,
and the hyperplane is the boundary of a uniquely determined halfspace that contains the
polytope. We’ll call this halfspace the halfspace of F .
Proposition 13.7.4 Every polytope is the intersection of the halfspaces of its facets.

13.7.2

The skeleton of a polytope

The vertices and edges of a polytope P form a simple graph GP , which we call the skeleton
of the polytope.
Proposition 13.7.5 Let P be a polytope in Rd , let a ∈ Rd , and let u be a vertex of P .
Suppose that there P has a vertex V such that aT u < aT v. Then P has a vertex w such that
uw is an edge and aT u < aT w.
Another way of formulating this is that if we consider the linear objective function aT x on
a polytope P , then from any vertex we can walk on the skeleton to a vertex that maximizes
the objective function so that the value of the objective function increases at every step. This
important fact is the basis for the Simplex Method.
For our purposes, however, the following corollaries of Proposition 13.7.5 will be important:
Corollary 13.7.6 The skeleton of any polytope is a connected graph.
Corollary 13.7.7 Let G be the skeleton of a d-polytope, and let H be an (open or closed)
halfspace containing an interior point of the polytope. Then the subgraph of GP induced by
those vertices of P that are contained in this halfspace is connected.

13.7. CONVEX POLYTOPES

191

From Corollary 13.7.7, it is not hard to derive the following (see Theorem 1.3.2 for d = 3):
Theorem 13.7.8 The skeleton of a d-dimensional polytope is d-connected.

13.7.3

Polar, blocker and antiblocker

Let P be a convex polytope containing the origin as an interior point. Then the polar of P
is defined as
P ∗ = {x ∈ Rd : xT y ≤ 1∀y ∈ P }
Proposition 13.7.9 (a) The polar of a polytope is a polytope. For every polytope P we have
(P ∗ )∗ = P .
(b) Let v0 , . . . , vm be the vertices of a k-dimensional face F of P . Then
T
F ⊥ = {x ∈ P ∗ : v0T x = 1, . . . , vm
x = 1}

defines a d − k − 1-dimensional face of P ∗ . Furthermore, (F ⊥ )⊥ = F .
In particular, every vertex v of P corresponds to a facet v ⊥ of P ∗ and vice versa. The
vector v is a normal vector of the facet v ⊥ .
There are two constructions similar to polarity that concern polyhedra that do not contain
the origin in their interior; rather, they are contained in the nonnegative orthant.
A polyhedron P in Rd is called ascending, if P ⊆ Rd+ and whenever x ∈ ¶, y ∈ Rd and
y ≥ x then y ∈ P .
The blocker of an ascending polyhedron is defined by
P bl = {x ∈ Rd+ : xT y ≤ 1∀y ∈ P }.
Proposition 13.7.10 The blocker of an ascending polyhedron is an ascending polyhedron.
For every ascending polyhedron P we have (P bl )bl = P .
The correspondence between faces of P and P bl is a bit more complicated than for polarity,
and we describe the relationship between vertices and facets only. Every vertex v of P gives
rise to a facet v⊥, which corresponds to the halfspace v T x ≥ 1. This construction gives
all the facets of P bl , except possibly those corresponding to the nonnegativity constraints
xi ≥ 0, which may or may not define facets.
A d-polytope P is called a corner polytope, if P ⊆ Rd+ and whenever x ∈ ¶, y ∈ Rd and
0 ≤ y ≤ x then y ∈ P .
The antiblocker of a corner polytope is defined by
P abl = {x ∈ Rd+ : xT y ≤ 1∀y ∈ P }.

192

CHAPTER 13. SOME GENERAL ISSUES

Proposition 13.7.11 The antiblocker of a corner polytope is a corner polytope. For every
corner polytope P we have (P abl )abl = P .
The correspondence between faces of P and P abl is more complicated than for the blocking polyhedra. The nonnegativity constraints xi ≥ 0 always define facets, and they don’t
correspond to vertices in the antiblocker. All other facets of P correspond to vertices of P abl .
Not every vertex of P defines a facet in P abl . The origin is a trivial exceptional vertex, but
there may be further exceptional vertices. We call a vertex v dominated, if there is another
vertex w such that v ≤ w. Now a vertex of P defines a facet of P ∗ if and only if it is not
dominated.

13.7.4

Optimization

Let P ⊆ Rn be an ascending polyhedron. It is easy to see that P has a unique point which
is closest to the origin. We’ll see later on that this point has combinatorial significance in
some cases. Right now, we state the following simple theorem that relates this point for the
analogous point in the blocker.
Theorem 13.7.12 Let P ⊆ Rn be an ascending polyhedron, and let a ∈ P minimize the
P
P
objective function i x2i . Let α = i a2i be the minimum value. Then b = (1/α)a is in the
P
blocker P bl , and it minimizes the objective function i x2i over P bl .
Proof.
¤

13.8

Semidefinite optimization

Linear programming has been one of the most fundamental and successful tools in optimization and discrete mathematics. Its applications include exact and approximation algorithms,
as well as structural results and estimates. The key point is that linear programs are very
efficiently solvable, and have a powerful duality theory.
Linear programs are special cases of convex programs; semidefinite programs are more
general but still convex programs, to which many of the useful properties of linear programs
extend. Recently, semidefinite programming arose as a generalization of linear programming
with substantial novel applications. Again, it can be used both in proofs and in the design
of exact and approximation algorithms. It turns out that various combinatorial optimization problems have semidefinite (rather than linear) relaxations which are still efficiently
computable, but approximate the optimum much better. This fact has lead to a real breakthrough in approximation algorithms.

13.8. SEMIDEFINITE OPTIMIZATION

193

Semidefinite programs arise in a variety of ways: as certain geometric extremal problems,
as relaxations (stronger than linear relaxations) of combinatorial optimization problems, in
optimizing eigenvalue bounds in graph theory, as stability problems in engineering, etc.
For more comprehensive studies of issues concerning semidefinite optimization, see [209,
135].

13.8.1

Semidefinite programs

A semidefinite program is an optimization problem of the following form:
minimize cT x
subject to x1 A1 + . . . xn An − B º 0

(13.16)

Here A1 , . . . , An , B are given symmetric m × m matrices, and c ∈ Rn is a given vector. We
can think of X = x1 A1 + . . . xn An − B as a matrix whose entries are linear functions of the
variables.
As usual, any choice of the values xi that satisfies the given constraint is called a feasible
solution. A solution is strictly feasible, if the matrix X is positive definite. We denote by
vprimal the supremum of the objective function.
The special case when A1 , . . . , An , B are diagonal matrices is just a “generic” linear program, and it is very fruitful to think of semidefinite programs as generalizations of linear
programs. But there are important technical differences. The following example shows that,
unlike in the case of linear programs, the supremum may be finite but not a maximum, i.e.,
not attained by any feasible solution.
Example 13.8.1 Consider the semidefinite program
minimize x1
µ
x1
subject to
1

1
x2

¶
º0

The semidefiniteness condition boils down to the inequalities x1 , x2 ≥ 0 and x1 x2 ≥ 1, so the
possible values of the objective function are all negative real numbers. Thus vprimal = 0, but
the supremum is not assumed.
As in the theory of linear programs, there are a large number of equivalent formulations
of a semidefinite program. Of course, we could consider minimization instead of maximization. We could stipulate that the xi are nonnegative, or more generally, we could allow
additional linear constraints on the variables xi (inequalities and/or equations). These could
be incorporated into the form above by extending the Ai and B with new diagonal entries.

194

CHAPTER 13. SOME GENERAL ISSUES

We could introduce the entries of A as variables, in which case the fact that they are linear
functions of the original variables translates into linear relations between them. Straightforward linear algebra transforms (13.16) into an optimization problem of the form
maximize C · X
subject to X º 0
D1 · X = d1
..
.

(13.17)

Dk · X = dk ,
where C, D1 , . . . , Dk are symmetric m × m matrices and d1 , . . . , dk ∈ R. Note that C · X is
the general form of a linear combination of entries of X, and so Di · X = di is the general
form of a linear equation in the entries of X.
It is easy to see that we would not get any substantially more general problem if we
allowed linear inequalities in the entries of X in addition to the equations.

13.8.2

Fundamental properties of semidefinite programs

We begin with the semidefinite version of the Farkas Lemma:
Lemma 13.8.2 [Homogeneous version] Let A1 , . . . , An be symmetric m × m matrices. The
system
x1 A1 + · · · + xn An Â 0
has no solution in x1 , . . . , xn if and only if there exists a symmetric matrix Y 6= 0 such that
A1 · Y = 0
A2 · Y = 0
..
.
An · Y = 0
Y º

0.

There is an inhomogeneous version of this lemma.
Lemma 13.8.3 [Inhomogeneous version] Let A1 , . . . , An , B be symmetric m × m matrices.
The system
x1 A1 + . . . xn An − B Â 0

13.8. SEMIDEFINITE OPTIMIZATION

195

has no solution in x1 , . . . , xn if and only if there exists a symmetric matrix Y 6= 0 such that
A1 · Y = 0
A2 · Y = 0
..
.
An · Y = 0
B·Y ≥0
Y º

0.

Given a semidefinite program (13.16), one can formulate the dual program:
maximize B · Y
subject to A1 · Y = c1
A2 · Y = c2
..
.

(13.18)

An · Y = cm
Y º 0.
Note that this too is a semidefinite program in the general sense. We denote by vdual the
infimum of the objective function.
With this notion of duality, the Duality Theorem holds in the following sense (see e.g.
[207, 202, 203]):
Theorem 13.8.4 Assume that both the primal and the dual semidefinite programs have feasible solutions. Then vprimal ≤ vdual . If, in addition, the primal program (say) has a strictly
feasible solution, then the dual optimum is attained and vprimal = vdual . In particular, if both
programs have strictly feasible solutions, then the supremum resp. infimum of the objective
functions are attained.
The following complementary slackness conditions also follow from this argument.
Proposition 13.8.5 Let x be a feasible solution of the primal program and Y , a feasible
solution of the dual program. Then vprimal = vdual and both x and Y are optimal solutions if
P
and only if Y ( i xi Ai − B) = 0.
The following example shows that the somewhat awkward conditions about the strictly
feasible solvability of the primal and dual programs cannot be omitted (see [163] for a detailed
discussion of conditions for exact duality).

196

CHAPTER 13. SOME GENERAL ISSUES

Example 13.8.6 Consider the semidefinite program
minimizex1


0
subject to x1
0

x1
x2
0


0
0 º0
x1 + 1

The feasible solutions are x1 = 0, x2 ≥ 0. Hence vprimal is assumed and is equal to 0. The
dual program is
maximize − Y33
subject toY12 + Y21 + Y33 = 1
Y22 = 0
Y º 0.
The feasible solutions are all matrices of the form


a 0 b
 0 0 0
b 0 1
where a ≥ b2 . Hence vdual = −1.

13.8.3

Algorithms for semidefinite programs

There are two essentially different algorithms known that solve semidefinite programs in
polynomial time: the ellipsoid method and interior point/barrier methods. Both of these
have many variants, and the exact technical descriptions are quite complicated; so we restrict ourselves to describing the general principles underlying these algorithms, and to some
comments on their usefulness. We ignore numerical problems, arising from the fact that the
optimum solutions may be irrational and the feasible regions may be very small; we refer to
[162, 163] for discussions of these problems.
The first polynomial time algorithm to solve semidefinite optimization problems in polynomial time was the ellipsoid method. Let K be a convex body (closed, compact, convex,
full-dimensional set) in RN . We set S(K, t) = {x ∈ RN : d(x, K) ≤ t}, where d denotes
euclidean distance. Thus S(0, t) is the ball with radius t about 0.
A (weak) separation oracle for a convex body K ⊆ RN is an oracle whose input is a
rational vector x ∈ RN and a rational ε > 0; the oracle either asserts that x ∈ S(K, ε) or
returns an “almost separating hyperplane” in the form of a vector 0 6= y ∈ RN such that
y T x > y T z − ε|y| for all z ∈ K.
If we have a weak separation oracle for a convex body (in practice, any subroutine the
realizes this oracle) then we can use the ellipsoid method to optimize any linear objective
function over K [83]:

13.8. SEMIDEFINITE OPTIMIZATION

197

Theorem 13.8.7 Let K be a convex body in Rn and assume that we know two real numbers
R > r > 0 such that S(0, r) ⊆ K ⊆ S(0, R). Assume further that we have a weak separation
oracle for K. Let a (rational) vector c ∈ Rn and an error bound 0 < ε < 1 be also given.
Then we can compute a (rational) vector x ∈ Rn such that x ∈ K and cT x ≥ cT z − ε for
every y ∈ K. The number of calls on the oracle and the number of arithmetic operations in
the algorithm are polynomial in log(R/r) + log(1/ε) + n.
This method can be applied to solve semidefinite programs in polynomial time, modulo
some technical conditions. (Note that some complications arise already from the fact that
the optimum value is not necessarily a rational number, even if all parameters are rational.
A further warning is example 13.8.6.)
Assume that we are given a semidefinite program (13.16) with rational coefficients and a
rational error bound ε > 0. Also assume that we know a rational, strictly feasible solution
x̃, and a bound R > 0 for the coordinates of an optimal solution. Then the set K of feasible
solutions is a closed, convex, bounded, full-dimensional set in Rn . It is easy to compute a
small ball around x0 that is contained in K.
The key step is to design a separation oracle for K. Given a vector x, we need only
check whether x ∈ K and if not, find a separating hyperplane. Ignoring numerical problems,
P
we can use Gaussian elimination to check whether the matrix Y = i xi Ai − B is positive
semidefinite. If it is, then x ∈ K. If not, the algorithm also returns a vector v ∈ Rm such
P
that v T Y v < 0. Then i xi v T Ai v = v T Bv is a separating hyperplane. (Because of numerical
problems, the error bound in the definition of the weak separation oracle is needed.)
Thus using the ellipsoid method we can compute, in time polynomial in log(1/ε) and in
the number of digits in the coefficients and in x0 , a feasible solution x such that the value of
the objective function is at most vprimal + ε.
Unfortunately, the above argument gives an algorithm which is polynomial, but hopelessly
slow, and practically useless. Still, the flexibility of the ellipsoid method makes it an inevitable
tool in proving the existence (and not much more) of a polynomial time algorithm for many
optimization problems.
Semidefinite programs can be solved in polynomial time and also practically efficiently by
interior point methods [158, 5, 6]. The key to this method is the following property of the
determinant of positive semidefinite matrices.
Lemma 13.8.8 The function F defined by
F (Y ) = − log det (Y )
is convex and analytic in the interior of the semidefinite cone Pn , and tends to ∞ at the
boundary.

198

CHAPTER 13. SOME GENERAL ISSUES

The algorithm can be described very informally as follows. The feasible domain of our
semidefinite optimization problem is of the form K = Pn ∩ A, where A is an affine subspace
of symmetric matrices. We want to minimize a linear function C · X over X ∈ K. The
good news is that K is convex. The bad news is that the minimum will be attained on
the boundary of K, and this boundary can have a very complicated structure; it is neither
smooth nor polyhedral. Therefore, neither gradient-type methods nor the methods of linear
programming can be used to minimize C · X.
The main idea of barrier methods is that instead of minimizing C T X, we minimize the
function FC (X) = F (X) + λC T X for some λ > 0. Since Fλ tends to infinity on the boundary
of K, the minimum will be attained in the interior. Since Fλ is convex and analytic in the
interior, the minimum can be very efficiently computed by a variety of numerical methods
(conjugate gradient etc.)
Of course, the point we obtain this way is not what we want, but if λ is large it will be
close. If we don’t like it, we can increase λ and use the minimizing point for the old Fλ as
the starting point for a new gradient type algorithm. (In practice, we can increase λ after
each iteration of this gradient algorithm.)
One can show that (under some technical assumptions about the feasible domain) this
algorithm gives an approximation of the optimum with relative error ε in time polynomial
in log(1/ε) and the size of the presentation of the program. The proof of this depends on a
further rather technical property of the determinant, called ”self-concordance”. We don’t go
into the details, but refer to the articles [6, 202, 203] and the book [157].

Bibliography
[1] B.M. Ábrego, S. Fernández-Merchant: A lower bound for the rectilinear crossing number.
Manuscript, 2003.
[2] O. Aichholzer, F. Aurenhammer and H. Krasser: On the crossing number of complete
graphs, in: Proc. 18th Ann. ACM Symp. Comp. Geom., Barcelona, Spain (2002), pp.
19–24.
[3] M. Ajtai, Chvtal, V.; Newborn, M.; Szemerdi, E. (1982). ”Crossing-free subgraphs”.
Theory and Practice of Combinatorics: 912, North-Holland Mathematics Studies, vol.
60.
[4] D. J. Aldous and J. Fill: Reversible Markov Chains and Random Walks on Graphs (book
in preparation); URL for draft at http://www.stat.Berkeley.edu/users/aldous
[5] F. Alizadeh: Combinatorial optimization with semi-definite matrices, in: Integer Programming and Combinatorial Optimization (Proceedings of IPCO ’92), (eds. E. Balas,
G. Cornuéjols and R. Kannan), Carnegie Mellon University Printing (1992), 385–405.
[6] F. Alizadeh, Interior point methods in semidefinite programming with applications to
combinatorial optimization, SIAM J. Optim. 5 (1995), 13–51.
[7] F. Alizadeh, J.-P. Haeberly, and M. Overton: Complementarity and nondegeneracy in
semidefinite programming, in: Semidefinite Programming, Math. Programming Ser. B,
77 (1997), 111–128.
[8] N. Alon: Eigenvalues and expanders, Combinatorica 6(1986), 83–96.
[9] N. Alon, The Shannon capacity of a union, Combinatorica 18 (1998), 301–310.
[10] N. Alon and E. Győri: The number of small semispaces of a finite set of points in the
plane, J. Combin. Theory Ser. A 41 (1986), 154–157.
[11] N. Alon and N. Kahale: Approximating the independence number via the ϑ-function,
Math. Programming 80 (1998), Ser. A, 253–264.
199

200

BIBLIOGRAPHY

[12] N. Alon and V. D. Milman: λ1 , isoperimetric inequalities for graphs and superconcentrators, J. Combinatorial Theory B 38(1985), 73–88.
[13] N. Alon, L. Lovász: Unextendible product bases, J. Combin. Theory Ser. A 95 (2001),
169–179.
[14] N. Alon, P.D. Seymour and R. Thomas: Planar separators, SIAM J. Disc. Math. 7
(1994), 184–193.
[15] N. Alon and J.H. Spencer: The Probabilistic Method, Wiley, New York, 1992.
[16] E. Andre’ev: On convex polyhedra in Lobachevsky spaces, Mat. Sbornik, Nov. Ser. 81
(1970), 445–478.
[17] S. Arora, C. Lund, R. Motwani, M. Sudan, M. Szegedy: Proof verification and hardness
of approximation problems Proc. 33rd FOCS (1992), 14–23.
[18] L. Asimov and B. Roth: The rigidity of graphs, Trans. Amer. Math. Soc., 245 (1978),
279–289.
[19] R. Bacher and Y. Colin de Verdière: Multiplicités des valeurs propres et transformations
étoile-triangle des graphes, Bull. Soc. Math. France 123 (1995), 101-117.
[20] E.G. Bajmóczy, I. Bárány, On a common generalization of Borsuk’s and Radon’s theorem, Acta Mathematica Academiae Scientiarum Hungaricae 34 (1979) 347–350.
[21] A.I. Barvinok: Problems of distance geometry and convex properties of quadratic maps,
Discrete and Comp. Geometry 13 (1995), 189–202.
[22] A.I. Barvinok: A remark on the rank of positive semidefinite matrices subject to affine
constraints, Discrete and Comp. Geometry (to appear)
[23] M. Bellare, O. Goldreich, and M. Sudan: Free bits, PCP and non-approximability—
towards tight results, SIAM J. on Computing 27 (1998), 804–915.
[24] I. Benjamini and L. Lovász: Global Information from Local Observation, Proc. 43rd
Ann. Symp. on Found. of Comp. Sci. (2002) 701–710.
[25] I. Benjamini and L. Lovász: Harmonic and analytic frunctions on graphs, Journal of
Geometry 76 (2003), 3–15.
[26] I. Benjamini and O. Schramm: Harmonic functions on planar and almost planar graphs
and manifolds, via circle packings. Invent. Math. 126 (1996), 565–587.
[27] I. Benjamini and O. Schramm: Random walks and harmonic functions on infinite planar
graphs using square tilings, Ann. Probab. 24 (1996), 1219–1238.

BIBLIOGRAPHY

201

[28] T. Böhme, On spatial representations of graphs, in: Contemporary Methods in Graph
Theory (R. Bodendieck, ed.), BI-Wiss.-Verl. Mannheim, Wien/Zurich, 1990, pp. 151–
167.
[29] J. Bourgain: On Lipschitz embedding of finite metric spaces in Hilbert space, Israel J.
Math. 52 (1985), 46–52.
[30] P. Brass: On the maximum number of unit distances among n points in dimension four,
Intuitive geometry Bolyai Soc. Math. Stud. 6, J. Bolyai Math. Soc., Budapest (1997),
277–290.
[31] P. Brass: Erdős distance problems in normed spaces, Comput. Geom. 6 (1996), 195–214.
[32] P. Brass: The maximum number of second smallest distances in finite planar sets. Discrete Comput. Geom. 7 (1992), 371–379.
[33] P. Brass, G. Károlyi: On an extremal theory of convex geometric graphs (manuscript)
[34] G.R. Brightwell, E. Scheinerman: Representations of planar graphs, SIAM J. Discrete
Math. 6 (1993) 214–229.
[35] R.L. Brooks, C.A.B. Smith, A.H. Stone and W.T. Tutte, The dissection of rectangles
into squares. Duke Math. J. 7, (1940). 312–340.
[36] A.E. Brouwer and W.H. Haemers: Association Schemes, in Handbook of Combinatorics
(ed. R. Graham, M. Grötschel, L. Lovász), Elsevier Science B.V. (1995), 747–771.
[37] P. Cameron, J.M. Goethals, J.J. Seidel and E.E. Shult, Line graphs, root systems and
elliptic geometry, J. Algebra 43 (1976), 305-327.
[38] A.K. Chandra, P. Raghavan, W.L. Ruzzo, R. Smolensky and P. Tiwari: The electrical
resistance of a graph captures its commute and cover times, Proc. 21st ACM STOC
(1989), 574–586.
[39] S.Y. Cheng, Eigenfunctions and nodal sets, Commentarii Mathematici Helvetici 51
(1976) 43–55.
[40] J. Cheriyan, J.H. Reif: Directed s-t numberings, rubber bands, and testing digraph
k-vertex connectivity, Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms, ACM, New York (1992), 335–344.
[41] M. Chudnovsky, N. Robertson, P. Seymour and R. Thomas: The strong perfect graph
theorem, Annals of Math., 164 (2006), 51-229.
[42] Y. Colin de Verdière: Sur la multiplicité de la première valeur propre non nulle du
laplacien, Comment. Math. Helv. 61 (1986), 254–270.

202

BIBLIOGRAPHY

[43] Y. Colin de Verdière: Sur un nouvel invariant des graphes et un critère de planarité,
Journal of Combinatorial Theory, Series B 50 (1990), 11–21. English translation: On a
new graph invariant and a criterion for planarity, in: Graph Structure Theory (Robertson
and P. D. Seymour, eds.), Contemporary Mathematics, Amer. Math. Soc., Providence,
RI (1993), 137–147.
[44] Y. Colin de Verdière: Une principe variationnel pour les empilements de cercles, Inventiones Math. 104 (1991) 655–669.
[45] Y. Colin de Verdière: Multiplicities of eigenvalues and tree-width of graphs, J. Combin.
Theory Ser. B 74 (1998), 121–146.
[46] Y. Colin de Verdière: Spectres de graphes, Cours Spécialisés 4, Société Mathématique
de France, Paris, 1998.
[47] R. Connelly: A counterexample to the rigidity conjecture for polyhedra, Publications
Mathmatiques de l’IHS 47 (1977), 333–338.
[48] G. Csizmadia: The multiplicity of the two smallest distances among points, Discrete
Math. 194 (1999), 67–86.
[49] C. Delorme and S. Poljak: Laplacian eigenvalues and the maximum cut problem, Math.
Programming 62 (1993) 557–574.
[50] C. Delorme and S. Poljak: Combinatorial properties and the complexity of max-cut
approximations, Europ. J. Combin. 14 (1993), 313–333.
[51] T.L. Dey: Improved bounds for planar k-sets and related problems, Discrete and Computational Geometry 19 (1998), 373–382.
[52] M. Deza and M. Laurent: Geometry of Cuts and Metrics, Springer Verlag, 1997.
[53] P.D. Doyle and J.L. Snell: Random walks and electric networks, Math. Assoc. of Amer.,
Washington, D.C. 1984.
[54] R.J. Duffin: Basic properties of discrete analytic functions, Duke Math. J. 23 (1956),
335–363.
[55] R.J. Duffin: Potential theory on the rhombic lattice, J. Comb. Theory 5 (1968) 258–272.
[56] R.J. Duffin and E.L. Peterson: The discrete analogue of a class of entire functions,
J. Math. Anal. Appl. 21 (1968) 619–642.
[57] I.A. Dynnikov and S.P. Novikov: Geometry of the Triangle Equation on Two-Manifolds,
Mosc. Math. J. 3 (2003), 419–438.

BIBLIOGRAPHY

203

[58] H. Edelsbrunner, P. Hajnal: A lower bound on the number of unit distances between
the vertices of a convex polygon, J. Combin. Theory Ser. A 56 (1991), 312–316.
[59] H.G. Eggleston: Covering a three-dimensional set with sets of smaller diameter, J.
London Math. Soc. 30 (1955), 11–24.
[60] P. Erdős, L. Lovász, G. Simmons, and E. Strauss: Dissection graphs of planar point
sets, in: A Survey of Combinatorial Theory, North Holland, Amsterdam, (1973), pp.
139–149.
[61] P. Erdős, On sets of distances of n points, Amer. Math. Monthly 53 (1946), 248–250.
[62] P. Erdős: On sets of distances of n points in Euclidean space, Publ. Math. Inst. Hung.
Acad. Sci. 5 (1960), 165–169.
[63] P. Erdős: Gráfok páros körüljárású részgráfjairól (On bipartite subgraphs of graphs, in
Hungarian), Mat. Lapok 18 (1967), 283–288.
[64] P. Erdős: On some applications of graph theory to geometry, Canad. J. Math. 19 (1967),
968–971.
[65] P. Erdős and N.G. de Bruijn: A colour problem for infinite graphs and a problem in
the theory of relations, Nederl. Akad. Wetensch. Proc. – Indagationes Math. 13 (1951),
369–373.
[66] P. Erdős, F. Harary and W.T. Tutte, On the dimension of a graph Mathematika 12
(1965), 118–122.
[67] P. Erdős, L. Lovász, K. Vesztergombi: On the graph of large distances, Discrete and
Computational Geometry 4 (1989) 541–549.
[68] P. Erdős, L. Lovász, K. Vesztergombi: The chromatic number of the graph of large
distances, in: Combinatorics, Proc. Coll. Eger 1987, Coll. Math. Soc. J. Bolyai 52,
North-Holland, 547–551.
[69] P. Erdős and M. Simonovits: On the chromatic number of geometric graphs, Ars Combin. 9 (1980), 229–246.
[70] U. Feige: Approximating the Bandwidth via Volume Respecting Embeddings, Tech.
Report CS98-03, Weizmann Institute (1998).
[71] J. Ferrand, Fonctions préharmoniques et fonctions préholomorphes. (French) Bull. Sci.
Math. 68, (1944). 152–180.
[72] C.M. Fiduccia, E.R. Scheinerman, A. Trenk, J.S. Zito: Dot product representations of
graphs, Discrete Math. 181 (1998), 113–138.

204

BIBLIOGRAPHY

[73] H. de Fraysseix, J. Pach, R. Pollack: How to draw a planar graph on a grid, Combinatorica 10 (1990), 41–51.
[74] P. Frankl, H. Maehara: The Johnson-Lindenstrauss lemma and the sphericity of some
graphs, J. Combin. Theory Ser. B 44 (1988), 355–362.
[75] P. Frankl, H. Maehara: On the contact dimension of graphs, Disc. Comput. Geom. 3
(1988) 89–96.
[76] P. Frankl, R.M. Wilson: Intersection theorems with geometric consequences, Combinatorica 1 (1981), 357–368.
[77] S. Friedland and R. Loewy, Subspaces of symmetric matrices containing matrices with
multiple first eigenvalue, Pacific J. Math. 62 (1976), 389–399.
[78] Z. Füredi: The maximum number of unit distances in a convex n-gon, J. Combin. Theory
Ser. A 55 (1990), 316–320.
[79] M. X. Goemans and D. P. Williamson: Improved approximation algorithms for maximum cut and satisfiability problems using semidefinite programming, J. Assoc. Comput.
Mach. 42 (1995), 1115–1145.
[80] M. Grötschel, L. Lovász and A. Schrijver: The ellipsoid method and its consequences in
combinatorial optimization, Combinatorica 1 (1981), 169-197.
[81] M. Grötschel, L. Lovász and A. Schrijver: Polynomial algorithms for perfect graphs,
Annals of Discrete Math. 21 (1984), 325-256.
[82] M. Grötschel, L. Lovász, A. Schrijver: Relaxations of vertex packing, J. Combin. Theory
B 40 (1986), 330-343.
[83] M. Grötschel, L. Lovász, A. Schrijver: Geometric Algorithms and Combinatorial Optimization, Springer (1988).
[84] B. Grünbaum: A proof of Vázsonyi’s conjecture, Bull. Res. Council Israel, section A 6
(1956), 77–78.
[85] Richard K. Guy: A combinatorial problem, Bull. Malayan Math. Soc. 7 (1967), 68–72.
[86] H. Hadwiger: Ein Überdeckungssatz für den Euklidischen Raum, Portugaliae Math. 4
(1944), 140–144.
[87] W. Haemers: On some problems of Lovász concerning the Shannon capacity of a graph,
IEEE Trans. Inform. Theory 25 (1979), 231–232.

BIBLIOGRAPHY

205

[88] J. Håstad: Some optimal in-approximability results, Proc. 29th ACM Symp. on Theory
of Comp., 1997, 1–10.
[89] J. Håstad: Clique is hard to approximate within a factor of n1−ε , Acta Math. 182 (1999),
105–142.
[90] Z.-X. He, O. Schramm: Hyperbolic and parabolic packings, Discrete Comput. Geom. 14
(1995), 123–149.
[91] Z.-X. He, O. Schramm: On the convergence of circle packings to the Riemann map,
Invent. Math. 125 (1996), 285–305.
[92] A. Heppes: Beweis einer Vermutung von A. Vázsonyi, Acta Math. Acad. Sci. Hung. 7
(1956), 463–466.
[93] A. Heppes, P. Révész: A splitting problem of Borsuk. (Hungarian) Mat. Lapok 7 (1956),
108–111.
[94] A.J. Hoffman, Some recent results on spectral properties of graphs, in: Beiträge zur
Graphentheorie, Teubner, Leipzig (1968) 75–80.
[95] H. van der Holst: A short proof of the planarity characterization of Colin de Verdière,
Journal of Combinatorial Theory, Series B 65 (1995) 269–272.
[96] H. van der Holst: Topological and Spectral Graph Characterizations, Ph.D. Thesis, University of Amsterdam, Amsterdam, 1996.
[97] H. van der Holst, M. Laurent, A. Schrijver, On a minor-monotone graph invariant,
Journal of Combinatorial Theory, Series B 65 (1995) 291–304.
[98] H. van der Holst, L. Lovász and A. Schrijver: On the invariance of Colin de Verdière’s
graph parameter under clique sums, Linear Algebra and its Applications, 226–228
(1995), 509–518.
[99] H. van der Holst, L. Lovász, A. Schrijver: The Colin de Verdière graph parameter,
in: Graph Theory and Combinatorial Biology, Bolyai Soc. Math. Stud. 7, János Bolyai
Math. Soc., Budapest (1999), 29–85.
[100] A.J. Hoffman: On graphs whose least eigenvalue exceeds −1 −

√

2, Linear Algebra and

Appl. 16 (1977), 153–165.
[101] A.J. Hoffman: On eigenvalues and colorings of graphs. Graph Theory and its Applications, Academic Press, New York (1970), 79–91.

206

BIBLIOGRAPHY

[102] H. van der Holst, L. Lovász, A. Schrijver: The Colin de Verdière graph parameter,
in: Graph Theory and Combinatorial Biology, Bolyai Soc. Math. Stud. 7, János Bolyai
Math. Soc., Budapest (1999), 29–85.
[103] H. Hopf, E. Pannwitz: Jahresbericht der Deutsch. Math. Vereinigung 43 (1934), 2 Abt.
114, Problem 167.
[104] R. Isaacs, Monodiffric functions, Natl. Bureau Standards App. Math. Series 18 (1952),
257–266.
[105] W.B. Johnson, J. Lindenstrauss: Extensions of Lipschitz mappings into a Hilbert space.
Conference in modern analysis and probability (New Haven, Conn., 1982), 189–206,
Contemp. Math., 26, Amer. Math. Soc., Providence, R.I., 1984.
[106] M.R. Jerrum and A. Sinclair: Approximating the permanent, SIAM J. Comput.
18(1989), 1149–1178.
[107] J. Jonasson and O. Schramm: On the cover time of planar graphs Electron. Comm.
Probab. 5 (2000), paper no. 10, 85–90.
[108] F. Juhász: The asymptotic behaviour of Lovász’ ϑ function for random graphs, Combinatorica 2 (1982) 153–155.
[109] J. Kahn, G, Kalai: A counterexample to Borsuk’s conjecture. Bull. Amer. Math. Soc.
29 (1993), 60–62.
[110] D. Karger, R. Motwani, M. Sudan: Approximate graph coloring by semidefinite programming, Proc. 35th FOCS (1994), 2–13.
[111] B. S. Kashin and S. V. Konyagin: On systems of vectors in Hilbert spaces, Trudy Mat.
Inst. V.A.Steklova 157 (1981), 64–67; English translation: Proc. of the Steklov Inst. of
Math. (AMS 1983), 67–70.
[112] R. Kenyon: The Laplacian and Dirac operators on critical planar graphs, Inventiones
Math 150 (2002), 409-439.
[113] R. Kenyon and J.-M. Schlenker: Rhombic embeddings of planar graphs with faces of
degree 4 (math-ph/0305057).
[114] D. E. Knuth: The sandwich theorem, The Electronic Journal of Combinatorics 1
(1994), 48 pp.
[115] P. Koebe: Kontaktprobleme der konformen Abbildung, Berichte über die Verhandlungen d. Sächs. Akad. d. Wiss., Math.–Phys. Klasse, 88 (1936) 141–164.

BIBLIOGRAPHY

207

[116] V. S. Konyagin, Systems of vectors in Euclidean space and an extremal problem for
polynomials, Mat. Zametky 29 (1981), 63–74. English translation: Math. Notes of the
Academy USSR 29 (1981), 33–39.
[117] A. Kotlov: A spectral characterization of tree-width-two graphs, Combinatorica 20
(2000)
[118] A. Kotlov, L. Lovász, S.!Vempala, The Colin de Verdière number and sphere representations of a graph, Combinatorica 17 (1997) 483–521.
[119] G. Laman (1970): On graphs and rigidity of plane skeletal structures, J. Engrg. Math.
4, 331–340.
[120] P. Lancaster, M. Tismenetsky, The Theory of Matrices, Second Edition, with Applications, Academic Press, Orlando, 1985.
[121] D.G. Larman, C.A. Rogers: The realization of distances within sets in Euclidean space.
Mathematika 19 (1972), 1–24.
[122] M. Laurent and S. Poljak: On the facial structure of the set of correlation matrices,
SIAM J. on Matrix Analysis and Applications 17 (1996), 530–547.
[123] F.T. Leighton: Complexity Issues in VLSI, Foundations of Computing Series, MIT
Press, Cambridge, MA (1983).
[124] F.T. Leighton and S. Rao, An approximate max-flow min-cut theorem for uniform
multicommodity flow problems with applications to approximation algorithms, Proc.
29th Annual Symp. on Found. of Computer Science, IEEE Computer Soc., (1988), 422431.
[125] N. Linial, E. London, Y. Rabinovich: The geometry of graphs and some of its algorithmic applications, Combinatorica 15 (1995), 215–245.
[126] N. Linial, L. Lovász, A. Wigderson: Rubber bands, convex embeddings, and graph
connectivity, Combinatorica 8 (1988) 91-102.
[127] R.J. Lipton, R.E. Tarjan: A Separator Theorem for Planar Graphs, SIAM J. on Applied
Math., 36 (1979), 177–189.
[128] L. Lovász: Normal hypergraphs and the perfect graph conjecture, Discrete Math. 2
(1972), 253-267.
[129] L. Lovász: Combinatorial Problems and Exercises, Second Edition, Akadémiai Kiadó
- North Holland, Budapest, 1991

208

BIBLIOGRAPHY

[130] L. Lovász: Flats in matroids and geometric graphs, in: Combinatorial Surveys, Proc.
6th British Comb. Conf., Academic Press (1977), 45-86.
[131] L. Lovász: On the Shannon capacity of graphs, IEEE Trans. Inform. Theory 25 (1979),
1–7.
[132] L. Lovász: Singular spaces of matrices and their applications in combinatorics, Bol.
Soc. Braz. Mat. 20 (1989), 87–99.
[133] L. Lovász: Random walks on graphs: a survey, in: Combinatorics, Paul Erdős is
Eighty, Vol. 2 (ed. D. Miklos, V. T. Sos, T. Szonyi), János Bolyai Mathematical Society,
Budapest, 1996, 353–398.
[134] L. Lovász: Steinitz representations of polyhedra and the Colin de Verdière number, J.
Comb. Theory B 82 (2001), 223–236.
[135] L. Lovász: Semidefinite programs and combinatorial optimization, in: Recent Advances in Algorithms and Combinatorics, CMS Books Math./Ouvrages Math. SMC 11,
Springer, New York (2003), 137-194.
[136] L. Lovász: Graph minor theory, Bull. Amer. Math. Soc. 43 (2006), 75–86.
[137] L. Lovász, M. Saks, A. Schrijver: Orthogonal representations and connectivity of
graphs, Linear Alg. Appl. 114/115 (1989), 439–454; A Correction: Orthogonal representations and connectivity of graphs, Linear Algebra Appl. 313 (2000) 101–105.
[138] L. Lovász, A. Schrijver: A Borsuk theorem for antipodal links and a spectral characterization of linklessly embedable graphs, Proceedings of the American Mathematical
Society 126 (1998) 1275–1285.
[139] L. Lovász, A. Schrijver: On the null space of a Colin de Verdière matrix, Annales de
l’Institute Fourier 49, 1017–1026.
[140] L. Lovász and B. Szegedy: Szemerédi’s Lemma for the analyst, Geom. Func. Anal. 17
(2007), 252–270.
[141] L. Lovász, K. Vesztergombi: Geometric representations of graphs, to appear in Paul
Erdos and his Mathematics, (ed. G. Halász, L. Lovász, M. Simonovits, V.T. Sós), Bolyai
Society–Springer Verlag.
[142] L. Lovász, K. Vesztergombi, U. Wagner, E. Welzl: Convex quadrilaterals and k-sets,
in: Towards a Theory of Geometric Graphs, (J. Pach, Ed.), AMS Contemporary Mathematics 342 (2004), 139–148.

BIBLIOGRAPHY

209

[143] L. Lovász and P. Winkler: Mixing times, in: Microsurveys in Discrete Probability (ed.
D. Aldous and J. Propp), DIMACS Series in Discr. Math. and Theor. Comp. Sci., 41
(1998), 85–133.
[144] Lubotzky, Phillips, Sarnak
[145] P. Mani, Automorphismen von polyedrischen Graphen, Math. Annalen 192 (1971),
279–303.
[146] Margulis [expander]
[147] J. Matoušek: On embedding expanders into lp spaces. Israel J. Math. 102 (1997),
189–197.
[148] C. Mercat, Discrete Riemann surfaces and the Ising model. Comm. Math. Phys. 218
(2001), no. 1, 177–216.
[149] C. Mercat: Discrete polynomials and discrete holomorphic approximation (2002)
[150] C. Mercat: Exponentials form a basis for discrete holomorphic functions (2002)
[151] G.L. Miller, W. Thurston: Separators in two and three dimensions, Proc. 22th ACM
STOC (1990), 300–309.
[152] G.L. Miller, S.-H. Teng, W. Thurston, S.A. Vavasis: Separators for sphere-packings
and nearest neighbor graphs. J. ACM 44 (1997), 1–29.
[153] B. Mohar and S. Poljak: Eigenvalues and the max-cut problem, Czechoslovak Mathematical Journal 40 (1990), 343–352.
[154] B. Mohar and C. Thomassen: Graphs on surfaces. Johns Hopkins Studies in the Mathematical Sciences. Johns Hopkins University Press, Baltimore, MD, 2001.
[155] A. Mowshowitz: Graphs, groups and matrices, in: Proc. 25th Summer Meeting Canad.
Math. Congress, Congr. Numer. 4 (1971), Utilitas Mathematica, Winnipeg, 509-522.
[156] C. St.J. A. Nash-Williams, Random walks and electric currents in networks, Proc.
Cambridge Phil. Soc. 55 (1959), 181–194.
[157] Yu. E. Nesterov and A. Nemirovsky: Interior-point polynomial methods in convex programming, Studies in Appl. Math. 13, SIAM, Philadelphia, 1994.
[158] M. L. Overton: On minimizing the maximum eigenvalue of a symmetric matrix, SIAM
J. on Matrix Analysis and Appl. 9 (1988), 256–268.
[159] M. L. Overton and R. Womersley: On the sum of the largest eigenvalues of a symmetric
matrix, SIAM J. on Matrix Analysis and Appl. 13 (1992), 41–45.

210

BIBLIOGRAPHY

[160] G. Pataki: On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues, Math. of Oper. Res. 23 (1998), 339–358.
[161] S. Poljak and F. Rendl: Nonpolyhedral relaxations of graph-bisection problems, DIMACS Tech. Report 92-55 (1992).
[162] L. Porkoláb and L. Khachiyan: On the complexity of semidefinite programs, J. Global
Optim. 10 (1997), 351–365.
[163] M. Ramana: An exact duality theory for semidefinite programming and its complexity
implications, in: Semidefinite programming. Math. Programming Ser. B, 77 (1997), 129–
162.
[164] A. Recski: Matroid theory and its applications in electric network theory and in statics,
Algorithms and Combinatorics, 6. Springer-Verlag, Berlin; Akadémiai Kiadó, Budapest,
1989.
[165] J. Reiterman, V. Rödl, E. Šinajová: Embeddings of graphs in Euclidean spaces, Discr.
Comput. Geom. 4 (1989), 349–364.
[166] J. Reiterman, V. Rödl and E. Šinajová: On embedding of graphs into Euclidean spaces
of small dimension, J. Combin. Theory Ser. B 56 (1992), 1–8.
[167] J. Richter-Gebert: Realization spaces of polytopes, Lecture Notes in Mathematics 1643,
Springer-Verlag, Berlin, 1996.
[168] N. Robertson, P.D. Seymour: Graph minors III: Planar tree-width, J. Combin. Theory
B 36 (1984), 49–64.
[169] N. Robertson, P.D. Seymour, Graph minors XX: Wagner’s conjecture (preprint, 1988).
[170] N. Robertson, P. Seymour, R. Thomas: A survey of linkless embeddings, in: Graph
Structure Theory (N. Robertson, P. Seymour, eds.), Contemporary Mathematics, American Mathematical Society, Providence, Rhode Island, 1993, pp. 125–136.
[171] N. Robertson, P. Seymour, R. Thomas: Hadwiger’s conjecture for K6 -free graphs,
Combinatorica 13 (1993) 279–361.
[172] N. Robertson, P. Seymour, R. Thomas: Sachs’ linkless embedding conjecture, Journal
of Combinatorial Theory, Series B 64 (1995) 185–227.
[173] B. Rodin and D. Sullivan: The convergence of circle packings to the Riemann mapping,
J. Diff. Geom. 26 (1987), 349–360.
[174] H. Sachs,

BIBLIOGRAPHY

211

[175] H. Sachs, Coin graphs, polyhedra, and conformal mapping, Discrete Math. 134 (1994),
133–138.
[176] F. Saliola and W. Whiteley: Constraining plane configurations in CAD: Circles, lines
and angles in the plane SIAM J. Discrete Math. 18 (2004), 246–271.
[177] H. Saran: Constructive Results in Graph Minors: Linkless Embeddings, Ph.D. Thesis,
University of California at Berkeley, 1989.
[178] W. Schnyder: Embedding planar graphs on the grid, Proc. First Annual ACM-SIAM
Symp. on Discrete Algorithms, SIAM, Philadelphia (1990), 138–148;
[179] O. Schramm: Existence and uniqueness of packings with specified combinatorics, Israel
Jour. Math. 73 (1991), 321–341.
[180] O. Schramm: How to cage an egg, Invent. Math. 107 (1992), 543–560.
[181] O. Schramm: Square tilings with prescribed combinatorics, Israel Jour. Math. 84
(1993), 97–118.
[182] A. Schrijver, Minor-monotone graph invariants, in: Combinatorics 1997 (P. Cameron,
ed.), Cambridge University Press, Cambridge, 1997, to appear.
[183] D. Singer:
Rectilinear crossing numbers, Manuscript (1971),
http://www.cwru.edu/artsci/math/singer/home.html.

available at

[184] P.M. Soardi: Potential Theory on Infinite Networks, Lecture notes in Math. 1590,
Springer-Verlag, Berlin–Heidelberg, 1994.
[185] J. Spencer, E. Szemerédi and W.T. Trotter: Unit distances in the Euclidean plane,
Graph Theory and Combinatorics, Academic Press, London–New York (1984), 293–303.
[186] D.A. Spielman and S.-H. Teng: Disk Packings and Planar Separators, 12th Annual
ACM Symposium on Computational Geometry (1996), 349–358.
[187] D.A. Spielman and S.-H. Teng: Spectral Partitioning Works: Planar graphs and finite
element meshes, Proceedings of the 37th Annual IEEE Conference on Foundations of
Comp. Sci. (1996), 96–105.
[188] E. Steinitz: Polyeder und Raumabteilungen, in:
senschaften 3 (Geometrie) 3AB12, 1–139, 1922.

Encyclopädie der Math. Wis-

[189] J.W. Sutherland: Jahresbericht der Deutsch. Math. Vereinigung 45 (1935), 2 Abt. 33.
[190] K. Stephenson: Introduction to Circle Packing: The Theory of Discrete Analytic Functions, Cambridge University Press, 2005.

212

BIBLIOGRAPHY

[191] S. Straszewicz: Sur un problème geometrique de Erdős, Bull. Acad. Polon. Sci. CI.III
5 (1957), 39–40.
[192] M. Szegedy: A note on the θ number of Lovász and the generalized Delsarte bound,
Proc. 35th FOCS (1994), 36–39.
[193] L.A. Székely: Measurable chromatic number of geometric graphs and sets without some
distances in Euclidean space, Combinatorica 4 (1984), 213–218.
[194] L.A. Székely: Crossing numbers and hard Erdős problems in discrete geometry. Combin.
Probab. Comput. 6 (1997), 353–358.
[195] W.P. Thurston: Three-dimensional Geometry and Topology, Princeton Mathematical
Series 35, Princeton University Press, Princeton, NJ, 1997.
[196] G. Tóth: Point sets with many k-sets, Discr. Comput. Geom. 26 (2001), 187–194.
[197] W.T. Tutte: How to draw a graph, Proc. London Math. Soc. 13 (1963), 743–768.
[198] K. Vesztergombi: On the distribution of distances in finite sets in the plane, Discrete
Math., 57, [1985], 129–145.
[199] K. Vesztergombi: Bounds on the number of small distances in a finite planar set, Stu.
Sci. Acad. Hung., 22 [1987], 95–101.
[200] K. Vesztergombi: On the two largest distances in finite planar sets, Discrete Math. 150
(1996) 379–386.
[201] W. Whiteley, Infinitesimally rigid polyhedra, Trans. Amer. Math. Soc. 285 (1984),
431–465.
[202] L. Vandeberghe and S. Boyd: Semidefinite programming, in: Math. Programming:
State of the Art (ed. J. R. Birge and K. G. Murty), Univ. of Michigan, 1994.
[203] L. Vandeberghe and S. Boyd: Semidefinite programming. SIAM Rev. 38 (1996), no. 1,
49–95.
[204] R.J. Vanderbei and B. Yang: The simplest semidefinite programs are trivial, Math. of
Oper. Res. 20 (1995), no. 3, 590–596.
[205] E. Welzl: Entering and leaving j-facets, Discr. Comput. Geom. 25 (2001), 351–364.
[206] W. Whiteley: ***
[207] H. Wolkowitz: Some applications of optimization in matrix theory, Linear Algebra and
its Applications 40 (1981), 101–118.

BIBLIOGRAPHY

213

[208] H. Wolkowicz: Explicit solutions for interval semidefinite linear programs, Linear Algebra Appl. 236 (1996), 95–104.
[209] H. Wolkowicz, R. Saigal and L. Vandenberghe: Handbook of semidefinite programming.
Theory, algorithms, and applications. Int. Ser. Oper. Res. & Man. Sci., 27 (2000) Kluwer
Academic Publishers, Boston, MA.
[210] D. Zeilberger: A new basis for discrete analytic polynomials, J. Austral. Math. Soc.
Ser. A 23 (1977), 95-104.
[211] D. Zeilberger: A new approach to the theory of discrete analytic functions, J. Math.
Anal. Appl. 57 (1977), 350-367.
[212] D. Zeilberger and H. Dym: Further properties of discrete analytic functions, J. Math.
Anal. Appl. 58 (1977), 405-418.

